<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Luca Simonetti">
<meta name="dcterms.date" content="2025-02-25">

<title>DPO: Direct Preference Optimization – L.</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-a973770d809960bcbda85a0f6824f60f.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
<meta property="og:title" content="DPO: Direct Preference Optimization">
<meta property="og:description" content="LA DPO spiegata, spero, facile.">
<meta property="og:image" content="https://theghoul21.github.io//images/dpo.jpeg">
<meta property="og:site_name" content="L.">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">L.</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../projects/index.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-lecture-notes" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Lecture notes</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-lecture-notes">    
        <li>
    <a class="dropdown-item" href="https://theghoul21.github.io/CIT-2024-2025/" target="_blank">
 <span class="dropdown-text">Complexity and Information Theory 2024/2025</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://theghoul21.github.io/DL-2024-2025/" target="_blank">
 <span class="dropdown-text">Deep Learning 2024/2025</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://theghoul21.github.io/RO-2024-2025/" target="_blank">
 <span class="dropdown-text">Ricerca Operativa 2024/2025</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://theghoul21.github.io/FNN-2024-2025/" target="_blank">
 <span class="dropdown-text">Foundations of Neural Networks 2024/2025</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://theghoul21.github.io/QC-2023-2024/" target="_blank">
 <span class="dropdown-text">Quantum Computing 2023/2024</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://theghoul21.github.io/MB-2024-2025/" target="_blank">
 <span class="dropdown-text">Biologia Molecolare 2024/2025</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://theghoul21.github.io/VVT-2024-2025/" target="_blank">
 <span class="dropdown-text">Verification and Validation Techniques 2024/2025</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://theghoul21.github.io/AR-2024-2025/" target="_blank">
 <span class="dropdown-text">Automated Reasoning 2024/2025</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://theghoul21.github.io/AA-2023-2024/" target="_blank">
 <span class="dropdown-text">Advanced Algorithms 2023/2024</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://theghoul21.github.io/AI-2023-2024/" target="_blank">
 <span class="dropdown-text">Artificial Intelligence 2023/2024</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://theghoul21.github.io/SA-2024-2025/" target="_blank">
 <span class="dropdown-text">Applied Statistics 2024/2025</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-page-left">
      <h1 class="title">DPO: Direct Preference Optimization</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">computer science</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta column-page-left">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Luca Simonetti </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 25, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#direct-preference-optimization" id="toc-direct-preference-optimization" class="nav-link active" data-scroll-target="#direct-preference-optimization">Direct Preference Optimization</a>
  <ul class="collapse">
  <li><a href="#rlhf" id="toc-rlhf" class="nav-link" data-scroll-target="#rlhf">RLHF</a></li>
  <li><a href="#dpo-la-figata-che-semplifica-tutto-o-quasi" id="toc-dpo-la-figata-che-semplifica-tutto-o-quasi" class="nav-link" data-scroll-target="#dpo-la-figata-che-semplifica-tutto-o-quasi">DPO: La Figata che Semplifica Tutto (O Quasi)</a></li>
  <li><a href="#limiti" id="toc-limiti" class="nav-link" data-scroll-target="#limiti">Limiti</a></li>
  <li><a href="#quindi" id="toc-quindi" class="nav-link" data-scroll-target="#quindi">Quindi</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block column-page-left" id="quarto-document-content">





<section id="direct-preference-optimization" class="level1">
<h1>Direct Preference Optimization</h1>
<p>Se vi siete mai chiesti come fanno i vari ChatGPT, Claude eccetera a rispondere in modo sensato e, soprattutto, a non insultarvi ogni due per tre, allora siete nel posto giusto. Perché dietro c’è spesso un processo chiamato <strong>Reinforcement Learning from Human Feedback (RLHF)</strong>, ovvero “Apprendimento per Rinforzo con Feedback Umano”. Sembra complicato, e in effetti lo è, ma cercherò di spiegarvelo senza farvi venire un’aneurisma.</p>
<section id="rlhf" class="level2">
<h2 class="anchored" data-anchor-id="rlhf">RLHF</h2>
<p>Immaginate di voler insegnare a un cane a fare il riporto. Come fate? Gli lanciate la pallina, se ve la riporta gli date un biscottino e gli dite “bravo!”. Se invece scappa col la pallina e se la va a sotterrare in giardino, lo sgridate (o magari tirate giù un porco, dipende da quanto siete incazzati). Ecco, l’RLHF è un po’ la stessa cosa, ma invece del cane abbiamo un modello linguistico gigante e invece dei biscottini e delle ciabatte abbiamo dei <strong>ricompense</strong> e delle <strong>penalità</strong> (in gergo tecnico si chiamano <em>reward</em> e <em>punishment</em>, ma siamo tra amici, quindi diciamo ricompense e penalità).</p>
<p>Il processo di RLHF, in parole povere, funziona così:</p>
<ol type="1">
<li><p><strong>Fase 1: Supervised Fine-Tuning - SFT:</strong> Si parte da un modello linguistico pre-addestrato (tipo GPT, per intenderci). Questo modello è stato addestrato su una quantità di testo immane presa da internet, quindi sa già un sacco di cose, ma è un po’ come un adolescente: sa tante cose teoriche, ma poi nella pratica fa un casino. Quindi, in questa fase, gli si fa fare un po’ di pratica con esempi specifici, tipo “rispondi a questa domanda in modo conciso”, “scrivi un riassunto di questo testo”, eccetera. È come dargli dei compiti a casa, insomma. Questa fase si chiama <strong>Supervised Fine-Tuning (SFT)</strong>, ovvero “Messa a Punto Supervisionata” (che schifo i nomi tradotti in italiano). Fin qui, tutto abbastanza facile.</p></li>
<li><p><strong>Fase 2: Addestramento del Modello di Ricompensa (Reward Modeling - RM):</strong> Qui inizia il casino. Perché per usare l’apprendimento per rinforzo, abbiamo bisogno di una cosa fondamentale: un <strong>modello di ricompensa</strong>. Ma che cazzo è ’sto modello di ricompensa? È un altro modello (sì, un altro, perché le cose semplici non ci piacciono) che impara a valutare quanto è “buona” una risposta generata dal modello linguistico. “Buona” in che senso? Nel senso che è utile, pertinente, non offensiva, non tossica, eccetera eccetera. Insomma, tutte quelle cose che vogliamo da un chatbot che non sia un troll di internet.</p>
<p>Per addestrare questo benedetto modello di ricompensa, abbiamo bisogno di <strong>dati di preferenza umana</strong>. UMANA, mi piace come suona. UMANA. Cosa vuol dire UMANA? Vuol dire che dobbiamo far valutare a delle persone vere (i cosiddetti “annotatori umani”, in Kenya o in India di solito) delle coppie di risposte generate dal modello linguistico. Tipo, gli si fa vedere due risposte diverse alla stessa domanda e gli si chiede: “Quale di queste due risposte ti piace di più?”. E l’annotatore umano deve scegliere: “Quetta qui!”. Dopo aver raccolto un bel po’ di ’ste coppie di preferenze, si addestra il modello di ricompensa a predire queste preferenze. In pratica, il modello di ricompensa impara a dire: “Ah, questa risposta è meglio di quest’altra perché è più [… qualcosa]”.</p>
<p>Matematicamente, il modello di ricompensa spesso si basa sul modello di <strong>Bradley-Terry</strong>. E voi chiederete: che cazzo è il modello di Bradley-Terry? È un modello statistico che serve per modellare le probabilità di vittoria in competizioni a coppie. Immagina di fare un torneo di braccio di ferro. Il modello di Bradley-Terry cerca di capire, date le prestazioni passate dei partecipanti, qual è la probabilità che Tizio batta Caio. Nel nostro caso, la “competizione” è tra due risposte generate dal modello linguistico, e il “vincitore” è la risposta preferita dall’annotatore umano.</p>
<p>Se indichiamo con <span class="math inline">\(r_\theta(x, y)\)</span> il punteggio (non normalizzato) che il modello di ricompensa <span class="math inline">\(\theta\)</span> assegna alla risposta <span class="math inline">\(y\)</span> data la domanda <span class="math inline">\(x\)</span>, allora il modello di Bradley-Terry dice che la probabilità che la risposta <span class="math inline">\(y_1\)</span> sia preferita alla risposta <span class="math inline">\(y_2\)</span> è data da:</p>
<p><span class="math display">\[P(y_1 \succ y_2 | x) = \sigma(r_\theta(x, y_1) - r_\theta(x, y_2))\]</span></p>
<p>dove <span class="math inline">\(\sigma(z) = \frac{1}{1 + e^{-z}}\)</span> è la funzione sigmoide, che serve a schiacciare i valori tra 0 e 1, in modo da ottenere una probabilità. In pratica, più la differenza tra i punteggi <span class="math inline">\(r_\theta(x, y_1)\)</span> e <span class="math inline">\(r_\theta(x, y_2)\)</span> è grande, più è probabile che <span class="math inline">\(y_1\)</span> sia preferita a <span class="math inline">\(y_2\)</span>. E viceversa. Il modello di ricompensa <span class="math inline">\(\theta\)</span> viene addestrato per massimizzare la probabilità di predire correttamente le preferenze umane sui dati di addestramento.</p></li>
<li><p><strong>Fase 3: Ottimizzazione della Politica con Apprendimento per Rinforzo (Reinforcement Learning Policy Optimization):</strong> Finalmente, arriviamo alla parte “rinforzo”. Ora che abbiamo ‘sto modello di ricompensa che ci dice quanto sono buone le risposte, possiamo usarlo per “guidare” il modello linguistico a generare risposte sempre migliori. Come? Usando un algoritmo di apprendimento per rinforzo, tipo <strong>Proximal Policy Optimization (PPO)</strong>. PPO è un algoritmo un po’ complicato (non ci addentriamo nei dettagli, se no non ne usciamo più), ma l’idea di base è questa: si cerca di modificare il modello linguistico in modo da aumentare la ricompensa media ottenuta, senza però allontanarsi troppo dal modello linguistico originale (quello SFT). È come cercare di migliorare il cane nel riporto, ma senza fargli dimenticare tutto quello che ha imparato prima.</p>
<p>In questa fase, si definisce una <strong>funzione obiettivo</strong> che cerca di massimizzare la ricompensa, ma anche di mantenere il modello linguistico “vicino” a quello SFT. Una funzione obiettivo tipica per RLHF è tipo questa:</p>
<p><span class="math display">\[J_{RLHF}(\pi_\phi) = \mathbb{E}_{(x, y) \sim D_\pi} [r_\theta(x, y) - \beta \cdot KL(\pi_\phi(y|x) || \pi_{SFT}(y|x))]\]</span></p>
<p>Dove:</p>
<ul>
<li><span class="math inline">\(\pi_\phi\)</span> è la <strong>politica</strong> (policy) del modello linguistico che stiamo ottimizzando (parametri <span class="math inline">\(\phi\)</span>). In pratica, è il modello linguistico stesso.</li>
<li><span class="math inline">\(D_\pi\)</span> è la distribuzione dei dati generati dalla politica <span class="math inline">\(\pi_\phi\)</span>.</li>
<li><span class="math inline">\(r_\theta(x, y)\)</span> è la ricompensa predetta dal modello di ricompensa <span class="math inline">\(\theta\)</span> per la risposta <span class="math inline">\(y\)</span> data la domanda <span class="math inline">\(x\)</span>.</li>
<li><span class="math inline">\(KL(\pi_\phi(y|x) || \pi_{SFT}(y|x))\)</span> è la divergenza di Kullback-Leibler (KL) tra la politica attuale <span class="math inline">\(\pi_\phi\)</span> e la politica SFT <span class="math inline">\(\pi_{SFT}\)</span>. La divergenza KL quantifica quanto sono diverse due distribuzioni di probabilità. In questo contesto, penalizza le politiche <span class="math inline">\(\pi_\phi\)</span> che si discostano troppo da <span class="math inline">\(\pi_{SFT}\)</span>. Anche se spesso si semplifica dicendo che è una “misura della distanza” tra distribuzioni, tecnicamente non è una distanza vera e propria poiché non rispetta la simmetria (e quindi la commutatività) né la disuguaglianza triangolare. È più corretto considerarla una misura di “dissimilarità”: più è alta, più le due distribuzioni sono diverse.</li>
<li><span class="math inline">\(\beta\)</span> è un iperparametro che controlla quanto vogliamo penalizzare l’allontanamento dalla politica SFT.</li>
</ul>
<p>In pratica, stiamo dicendo: “Vogliamo che il modello linguistico generi risposte che ottengono una ricompensa alta dal modello di ricompensa, ma non vogliamo che si discosti troppo dal modo in cui rispondeva prima (quello SFT)”. È un compromesso tra migliorare la qualità delle risposte e mantenere una certa coerenza con il modello originale.</p></li>
</ol>
<p><strong>Problemi dell’RLHF:</strong></p>
<p>Tutto figo, no? Sì, sulla carta. Ma l’RLHF ha un sacco di problemi, tipo:</p>
<ul>
<li><strong>Complessità:</strong> Sono tre fasi separate, ognuna con i suoi casini.</li>
<li><strong>Instabilità:</strong> L’addestramento con RL è notoriamente instabile. Può succedere che la politica impazzisca e che la ricompensa parta in quinta.</li>
<li><strong>Costo:</strong> Raccogliere dati di preferenza umana costa un sacco di soldi e tempo.</li>
<li><strong>Subottimalità:</strong> Anche se tutto va bene, l’RLHF non è detto che trovi la politica <em>ottimale</em>. Perché? Perché stiamo ottimizzando una funzione obiettivo surrogata (quella roba di prima <span class="math inline">\(J_{RLHF}\)</span>), che non è detto che corrisponda esattamente a quello che vogliamo veramente (ovvero, un chatbot che risponde bene).</li>
</ul>
<p>Insomma, l’RLHF è un po’ come costruire un castello di carte: figo, ma rischia di crollare al primo soffio di vento.</p>
</section>
<section id="dpo-la-figata-che-semplifica-tutto-o-quasi" class="level2">
<h2 class="anchored" data-anchor-id="dpo-la-figata-che-semplifica-tutto-o-quasi">DPO: La Figata che Semplifica Tutto (O Quasi)</h2>
<p>Ed è qui che entra in gioco il <strong>Direct Preference Optimization (DPO)</strong>. Il DPO è una figata perché <strong>elimina la necessità di addestrare esplicitamente un modello di ricompensa</strong>. Avete capito bene: niente più modello di ricompensa, niente più fase 2 dell’RLHF. Solo fase 1 (SFT) e poi si passa direttamente alla fase di ottimizzazione, ma in modo molto più semplice e diretto.</p>
<p>Come fa ’sta magia il DPO? Sfrutta una proprietà matematica del modello di Bradley-Terry. Si può <a href="https://arxiv.org/pdf/2305.18290">dimostrare</a> che la funzione obiettivo dell’RLHF (quella <span class="math inline">\(J_{RLHF}\)</span> di prima) può essere riscritta in una forma molto più semplice, che <strong>dipende solo dalle preferenze umane e non dal modello di ricompensa</strong>.</p>
<p>In pratica, invece di massimizzare la ricompensa predetta dal modello di ricompensa, il DPO massimizza direttamente la probabilità di generare la risposta preferita, date le coppie di preferenze umane. La funzione obiettivo del DPO è questa:</p>
<p><span class="math display">\[J_{DPO}(\pi_\phi) = \mathbb{E}_{(x, y_w, y_l) \sim D_{pref}} [\log \sigma( \beta \log \frac{\pi_\phi(y_w|x)}{\pi_\phi(y_l|x)} )]\]</span></p>
<p>Dove:</p>
<ul>
<li><span class="math inline">\((x, y_w, y_l)\)</span> è una tripla di dati di preferenza: <span class="math inline">\(x\)</span> è la domanda, <span class="math inline">\(y_w\)</span> è la risposta preferita (“winner”), <span class="math inline">\(y_l\)</span> è la risposta non preferita (“loser”).</li>
<li><span class="math inline">\(D_{pref}\)</span> è il dataset di preferenze umane.</li>
<li><span class="math inline">\(\pi_\phi\)</span> è la politica del modello linguistico che stiamo ottimizzando.</li>
<li><span class="math inline">\(\sigma(z) = \frac{1}{1 + e^{-z}}\)</span> è la funzione sigmoide.</li>
<li><span class="math inline">\(\beta\)</span> è un iperparametro (come prima).</li>
</ul>
<p>Ma che cazzo significa ’sta roba? Significa che stiamo cercando di <strong>aumentare la probabilità di generare la risposta preferita (<span class="math inline">\(y_w\)</span>) rispetto alla risposta non preferita (<span class="math inline">\(y_l\)</span>)</strong>, per ogni coppia di preferenze nel dataset <span class="math inline">\(D_{pref}\)</span>. E lo facciamo in modo “diretto”, senza passare per il modello di ricompensa. La funzione logaritmo e la sigmoide servono solo a rendere l’ottimizzazione più stabile e a controllare quanto vogliamo spingere per la risposta preferita.</p>
<p><strong>Vantaggi del DPO:</strong></p>
<ul>
<li><strong>Semplicità:</strong> Molto più semplice dell’RLHF. Niente modello di ricompensa da addestrare, solo una funzione obiettivo da ottimizzare direttamente.</li>
<li><strong>Stabilità:</strong> Più stabile dell’RLHF. L’ottimizzazione è più diretta e meno soggetta a instabilità.</li>
<li><strong>Efficienza:</strong> Più efficiente dell’RLHF. Meno modelli da addestrare, meno risorse computazionali richieste.</li>
<li><strong>Performance comparabili (o migliori) all’RLHF:</strong> In molti casi, il DPO ottiene performance simili o addirittura migliori dell’RLHF, pur essendo molto più semplice.</li>
</ul>
</section>
<section id="limiti" class="level2">
<h2 class="anchored" data-anchor-id="limiti">Limiti</h2>
<p>Ovviamente, anche il DPO non è la cura di tutti i mali. Ha anche lui, ahimé, i suoi limiti:</p>
<ul>
<li><strong>Dipendenza dai dati di preferenza:</strong> Il DPO funziona bene solo se abbiamo dati di preferenza umana di alta qualità e in quantità sufficiente. Se i dati di preferenza sono di scarsa qualità, anche il chatbot addestrato con DPO sarà di scarsa qualità. “Garbage in, garbage out”, come dicono gli americani.</li>
<li><strong>Iperparametri:</strong> Anche il DPO ha i suoi iperparametri da sintonizzare (tipo <span class="math inline">\(\beta\)</span>). Sintonizzare gli iperparametri può essere un casino, soprattutto perché non è detto che esista un valore ottimo per tutto il dataset.</li>
<li><strong>Non è una soluzione magica:</strong> Il DPO è un miglioramento rispetto all’RLHF, ma non risolve tutti i problemi dell’allineamento dei modelli linguistici. Ci sono ancora un sacco di sfide aperte, tipo come garantire che i chatbot siano veramente sicuri, affidabili, etici, eccetera eccetera.</li>
</ul>
<p>Insomma, il DPO è una figata ma c’è ancora un sacco di lavoro da fare.</p>
</section>
<section id="quindi" class="level2">
<h2 class="anchored" data-anchor-id="quindi">Quindi</h2>
<p>Il Direct Preference Optimization (DPO) è una tecnica nuova e promettente per addestrare modelli linguistici che siano allineati con le preferenze umane. È più semplice, stabile ed efficiente dell’RLHF, e ottiene performance comparabili o migliori. Il DPO sta aprendo nuove possibilità per lo sviluppo di chatbot più fighi e accessibili a tutti.</p>
<p>La prossima volta che sentirete parlare di DPO, pensate a ’sta spiegazione (spero) non troppo incasinata, e ricordatevi che dietro ai chatbot che usate tutti i giorni c’è un sacco di matematica, sudore e, a volte, anche qualche parolaccia. E se qualcuno vi chiede “Ma che cazzo è ’sto DPO?” (per qualsiasi motivo questa cosa sia uscita durante una conversazione), mandategli questo articolo!</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/theghoul21\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025, Luca Simonetti</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>