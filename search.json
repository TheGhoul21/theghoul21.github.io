[
  {
    "objectID": "blog/blockchain.html",
    "href": "blog/blockchain.html",
    "title": "Blockchain: Non Solo Criptovalute",
    "section": "",
    "text": "Se non hai sentito parlare di Bitcoin e altre criptovalute vuol dire che fino ad oggi hai vissuto in una caverna, perché i media ormai hanno scartavetrato con questo argomento. Ecco, le criptovalute, spesso, si nascondono dietro a una tecnologia chiamata blockchain. Ma che cavolo è veramente una blockchain? E serve solo alle valute virtuali? In questo articolo voglio addentrarmi un po’ alla materia, scoprendo anche i suoi lati più oscuri e magari meno noti in modo (spero) semplice e chiaro, senza bisogno di essere esperti di informatica o finanza.\n\n\nFirst things first…\nImmaginiamo di avere un registro pubblico, come un grande quaderno condiviso tra tantissime persone. Per esempio uno di questi registri potrebbe essere il vostro conto in banca, o il registro del catasto, o il registro delle nascite di un comune o il vostro personalissimo e dettagliatissimo resoconto delle maleducate che incontrate nei vostri felicissimi sabati sera.\nNormalmente, un registro è gestito da una sola entità, una banca, un governo, voi, ecc. La blockchain invece è diversa: è un registro distribuito.\n\nEcco che comincia: che cazzo vuol dire distribuito??\n\nCalma.\nSignifica che questo “quaderno” non è in un unico posto (la banca, il comune, un server ecc.), ma è copiato e tenuto aggiornato contemporaneamente da migliaia o milioni di computer in tutto il mondo.\n\nEh vabbè capirai, non è già così? Eh? Facebook non ha milioni di computer? EHH??\n\nNon è la stessa cosa. Nel caso di aziende come google, facebook ecc il fatto di avere una copia distribuita dei dati su tanti server fa comunque sempre capo ad un’unica autorità che è appunto la multinazionale. In questo caso parliamo di persone comuni: io, te, la maleducata di venerdì scorso.\n\nScusa, COSA? Tu vorresti distribuire i conti correnti della gente sui computer di tutta la gente? Ma sei rincoglionito?\n\nNo. Calmati. Ci arriviamo.\nIntanto… un altro po’ di nomenclatura.\nOgni “pagina” di questo quaderno è chiamata blocco. In ogni blocco vengono scritte delle transazioni, ovvero delle informazioni. Una volta che una pagina (un blocco) è piena, viene sigillata in modo speciale, usando una specie di “impronta digitale” matematica. Questa impronta digitale è collegata alla pagina precedente, formando una catena (in inglese “chain”) di blocchi: ecco perché si chiama blockchain, “catena di blocchi”.\n\nSì ma perché?\n\nEh perché… Perché se è su tutti i server del mondo, di gente comune, bisogna trovare una maniera per far sì che sia impossibile alterarla. Se qualche malintenzionato avesse il documento sul proprio computer e lo alterasse, comunicando agli altri la modifica potrebbe rubare i soldi della gente per esempio, o appropriarsi della casa di qualcuno. Ma se la pagina è firmata allora è impossibile farlo, perché nessuno riconoscerebbe la nuova firma (la firma non può essere alterata da un solo partecipante, visto che viene generata da tutti).\nRiassunto intermedio:\nDunque, ricapitolandolo:\n\nDistribuita: Non controllata da uno solo, ma da tanti, tutti.\nTrasparente (ma non sempre, ci arriviamo): Tutti i partecipanti (o chi ha accesso alla blockchain, che può essere pubblica o privata) possono vedere le “pagine” (i blocchi) e le transazioni.\nImmutabile: Una volta scritta una “pagina” (blocco) e sigillata, è quasi impossibile modificarla o cancellarla. Questo è dovuto all’uso dell’“impronta digitale” e alla catena di blocchi. Dico quasi impossibile, perché se uno ha una botta di fortuna terrificante potrebbe alterare una pagina. Ma con botta di fortuna, intendo che è più probabile che Salvini diventi comunista.\n\nFacciamo un esempio pratico, che altrimenti vi perdete…\nPensiamo ad un gruppo di amici che vogliono tenere traccia di chi presta soldi a chi. Invece di usare foglietti volanti che si possono perdere o modificare di nascosto, decidono di usare un quaderno pubblico.\n\nQuaderno Pubblico: Mettono un quaderno al centro del tavolo, a disposizione di tutti.\nNuova Transazione (Blocco): Se Mariangela presta 10 euro a Gianfranco, tutti gli amici vedono Mariangela scrivere nel quaderno: “Blocco 1: Mariangela presta 10€ a Gianfranco”. Questo è il primo “blocco”.\nBlocco Successivo: Poi, se Gianfranco restituisce 5 euro a Mariangela, scrivono un nuovo “blocco” sotto il precedente: “Blocco 2: Gianfranco restituisce 5€ a Mariangela”.\nCatena e Impronta Digitale (Semplificata): Immagina che ogni volta che scrivono un blocco, oltre al contenuto della transazione, scrivono anche un breve riassunto del blocco precedente (l’impronta digitale semplificata). Così, il Blocco 2 riassumerebbe il Blocco 1. Se qualcuno volesse modificare il Blocco 1 dopo, dovrebbe cambiare anche il riassunto nel Blocco 2, e poi in tutti i blocchi successivi. Diventa complicatissimo e evidente che c’è stato un tentativo di manipolazione.\n\nIn una blockchain vera, questo “riassunto” (l’“impronta digitale”, tecnicamente chiamata hash) è molto complesso e generato da calcoli matematici avanzati, rendendo le modifiche praticamente impossibili senza essere scoperti.\nL’osservatore più attento (nessuno…) avrà notato una cosa particolare &gt; Ma Luca, perché cazzo dici “praticamente impossibile”? Vuol dire che è possibile?\nSì. Come detto prima la probabilità di riuscirci è talmente bassa che si può dormire sonni molto tranquilli. Nel senso che: è più probabile avere un incidente d’auto, ma vedo che la gente non si barrica in casa per questo motivo.\n\n\n\nOk, le criptovalute come Bitcoin usano la blockchain. Ma la blockchain è solo criptovaluta o c’è altro? Beh che domanda del cavolo… è scritto nel titolo Luca.\nUn paio di esempi pratici e concreti:\n\nTracciamento della Filiera Alimentare e Prodotti: Immagina di comprare una bottiglia d’olio d’oliva. Grazie alla blockchain, potresti scansionare un codice sulla bottiglia e vedere tutta la storia di quell’olio: da quale uliveto provengono le olive, in quale frantoio sono state lavorate, quando è stata imbottigliata, quando è arrivata al supermercato. Questo rende la filiera più trasparente e aiuta a combattere frodi e prodotti contraffatti. Funziona per il cibo, ma anche per vestiti, medicine, pezzi di ricambio per auto… qualsiasi cosa compriate per soddisfare i vostri desideri animali. Putridi bestie.\nGestione dell’Identità Digitale: Oggi abbiamo tantissime password e identità digitali sparse ovunque (social media, banche, siti web…). La blockchain potrebbe permettere di creare un’unica identità digitale sicura e controllata da noi stessi. Invece di dare i nostri dati a ogni sito, potremmo usare la nostra “identità blockchain” per autenticarci, scegliendo quali informazioni condividere e con chi.\nContratti Intelligenti (Smart Contracts): Sono programmi che si attivano automaticamente quando si verificano certe condizioni. Immagina per esempio che l’olivaio di prima si accorda con il frantumaio. Gli dice: “ascolta capo, quando mi arrivano i prossimi 3 quintali di olive te li mando per 200 euro, fatta?” E il frantumaio che sa che l’olivaio è un gran raccontaballe gli dice “eh no capo… che l’ultima volta non mi hai mandato un bel niente”. Allora stipulano uno smart contract che si attiva in automatico, non appena arrivano le olive vengono consegnate. Nello smart contract potrebbe per esempio essere inclusa una penalità per cui se l’olivaio si rifiuta di consegnare le olive paga in automatico una multa. Non una multa che dice “sì vabbè sticazzi hai capito tutto, non pago niente” bensì una multa che viene prelevata in automatico dal conto del signor olivaio.\n\nQuesti sono solo alcuni esempi. Le applicazioni della blockchain sono potenzialmente infinite e ne vengono fuori ogni giorno come i funghi.\n\n\n\nCome ogni tecnologia potente, ahimé, anche la blockchain ha i suoi lati negativi e presenta delle sfide. È importante conoscerli per avere un quadro completo e non farsi abbagliare solo dai vantaggi.\n\nDecentralizzazione e Attività Illegali: Uno dei punti di forza della blockchain, la decentralizzazione, può anche essere un punto debole. Se non c’è un’autorità centrale di controllo, diventa più difficile intervenire contro attività illegali. Per esempio:\n\nCrimine Organizzato: Organizzazioni criminali potrebbero usare blockchain e criptovalute per riciclare denaro sporco, finanziare attività illegali, o condurre transazioni non tracciabili. Nessuno saprebbe mai chi ha fatto che cosa.\nEvasione Fiscale: È più complicato per le autorità fiscali monitorare transazioni che avvengono al di fuori dei sistemi bancari tradizionali. Ma se l’intero sistema fosse gestito da blockchain sarebbe impossibile evadere le tasse visto che ogni transazione sarebbe tracciata e avrebbe delle fee.\nMercati Neri Online: Le blockchain possono facilitare la creazione di mercati online per beni e servizi illegali, come droghe o armi, rendendo più difficile per le forze dell’ordine smantellarli.\n\nOVVIAMENTE: Questo non significa che la blockchain è fatta per il crimine. Significa che, come qualsiasi strumento, può essere usata per scopi positivi e negativi. È come internet: è una risorsa fantastica se vi mandate le foto di gattini e i reel della gente che fa incidenti stradali. Ma può essere fonte di disinformazione oltre che dei reel di Salvini che mangia le ciliegie (oggi ce l’ho con Salvini perdonatemi).\nMancanza di Regolamentazione: Essendo una tecnologia decentralizzata, la blockchain e le criptovalute non sono regolate. Questo può creare problemi:\n\nProtezione dei Consumatori: Se investi in criptovalute e perdi tutto, spesso non c’è un’autorità a cui puoi rivolgerti per essere risarcito. Mancano le tutele che esistono nel sistema finanziario tradizionale.\nInstabilità e Speculazione: Il valore delle criptovalute è estremamente volatile, con forti oscillazioni di prezzo. Questo le rende rischiose per gli investimenti e può creare bolle speculative.\n\nScalabilità e Consumo Energetico: Alcune blockchain, come quella di Bitcoin, hanno problemi di scalabilità. Significa che possono gestire un numero limitato di transazioni al secondo. Se troppe persone vogliono usare la blockchain nello stesso momento, le transazioni possono diventare lente e costose. Inoltre, alcune blockchain (come ancora Bitcoin, anche se ci sono alternative più efficienti) consumano molta energia elettrica per funzionare, il che ha un impatto ambientale negativo. Ovviamente questo non vale per tutte le blockchain: sono nati nel tempo metodi decisamente più economici e meno dispendiosi in termini di energia consumata che risolvono parzialmente o totalmente questi problemi.\nRischi per la Sicurezza: Anche se la blockchain è progettata per essere sicura, non è immune da attacchi.\n\nAttacchi “51%”: In alcune blockchain, se un singolo soggetto riuscisse a controllare più del 50% della potenza di calcolo della rete, potrebbe teoricamente manipolare le transazioni. Anche se molto difficile, questo tipo di attacco è un rischio teorico che deve sempre e comunque essere preso in considerazione. Ad esempio una soluzione è garantirsi di avere il controllo sul 51% fitantoché la diffusione della blockchain è alta a sufficienza da mitigare i rischi.\nSicurezza dei “Portafogli” Digitali (Wallet): Per usare criptovalute e interagire con la blockchain, si usano dei “portafogli” digitali. Se questi portafogli non sono protetti adeguatamente (es. password deboli, phishing), si rischia di perdere i propri fondi. O anche (ed è capitato… ) non essendoci un’autorità centrale a cui fare riferimento se perdete la vostra “password” siete fotttuti. Se vi dimenticate la password dell’account facebook dove chattate cone le escort, chiedete a facebook il reset della password e facebook gestisce tutto. Nel mondo blockchain se perdete la vostra chiave privata (\\(\\approx\\) password), basta, that’s it, siete tagliati fuori dal vostro portafoglio. Anche per questo punto ci sono soluzioni come i portafogli gestiti da autorità centrali. Il che però fa perdere un po’ di senso alla blokchain.\n\n\n\n\n\nLa blockchain è una tecnologia potenzialmente rivoluzionaria. Non è assolutamente sinonimo di “criptovalute” e potrebbe trasformare molti settori attualmente gestiti in maniera tradizionale, dalla finanza alla logistica, dalla sanità alla pubblica amministrazione. Offre vantaggi importanti come la trasparenza, la sicurezza, la decentralizzazione e l’immutabilità dei dati.\nPerò, è fondamentale essere consapevoli anche dei rischi che comporta. La decentralizzazione, che è un punto di forza, può anche essere usata per attività illegali. La mancanza di regolamentazione e la complessità tecnica sono ostacoli da superare. Come per ogni innovazione, è importante un approccio equilibrato: sfruttare le potenzialità della blockchain, affrontando i rischi con consapevolezza e lavorando per creare un futuro digitale più sicuro e trasparente per tutti.\nLa blockchain è ancora in fase di sviluppo. Il futuro dipenderà da come sapremo guidare questa tecnologia e farla evolvere in modo responsabile e sostenibile, massimizzando i benefici e minimizzando i rischi.\n\n\n\nUn’ultima riflessione. Avrai notato che io, Luca Simonetti, sono l’autore di questo articolo. L’ho scritto, l’ho pubblicato (ipoteticamente, eh!). Ora, immagina che tra un’ora io veda la foto della tipa mezza gnuda su instagram, o che cambi idea su qualcosa che ho scritto, o che semplicemente trovi un errore di battitura che mi infastidisce. Posso tranquillamente tornare qui e modificare il cazzo che voglio. Posso cambiare intere frasi, aggiungere o togliere paragrafi, stravolgere il senso di quello che ho detto. E nessuno potrebbe contestare più di tanto la cosa, perché l’autorità che gestisce questo contenuto (io, in questo caso) ho il pieno controllo e posso cambiarlo a mio piacimento.\nChi ha letto questo articolo ora, e magari lo rilegge tra qualche ora, potrebbe trovarlo diverso. Non c’è un modo per dimostrare quale fosse la versione originale, se non affidandosi alla mia parola (o a servizi esterni che archiviano pagine web, ma che sono comunque “esterni” e non parte integrante dell’articolo stesso).\nEcco, questa è la grande differenza con un’informazione registrata su una blockchain. Una volta che un blocco di informazioni viene scritto e “sigillato” sulla catena, non può essere modificato retroattivamente in modo furtivo. Ogni modifica lascerebbe una traccia evidente e la versione originale resterebbe sempre verificabile e consultabile da chiunque. Questo articolo, invece, è soggetto al capriccio del suo autore (ovvero il mio!) e alla natura effimera del web tradizionale. Un piccolo esempio concreto per capire ancora meglio la potenza (e la differenza!) della blockchain.\nSpoiler: questo ultimo paragrafo l’ho aggiunto dopo aver pubblicato la versione originale dell’articolo. E se nesusno l’avesse letto prima e se io non avessi scritto questa roba qui, nessuno l’avrebbe mai saputo."
  },
  {
    "objectID": "blog/blockchain.html#che-cosè-in-parole-semplici-una-blockchain",
    "href": "blog/blockchain.html#che-cosè-in-parole-semplici-una-blockchain",
    "title": "Blockchain: Non Solo Criptovalute",
    "section": "",
    "text": "First things first…\nImmaginiamo di avere un registro pubblico, come un grande quaderno condiviso tra tantissime persone. Per esempio uno di questi registri potrebbe essere il vostro conto in banca, o il registro del catasto, o il registro delle nascite di un comune o il vostro personalissimo e dettagliatissimo resoconto delle maleducate che incontrate nei vostri felicissimi sabati sera.\nNormalmente, un registro è gestito da una sola entità, una banca, un governo, voi, ecc. La blockchain invece è diversa: è un registro distribuito.\n\nEcco che comincia: che cazzo vuol dire distribuito??\n\nCalma.\nSignifica che questo “quaderno” non è in un unico posto (la banca, il comune, un server ecc.), ma è copiato e tenuto aggiornato contemporaneamente da migliaia o milioni di computer in tutto il mondo.\n\nEh vabbè capirai, non è già così? Eh? Facebook non ha milioni di computer? EHH??\n\nNon è la stessa cosa. Nel caso di aziende come google, facebook ecc il fatto di avere una copia distribuita dei dati su tanti server fa comunque sempre capo ad un’unica autorità che è appunto la multinazionale. In questo caso parliamo di persone comuni: io, te, la maleducata di venerdì scorso.\n\nScusa, COSA? Tu vorresti distribuire i conti correnti della gente sui computer di tutta la gente? Ma sei rincoglionito?\n\nNo. Calmati. Ci arriviamo.\nIntanto… un altro po’ di nomenclatura.\nOgni “pagina” di questo quaderno è chiamata blocco. In ogni blocco vengono scritte delle transazioni, ovvero delle informazioni. Una volta che una pagina (un blocco) è piena, viene sigillata in modo speciale, usando una specie di “impronta digitale” matematica. Questa impronta digitale è collegata alla pagina precedente, formando una catena (in inglese “chain”) di blocchi: ecco perché si chiama blockchain, “catena di blocchi”.\n\nSì ma perché?\n\nEh perché… Perché se è su tutti i server del mondo, di gente comune, bisogna trovare una maniera per far sì che sia impossibile alterarla. Se qualche malintenzionato avesse il documento sul proprio computer e lo alterasse, comunicando agli altri la modifica potrebbe rubare i soldi della gente per esempio, o appropriarsi della casa di qualcuno. Ma se la pagina è firmata allora è impossibile farlo, perché nessuno riconoscerebbe la nuova firma (la firma non può essere alterata da un solo partecipante, visto che viene generata da tutti).\nRiassunto intermedio:\nDunque, ricapitolandolo:\n\nDistribuita: Non controllata da uno solo, ma da tanti, tutti.\nTrasparente (ma non sempre, ci arriviamo): Tutti i partecipanti (o chi ha accesso alla blockchain, che può essere pubblica o privata) possono vedere le “pagine” (i blocchi) e le transazioni.\nImmutabile: Una volta scritta una “pagina” (blocco) e sigillata, è quasi impossibile modificarla o cancellarla. Questo è dovuto all’uso dell’“impronta digitale” e alla catena di blocchi. Dico quasi impossibile, perché se uno ha una botta di fortuna terrificante potrebbe alterare una pagina. Ma con botta di fortuna, intendo che è più probabile che Salvini diventi comunista.\n\nFacciamo un esempio pratico, che altrimenti vi perdete…\nPensiamo ad un gruppo di amici che vogliono tenere traccia di chi presta soldi a chi. Invece di usare foglietti volanti che si possono perdere o modificare di nascosto, decidono di usare un quaderno pubblico.\n\nQuaderno Pubblico: Mettono un quaderno al centro del tavolo, a disposizione di tutti.\nNuova Transazione (Blocco): Se Mariangela presta 10 euro a Gianfranco, tutti gli amici vedono Mariangela scrivere nel quaderno: “Blocco 1: Mariangela presta 10€ a Gianfranco”. Questo è il primo “blocco”.\nBlocco Successivo: Poi, se Gianfranco restituisce 5 euro a Mariangela, scrivono un nuovo “blocco” sotto il precedente: “Blocco 2: Gianfranco restituisce 5€ a Mariangela”.\nCatena e Impronta Digitale (Semplificata): Immagina che ogni volta che scrivono un blocco, oltre al contenuto della transazione, scrivono anche un breve riassunto del blocco precedente (l’impronta digitale semplificata). Così, il Blocco 2 riassumerebbe il Blocco 1. Se qualcuno volesse modificare il Blocco 1 dopo, dovrebbe cambiare anche il riassunto nel Blocco 2, e poi in tutti i blocchi successivi. Diventa complicatissimo e evidente che c’è stato un tentativo di manipolazione.\n\nIn una blockchain vera, questo “riassunto” (l’“impronta digitale”, tecnicamente chiamata hash) è molto complesso e generato da calcoli matematici avanzati, rendendo le modifiche praticamente impossibili senza essere scoperti.\nL’osservatore più attento (nessuno…) avrà notato una cosa particolare &gt; Ma Luca, perché cazzo dici “praticamente impossibile”? Vuol dire che è possibile?\nSì. Come detto prima la probabilità di riuscirci è talmente bassa che si può dormire sonni molto tranquilli. Nel senso che: è più probabile avere un incidente d’auto, ma vedo che la gente non si barrica in casa per questo motivo."
  },
  {
    "objectID": "blog/blockchain.html#blockchain-non-solo-criptovalute-1",
    "href": "blog/blockchain.html#blockchain-non-solo-criptovalute-1",
    "title": "Blockchain: Non Solo Criptovalute",
    "section": "",
    "text": "Ok, le criptovalute come Bitcoin usano la blockchain. Ma la blockchain è solo criptovaluta o c’è altro? Beh che domanda del cavolo… è scritto nel titolo Luca.\nUn paio di esempi pratici e concreti:\n\nTracciamento della Filiera Alimentare e Prodotti: Immagina di comprare una bottiglia d’olio d’oliva. Grazie alla blockchain, potresti scansionare un codice sulla bottiglia e vedere tutta la storia di quell’olio: da quale uliveto provengono le olive, in quale frantoio sono state lavorate, quando è stata imbottigliata, quando è arrivata al supermercato. Questo rende la filiera più trasparente e aiuta a combattere frodi e prodotti contraffatti. Funziona per il cibo, ma anche per vestiti, medicine, pezzi di ricambio per auto… qualsiasi cosa compriate per soddisfare i vostri desideri animali. Putridi bestie.\nGestione dell’Identità Digitale: Oggi abbiamo tantissime password e identità digitali sparse ovunque (social media, banche, siti web…). La blockchain potrebbe permettere di creare un’unica identità digitale sicura e controllata da noi stessi. Invece di dare i nostri dati a ogni sito, potremmo usare la nostra “identità blockchain” per autenticarci, scegliendo quali informazioni condividere e con chi.\nContratti Intelligenti (Smart Contracts): Sono programmi che si attivano automaticamente quando si verificano certe condizioni. Immagina per esempio che l’olivaio di prima si accorda con il frantumaio. Gli dice: “ascolta capo, quando mi arrivano i prossimi 3 quintali di olive te li mando per 200 euro, fatta?” E il frantumaio che sa che l’olivaio è un gran raccontaballe gli dice “eh no capo… che l’ultima volta non mi hai mandato un bel niente”. Allora stipulano uno smart contract che si attiva in automatico, non appena arrivano le olive vengono consegnate. Nello smart contract potrebbe per esempio essere inclusa una penalità per cui se l’olivaio si rifiuta di consegnare le olive paga in automatico una multa. Non una multa che dice “sì vabbè sticazzi hai capito tutto, non pago niente” bensì una multa che viene prelevata in automatico dal conto del signor olivaio.\n\nQuesti sono solo alcuni esempi. Le applicazioni della blockchain sono potenzialmente infinite e ne vengono fuori ogni giorno come i funghi."
  },
  {
    "objectID": "blog/blockchain.html#non-è-tutto-oro-ciò-che-luccica.",
    "href": "blog/blockchain.html#non-è-tutto-oro-ciò-che-luccica.",
    "title": "Blockchain: Non Solo Criptovalute",
    "section": "",
    "text": "Come ogni tecnologia potente, ahimé, anche la blockchain ha i suoi lati negativi e presenta delle sfide. È importante conoscerli per avere un quadro completo e non farsi abbagliare solo dai vantaggi.\n\nDecentralizzazione e Attività Illegali: Uno dei punti di forza della blockchain, la decentralizzazione, può anche essere un punto debole. Se non c’è un’autorità centrale di controllo, diventa più difficile intervenire contro attività illegali. Per esempio:\n\nCrimine Organizzato: Organizzazioni criminali potrebbero usare blockchain e criptovalute per riciclare denaro sporco, finanziare attività illegali, o condurre transazioni non tracciabili. Nessuno saprebbe mai chi ha fatto che cosa.\nEvasione Fiscale: È più complicato per le autorità fiscali monitorare transazioni che avvengono al di fuori dei sistemi bancari tradizionali. Ma se l’intero sistema fosse gestito da blockchain sarebbe impossibile evadere le tasse visto che ogni transazione sarebbe tracciata e avrebbe delle fee.\nMercati Neri Online: Le blockchain possono facilitare la creazione di mercati online per beni e servizi illegali, come droghe o armi, rendendo più difficile per le forze dell’ordine smantellarli.\n\nOVVIAMENTE: Questo non significa che la blockchain è fatta per il crimine. Significa che, come qualsiasi strumento, può essere usata per scopi positivi e negativi. È come internet: è una risorsa fantastica se vi mandate le foto di gattini e i reel della gente che fa incidenti stradali. Ma può essere fonte di disinformazione oltre che dei reel di Salvini che mangia le ciliegie (oggi ce l’ho con Salvini perdonatemi).\nMancanza di Regolamentazione: Essendo una tecnologia decentralizzata, la blockchain e le criptovalute non sono regolate. Questo può creare problemi:\n\nProtezione dei Consumatori: Se investi in criptovalute e perdi tutto, spesso non c’è un’autorità a cui puoi rivolgerti per essere risarcito. Mancano le tutele che esistono nel sistema finanziario tradizionale.\nInstabilità e Speculazione: Il valore delle criptovalute è estremamente volatile, con forti oscillazioni di prezzo. Questo le rende rischiose per gli investimenti e può creare bolle speculative.\n\nScalabilità e Consumo Energetico: Alcune blockchain, come quella di Bitcoin, hanno problemi di scalabilità. Significa che possono gestire un numero limitato di transazioni al secondo. Se troppe persone vogliono usare la blockchain nello stesso momento, le transazioni possono diventare lente e costose. Inoltre, alcune blockchain (come ancora Bitcoin, anche se ci sono alternative più efficienti) consumano molta energia elettrica per funzionare, il che ha un impatto ambientale negativo. Ovviamente questo non vale per tutte le blockchain: sono nati nel tempo metodi decisamente più economici e meno dispendiosi in termini di energia consumata che risolvono parzialmente o totalmente questi problemi.\nRischi per la Sicurezza: Anche se la blockchain è progettata per essere sicura, non è immune da attacchi.\n\nAttacchi “51%”: In alcune blockchain, se un singolo soggetto riuscisse a controllare più del 50% della potenza di calcolo della rete, potrebbe teoricamente manipolare le transazioni. Anche se molto difficile, questo tipo di attacco è un rischio teorico che deve sempre e comunque essere preso in considerazione. Ad esempio una soluzione è garantirsi di avere il controllo sul 51% fitantoché la diffusione della blockchain è alta a sufficienza da mitigare i rischi.\nSicurezza dei “Portafogli” Digitali (Wallet): Per usare criptovalute e interagire con la blockchain, si usano dei “portafogli” digitali. Se questi portafogli non sono protetti adeguatamente (es. password deboli, phishing), si rischia di perdere i propri fondi. O anche (ed è capitato… ) non essendoci un’autorità centrale a cui fare riferimento se perdete la vostra “password” siete fotttuti. Se vi dimenticate la password dell’account facebook dove chattate cone le escort, chiedete a facebook il reset della password e facebook gestisce tutto. Nel mondo blockchain se perdete la vostra chiave privata (\\(\\approx\\) password), basta, that’s it, siete tagliati fuori dal vostro portafoglio. Anche per questo punto ci sono soluzioni come i portafogli gestiti da autorità centrali. Il che però fa perdere un po’ di senso alla blokchain."
  },
  {
    "objectID": "blog/blockchain.html#quindi",
    "href": "blog/blockchain.html#quindi",
    "title": "Blockchain: Non Solo Criptovalute",
    "section": "",
    "text": "La blockchain è una tecnologia potenzialmente rivoluzionaria. Non è assolutamente sinonimo di “criptovalute” e potrebbe trasformare molti settori attualmente gestiti in maniera tradizionale, dalla finanza alla logistica, dalla sanità alla pubblica amministrazione. Offre vantaggi importanti come la trasparenza, la sicurezza, la decentralizzazione e l’immutabilità dei dati.\nPerò, è fondamentale essere consapevoli anche dei rischi che comporta. La decentralizzazione, che è un punto di forza, può anche essere usata per attività illegali. La mancanza di regolamentazione e la complessità tecnica sono ostacoli da superare. Come per ogni innovazione, è importante un approccio equilibrato: sfruttare le potenzialità della blockchain, affrontando i rischi con consapevolezza e lavorando per creare un futuro digitale più sicuro e trasparente per tutti.\nLa blockchain è ancora in fase di sviluppo. Il futuro dipenderà da come sapremo guidare questa tecnologia e farla evolvere in modo responsabile e sostenibile, massimizzando i benefici e minimizzando i rischi."
  },
  {
    "objectID": "blog/blockchain.html#post-scriptum-e-questo-articolo",
    "href": "blog/blockchain.html#post-scriptum-e-questo-articolo",
    "title": "Blockchain: Non Solo Criptovalute",
    "section": "",
    "text": "Un’ultima riflessione. Avrai notato che io, Luca Simonetti, sono l’autore di questo articolo. L’ho scritto, l’ho pubblicato (ipoteticamente, eh!). Ora, immagina che tra un’ora io veda la foto della tipa mezza gnuda su instagram, o che cambi idea su qualcosa che ho scritto, o che semplicemente trovi un errore di battitura che mi infastidisce. Posso tranquillamente tornare qui e modificare il cazzo che voglio. Posso cambiare intere frasi, aggiungere o togliere paragrafi, stravolgere il senso di quello che ho detto. E nessuno potrebbe contestare più di tanto la cosa, perché l’autorità che gestisce questo contenuto (io, in questo caso) ho il pieno controllo e posso cambiarlo a mio piacimento.\nChi ha letto questo articolo ora, e magari lo rilegge tra qualche ora, potrebbe trovarlo diverso. Non c’è un modo per dimostrare quale fosse la versione originale, se non affidandosi alla mia parola (o a servizi esterni che archiviano pagine web, ma che sono comunque “esterni” e non parte integrante dell’articolo stesso).\nEcco, questa è la grande differenza con un’informazione registrata su una blockchain. Una volta che un blocco di informazioni viene scritto e “sigillato” sulla catena, non può essere modificato retroattivamente in modo furtivo. Ogni modifica lascerebbe una traccia evidente e la versione originale resterebbe sempre verificabile e consultabile da chiunque. Questo articolo, invece, è soggetto al capriccio del suo autore (ovvero il mio!) e alla natura effimera del web tradizionale. Un piccolo esempio concreto per capire ancora meglio la potenza (e la differenza!) della blockchain.\nSpoiler: questo ultimo paragrafo l’ho aggiunto dopo aver pubblicato la versione originale dell’articolo. E se nesusno l’avesse letto prima e se io non avessi scritto questa roba qui, nessuno l’avrebbe mai saputo."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "L’s blog",
    "section": "",
    "text": "Hashing: Che Cos’è ’sto Hashing?\n\n\n\n\n\n\ncomputer science\n\n\n\n\n\n\n\n\n\nFeb 7, 2025\n\n\nLuca Simonetti\n\n\n\n\n\n\n\n\n\n\n\n\nBlockchain: Non Solo Criptovalute\n\n\n\n\n\n\ncomputer science\n\n\n\n\n\n\n\n\n\nFeb 2, 2025\n\n\nLuca Simonetti\n\n\n\n\n\n\n\n\n\n\n\n\nGruppi di Ordine Primo\n\n\n\n\n\n\nmath\n\n\ncryptography\n\n\nalgebra\n\n\n\n\n\n\n\n\n\nFeb 1, 2025\n\n\nLuca Simonetti\n\n\n\n\n\n\n\n\n\n\n\n\nShamir’s Secret Sharing\n\n\n\n\n\n\nprogramming\n\n\nweb development\n\n\n\n\n\n\n\n\n\nJan 30, 2025\n\n\nLuca Simonetti\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/hashing.html",
    "href": "blog/hashing.html",
    "title": "Hashing: Che Cos’è ’sto Hashing?",
    "section": "",
    "text": "Se hai mai sentito parlare di password, sicurezza informatica o anche solo di blockchain (di cui abbiamo parlato la settimana scorsa, se te lo sei perso sei un pirla), probabilmente ti sei imbattuto in questa parola: hashing. Ma che cavolo è sta roba? Sembra il nome di una nuova droga sintetica o di un ballo di gruppo degli anni ’80. In realtà, è una figata (nel senso buono, non come quando ti dicono “che figata le tue Crocs 😐”). In questo articolo voglio spiegare (spero) in parole semplici cosa è l’hashing, perché è importante e perché conviene smettere di usare password di merda come “password123”.\n\n\nPartiamo dalle basi, come quando a scuola di cucina ti spiegano come si le uova fritte (eppure, ahimé, anche riesce a fare un disastro…).\nImmagina di avere una di quelle macchinette per fare la carne trita, hai presente? Ci metti dentro un pezzo di carne, giri la manovella e dall’altra parte esce carne trita. Ecco, l’hashing è un po’ come quella macchinetta, ma per i dati.\n\nAh, figo, quindi se ci metto dentro un PDF esce… un PDF tritato?\n\nMinchia, sei simpatico, ti ucciderò per ultimo. No, non proprio. Non trita i dati nel senso che li spezzetta. Ma in un certo senso li trasforma. La macchinetta dell’hashing prende qualsiasi tipo di dato (un testo, un’immagine, un video, un file intero, la tua lista di maleducate) e lo trasforma in una stringa (una sequnza, ma in gergo si chiama stringa) di caratteri di lunghezza fissa, che all’apparenza è completamente casuale. Questa stringa (sequenza) di caratteri è chiamata hash (o “valore hash”, o “digest”, insomma, chiamala come cazzo ti pare, basta che ci capiamo).\n\nOk, ok, ci sono quasi. Ma perché dovrebbe servirmi trasformare i miei dati in una stringa di caratteri a caso? Che me ne faccio? Ci tappezzo il cesso?\n\nCalma, non fare il fenomeno, che cazzo hai mangiato a colazione?! L’hash ha delle proprietà molto particolari che lo rendono decisamente utile. In ordine (o no?):\n\nÈ Deterministico: Se prendi lo stesso dato e lo passi alla stessa “macchinetta” (funzione di hash), otterrai sempre lo stesso identico hash. Sempre. Non importa quante volte lo fai, il risultato sarà sempre lo stesso. È come se la macchinetta avesse una memoria di ferro.\n\nQuindi se faccio l’hash della parola “ciao” ottengo sempre lo stesso hash?\n\nEsatto. Prova. Ci sono un sacco di siti online che fanno hash. Cerca “online hash calculator”, mettici “ciao” e usa l’algoritmo SHA-256 (uno dei più comuni). Vedrai che ti uscirà sempre la stessa stringa di caratteri (tipo b133a0c0e9bee3be20163d2ad31d6248db292aa6dcb1ee087a2aa50e0fc75ae2, ma non ti fissare con questa stringa, è solo un esempio).\nÈ One-Way (A Senso Unico): Questa è la parte veramente interessante e importante. È praticamente impossibile (e dico praticamente perché nella vita nulla è impossibile al 100%, tranne forse te che te ne stai a casa il sabato sera invece di andare a far baldoria con Salvini) tornare indietro dall’hash al dato originale. Cioè, se hai l’hash, non puoi in alcun modo (se non per forza bruta) ricostruire il dato di partenza. È come la macchinetta della carne trita: puoi fare la carne trita dalla carne, ma non puoi rifare il pezzo di carne dalla carne trita. Chiaro no?\n\nSì, sì, ho capito. Quindi se io faccio l’hash della mia password “password123”, anche se qualcuno ruba l’hash, non può risalire alla mia password?\n\nYES! Finalmente hai detto una roba giusta, porca troia. Ed è proprio per questo che l’hashing è fondamentale per la sicurezza delle password. Quando ti registri su un sito web e scegli una password, il sito non salva la tua password originale (sarebbe da cretini, se lo fanno prendi un martello). Bensì, salva solo l’hash della tua password. Quando poi fai il login e inserisci la tua password, il sito calcola l’hash della password che hai inserito e lo confronta con l’hash che ha salvato. Se i due hash coincidono, significa che hai inserito la password giusta, senza che il sito abbia mai dovuto conoscere la tua password in chiaro. Così l’unico che in teoria sa la tua password sei tu (e la tua morosa…).\nÈ Resistente alle Collisioni (Idealmente, come sempre): Questa è un po’ più complicata, ma cercherò di semplificare al massimo. Idealmente (ovvero nei mondi con le fate, le lucine eccetera), è estremamente improbabile che due dati diversi producano lo stesso hash. Si chiama “collisione” quando due dati diversi producono lo stesso hash. Una buona funzione di hash è progettata per rendere le collisioni praticamente impossibili (di nuovo, praticamente). È come dire che è quasi impossibile che due pezzi di carne diversi, messi nella stessa macchinetta, producano esattamente la stessa carne trita. Capito no? Più o meno dai…\n\nPiù o meno… Ma quindi le collisioni possono succedere?\n\nSì, in teoria (e badate, che le cose che possono succedere in teoria anche con probabilità infime sono tante. Tipo che morite in un incidente d’auto, così, per dire.) possono succedere, ma con le funzioni di hash moderne e robuste, la probabilità è talmente bassa che è come vincere la lotteria 10 volte di fila mentre ti cade un meteorite in testa e Salvini dice una roba sensata. Insomma, per scopi pratici, possiamo dire che le collisioni sono trascurabili. Esistono però funzioni di hash più vecchie e meno sicure che sono più vulnerabili alle collisioni, ma c’è già troppa carne al fuoco (pun intended).\n\nRicapitolandolo:\nQuindi, ricapitolando, l’hashing è:\n\nUna macchinetta per i dati: Prende qualsiasi dato e lo trasforma in una stringa di caratteri di lunghezza fissa (hash).\nDeterministico: Stesso dato, stesso hash, sempre.\nOne-Way: Impossibile (praticamente) tornare indietro dall’hash al dato originale.\nResistente alle Collisioni: Improbabile (molto improbabile) che due dati diversi producano lo stesso hash.\n\n\n\n\nOk, abbiamo capito che cos’è l’hashing e le sue proprietà magiche. Ma in pratica, a cosa serve? Oltre a proteggere le password (che è già una figata non da poco), l’hashing ha un sacco di altre applicazioni. Eccone alcune:\n\nVerifica dell’Integrità dei Dati: Immagina di scaricare un file enorme da internet, tipo un film in HD o un videogioco. Come fai a essere sicuro che il file che hai scaricato sia integro e non sia stato corrotto durante il download? Semplice: il sito web da cui hai scaricato il file di solito fornisce anche l’hash del file. Tu, dopo aver scaricato il file, puoi calcolare l’hash del file scaricato e confrontarlo con l’hash fornito dal sito. Se i due hash coincidono, significa che il file è integro al 100%. Se non coincidono, significa che c’è stato un problema durante il download e il file è corrotto (o peggio, potrebbe essere stato manomesso da qualcuno). È come avere un sigillo di garanzia digitale sul file. Figata, no?\nBlockchain e Criptovalute: Se hai letto l’articolo sulla blockchain (e se non l’hai fatto, vergognati, tanto.), sai che l’hashing è un ingrediente fondamentale della blockchain. Viene usato per creare l’“impronta digitale” di ogni blocco e per collegare i blocchi in una catena immutabile. Senza l’hashing, la blockchain non esisterebbe. Quindi, se ti piacciono le criptovalute (o se le odi, non importa), sappi che devi ringraziare (o maledire) l’hashing.\nFirme Digitali: L’hashing è anche usato nelle firme digitali per garantire l’autenticità e l’integrità dei documenti digitali. Quando firmi digitalmente un documento, in realtà stai firmando l’hash del documento, non il documento intero. Questo rende la firma molto più efficiente e sicura.\nRicerca Efficiente di Dati: L’hashing può essere usato per creare delle “tabelle hash” (o “hash map”), che sono delle strutture dati che permettono di cercare dati in modo molto veloce. È un po’ come avere un indice di un libro: invece di dover sfogliare tutto il libro per trovare un’informazione, puoi consultare l’indice e trovarla subito.\n\n\n\n\nAnche l’hashing, come tutte le cose belle della vita, tipo me, ha i suoi limiti:\n\nCollisioni (anche se improbabili): Abbiamo detto che le collisioni sono molto improbabili, ma non impossibili. In teoria, un attaccante potrebbe cercare di trovare due dati diversi che producono lo stesso hash (un “attacco di collisione”). Se ci riuscisse, potrebbe usare questa collisione per scopi malevoli, tipo sostituire un file legittimo con uno fasullo che ha lo stesso hash. Per fortuna, trovare collisioni per le funzioni di hash moderne e robuste è estremamente difficile e costoso, ma è un rischio teorico da tenere presente.\nAttacchi di “Rainbow Table” e “Brute Force” alle Password: Anche se l’hashing rende difficile risalire alla password originale dall’hash, non è una protezione infallibile al 100%. Esistono tecniche come gli attacchi di “rainbow table” e “brute force” che possono essere usate per cercare di “indovinare” le password a partire dagli hash. Le “rainbow table” sono delle tabelle precalcolate che contengono gli hash di milioni di password comuni. Gli attacchi “brute force” invece consistono nel provare a calcolare l’hash di tutte le possibili password finché non si trova una corrispondenza con l’hash rubato. Per difendersi da questi attacchi, è fondamentale usare password lunghe, complesse e diverse per ogni sito. E smettetela di usare “password123”, che cazzo!\nNon è Crittografia: È importante capire che l’hashing non è crittografia. La crittografia serve per nascondere i dati, rendendoli illeggibili a chi non ha la chiave giusta. L’hashing serve per creare un’impronta digitale dei dati, per verificarne l’integrità e l’autenticità. E’ importante non confondere le due cose. È come confondere un lucchetto (crittografia) con un sigillo di ceralacca (hashing). Entrambi servono per la sicurezza, ma in modi diversi.\n\n\n\n\nL’hashing è una tecnologia che sta alla base nel mondo digitale di oggi e di un sacco di cose che usiamo tutti i giorni, dalle password alla blockchain, dalla verifica dell’integrità dei file alle firme digitali. È una tecnologia potente e versatile, ma è importante conoscerne anche i limiti e le potenziali vulnerabilità.\nLa prossima volta che sentirai parlare di hashing pensa alla macchinetta per la carne trita dei dati, alla one-way, e alla sicurezza che può garantire. E soprattutto, smettila di usare password di merda! Grazie per l’attenzione e alla prossima settimana con un altro argomento nerd e (spero) interessante.\n\n\n\nGiusto per fare i fighi e per farti capire ancora meglio la potenza dell’hashing, ho calcolato l’hash SHA-256 di questo articolo (in formato testo semplice, senza HTML o altre formattazioni). Ecco l’hash:\n31e452bbf5134ac1d0d0a3a5897359b83eaccf6a46aed7d4323ddb51ec2d76f0\nOra, se io modificassi anche solo una virgola di questo articolo, anche solo uno spazio bianco, anche solo una lettera, l’hash cambierebbe completamente. Questo dimostra quanto anche una piccola modifica al dato originale cambi radicalmente l’hash. È proprio questa proprietà che rende l’hashing così utile per la verifica dell’integrità dei dati. E anche per farmi sentire un grande (imbecille) a scrivere un post scriptum del genere. Ciao stelline!"
  },
  {
    "objectID": "blog/hashing.html#che-cosè-in-parole-povere-lhashing",
    "href": "blog/hashing.html#che-cosè-in-parole-povere-lhashing",
    "title": "Hashing: Che Cos’è ’sto Hashing?",
    "section": "",
    "text": "Partiamo dalle basi, come quando a scuola di cucina ti spiegano come si le uova fritte (eppure, ahimé, anche riesce a fare un disastro…).\nImmagina di avere una di quelle macchinette per fare la carne trita, hai presente? Ci metti dentro un pezzo di carne, giri la manovella e dall’altra parte esce carne trita. Ecco, l’hashing è un po’ come quella macchinetta, ma per i dati.\n\nAh, figo, quindi se ci metto dentro un PDF esce… un PDF tritato?\n\nMinchia, sei simpatico, ti ucciderò per ultimo. No, non proprio. Non trita i dati nel senso che li spezzetta. Ma in un certo senso li trasforma. La macchinetta dell’hashing prende qualsiasi tipo di dato (un testo, un’immagine, un video, un file intero, la tua lista di maleducate) e lo trasforma in una stringa (una sequnza, ma in gergo si chiama stringa) di caratteri di lunghezza fissa, che all’apparenza è completamente casuale. Questa stringa (sequenza) di caratteri è chiamata hash (o “valore hash”, o “digest”, insomma, chiamala come cazzo ti pare, basta che ci capiamo).\n\nOk, ok, ci sono quasi. Ma perché dovrebbe servirmi trasformare i miei dati in una stringa di caratteri a caso? Che me ne faccio? Ci tappezzo il cesso?\n\nCalma, non fare il fenomeno, che cazzo hai mangiato a colazione?! L’hash ha delle proprietà molto particolari che lo rendono decisamente utile. In ordine (o no?):\n\nÈ Deterministico: Se prendi lo stesso dato e lo passi alla stessa “macchinetta” (funzione di hash), otterrai sempre lo stesso identico hash. Sempre. Non importa quante volte lo fai, il risultato sarà sempre lo stesso. È come se la macchinetta avesse una memoria di ferro.\n\nQuindi se faccio l’hash della parola “ciao” ottengo sempre lo stesso hash?\n\nEsatto. Prova. Ci sono un sacco di siti online che fanno hash. Cerca “online hash calculator”, mettici “ciao” e usa l’algoritmo SHA-256 (uno dei più comuni). Vedrai che ti uscirà sempre la stessa stringa di caratteri (tipo b133a0c0e9bee3be20163d2ad31d6248db292aa6dcb1ee087a2aa50e0fc75ae2, ma non ti fissare con questa stringa, è solo un esempio).\nÈ One-Way (A Senso Unico): Questa è la parte veramente interessante e importante. È praticamente impossibile (e dico praticamente perché nella vita nulla è impossibile al 100%, tranne forse te che te ne stai a casa il sabato sera invece di andare a far baldoria con Salvini) tornare indietro dall’hash al dato originale. Cioè, se hai l’hash, non puoi in alcun modo (se non per forza bruta) ricostruire il dato di partenza. È come la macchinetta della carne trita: puoi fare la carne trita dalla carne, ma non puoi rifare il pezzo di carne dalla carne trita. Chiaro no?\n\nSì, sì, ho capito. Quindi se io faccio l’hash della mia password “password123”, anche se qualcuno ruba l’hash, non può risalire alla mia password?\n\nYES! Finalmente hai detto una roba giusta, porca troia. Ed è proprio per questo che l’hashing è fondamentale per la sicurezza delle password. Quando ti registri su un sito web e scegli una password, il sito non salva la tua password originale (sarebbe da cretini, se lo fanno prendi un martello). Bensì, salva solo l’hash della tua password. Quando poi fai il login e inserisci la tua password, il sito calcola l’hash della password che hai inserito e lo confronta con l’hash che ha salvato. Se i due hash coincidono, significa che hai inserito la password giusta, senza che il sito abbia mai dovuto conoscere la tua password in chiaro. Così l’unico che in teoria sa la tua password sei tu (e la tua morosa…).\nÈ Resistente alle Collisioni (Idealmente, come sempre): Questa è un po’ più complicata, ma cercherò di semplificare al massimo. Idealmente (ovvero nei mondi con le fate, le lucine eccetera), è estremamente improbabile che due dati diversi producano lo stesso hash. Si chiama “collisione” quando due dati diversi producono lo stesso hash. Una buona funzione di hash è progettata per rendere le collisioni praticamente impossibili (di nuovo, praticamente). È come dire che è quasi impossibile che due pezzi di carne diversi, messi nella stessa macchinetta, producano esattamente la stessa carne trita. Capito no? Più o meno dai…\n\nPiù o meno… Ma quindi le collisioni possono succedere?\n\nSì, in teoria (e badate, che le cose che possono succedere in teoria anche con probabilità infime sono tante. Tipo che morite in un incidente d’auto, così, per dire.) possono succedere, ma con le funzioni di hash moderne e robuste, la probabilità è talmente bassa che è come vincere la lotteria 10 volte di fila mentre ti cade un meteorite in testa e Salvini dice una roba sensata. Insomma, per scopi pratici, possiamo dire che le collisioni sono trascurabili. Esistono però funzioni di hash più vecchie e meno sicure che sono più vulnerabili alle collisioni, ma c’è già troppa carne al fuoco (pun intended).\n\nRicapitolandolo:\nQuindi, ricapitolando, l’hashing è:\n\nUna macchinetta per i dati: Prende qualsiasi dato e lo trasforma in una stringa di caratteri di lunghezza fissa (hash).\nDeterministico: Stesso dato, stesso hash, sempre.\nOne-Way: Impossibile (praticamente) tornare indietro dall’hash al dato originale.\nResistente alle Collisioni: Improbabile (molto improbabile) che due dati diversi producano lo stesso hash."
  },
  {
    "objectID": "blog/hashing.html#ma-a-che-cazzo-serve-veramente-lhashing",
    "href": "blog/hashing.html#ma-a-che-cazzo-serve-veramente-lhashing",
    "title": "Hashing: Che Cos’è ’sto Hashing?",
    "section": "",
    "text": "Ok, abbiamo capito che cos’è l’hashing e le sue proprietà magiche. Ma in pratica, a cosa serve? Oltre a proteggere le password (che è già una figata non da poco), l’hashing ha un sacco di altre applicazioni. Eccone alcune:\n\nVerifica dell’Integrità dei Dati: Immagina di scaricare un file enorme da internet, tipo un film in HD o un videogioco. Come fai a essere sicuro che il file che hai scaricato sia integro e non sia stato corrotto durante il download? Semplice: il sito web da cui hai scaricato il file di solito fornisce anche l’hash del file. Tu, dopo aver scaricato il file, puoi calcolare l’hash del file scaricato e confrontarlo con l’hash fornito dal sito. Se i due hash coincidono, significa che il file è integro al 100%. Se non coincidono, significa che c’è stato un problema durante il download e il file è corrotto (o peggio, potrebbe essere stato manomesso da qualcuno). È come avere un sigillo di garanzia digitale sul file. Figata, no?\nBlockchain e Criptovalute: Se hai letto l’articolo sulla blockchain (e se non l’hai fatto, vergognati, tanto.), sai che l’hashing è un ingrediente fondamentale della blockchain. Viene usato per creare l’“impronta digitale” di ogni blocco e per collegare i blocchi in una catena immutabile. Senza l’hashing, la blockchain non esisterebbe. Quindi, se ti piacciono le criptovalute (o se le odi, non importa), sappi che devi ringraziare (o maledire) l’hashing.\nFirme Digitali: L’hashing è anche usato nelle firme digitali per garantire l’autenticità e l’integrità dei documenti digitali. Quando firmi digitalmente un documento, in realtà stai firmando l’hash del documento, non il documento intero. Questo rende la firma molto più efficiente e sicura.\nRicerca Efficiente di Dati: L’hashing può essere usato per creare delle “tabelle hash” (o “hash map”), che sono delle strutture dati che permettono di cercare dati in modo molto veloce. È un po’ come avere un indice di un libro: invece di dover sfogliare tutto il libro per trovare un’informazione, puoi consultare l’indice e trovarla subito."
  },
  {
    "objectID": "blog/hashing.html#non-è-tutto-rose-e-fiori-o-hash-e-fiori-in-questo-caso",
    "href": "blog/hashing.html#non-è-tutto-rose-e-fiori-o-hash-e-fiori-in-questo-caso",
    "title": "Hashing: Che Cos’è ’sto Hashing?",
    "section": "",
    "text": "Anche l’hashing, come tutte le cose belle della vita, tipo me, ha i suoi limiti:\n\nCollisioni (anche se improbabili): Abbiamo detto che le collisioni sono molto improbabili, ma non impossibili. In teoria, un attaccante potrebbe cercare di trovare due dati diversi che producono lo stesso hash (un “attacco di collisione”). Se ci riuscisse, potrebbe usare questa collisione per scopi malevoli, tipo sostituire un file legittimo con uno fasullo che ha lo stesso hash. Per fortuna, trovare collisioni per le funzioni di hash moderne e robuste è estremamente difficile e costoso, ma è un rischio teorico da tenere presente.\nAttacchi di “Rainbow Table” e “Brute Force” alle Password: Anche se l’hashing rende difficile risalire alla password originale dall’hash, non è una protezione infallibile al 100%. Esistono tecniche come gli attacchi di “rainbow table” e “brute force” che possono essere usate per cercare di “indovinare” le password a partire dagli hash. Le “rainbow table” sono delle tabelle precalcolate che contengono gli hash di milioni di password comuni. Gli attacchi “brute force” invece consistono nel provare a calcolare l’hash di tutte le possibili password finché non si trova una corrispondenza con l’hash rubato. Per difendersi da questi attacchi, è fondamentale usare password lunghe, complesse e diverse per ogni sito. E smettetela di usare “password123”, che cazzo!\nNon è Crittografia: È importante capire che l’hashing non è crittografia. La crittografia serve per nascondere i dati, rendendoli illeggibili a chi non ha la chiave giusta. L’hashing serve per creare un’impronta digitale dei dati, per verificarne l’integrità e l’autenticità. E’ importante non confondere le due cose. È come confondere un lucchetto (crittografia) con un sigillo di ceralacca (hashing). Entrambi servono per la sicurezza, ma in modi diversi."
  },
  {
    "objectID": "blog/hashing.html#quindi",
    "href": "blog/hashing.html#quindi",
    "title": "Hashing: Che Cos’è ’sto Hashing?",
    "section": "",
    "text": "L’hashing è una tecnologia che sta alla base nel mondo digitale di oggi e di un sacco di cose che usiamo tutti i giorni, dalle password alla blockchain, dalla verifica dell’integrità dei file alle firme digitali. È una tecnologia potente e versatile, ma è importante conoscerne anche i limiti e le potenziali vulnerabilità.\nLa prossima volta che sentirai parlare di hashing pensa alla macchinetta per la carne trita dei dati, alla one-way, e alla sicurezza che può garantire. E soprattutto, smettila di usare password di merda! Grazie per l’attenzione e alla prossima settimana con un altro argomento nerd e (spero) interessante."
  },
  {
    "objectID": "blog/hashing.html#post-scriptum-hash-di-sto-articolo",
    "href": "blog/hashing.html#post-scriptum-hash-di-sto-articolo",
    "title": "Hashing: Che Cos’è ’sto Hashing?",
    "section": "",
    "text": "Giusto per fare i fighi e per farti capire ancora meglio la potenza dell’hashing, ho calcolato l’hash SHA-256 di questo articolo (in formato testo semplice, senza HTML o altre formattazioni). Ecco l’hash:\n31e452bbf5134ac1d0d0a3a5897359b83eaccf6a46aed7d4323ddb51ec2d76f0\nOra, se io modificassi anche solo una virgola di questo articolo, anche solo uno spazio bianco, anche solo una lettera, l’hash cambierebbe completamente. Questo dimostra quanto anche una piccola modifica al dato originale cambi radicalmente l’hash. È proprio questa proprietà che rende l’hashing così utile per la verifica dell’integrità dei dati. E anche per farmi sentire un grande (imbecille) a scrivere un post scriptum del genere. Ciao stelline!"
  },
  {
    "objectID": "blog/sss.html",
    "href": "blog/sss.html",
    "title": "Shamir’s Secret Sharing",
    "section": "",
    "text": "Shamir’s Secret Sharing\nIn questo articolo mi voglio concentrare su una spiegazione (speriamo) dettagliata dell’algoritmo Shamir’s Secret Sharing, provando ad illustrare sia la parte più teorica che quella un po’ più pratica con degli esempi concreti. Allacciamo le cinture.\nFirst things first…\nLo Shamir’s Secret Sharing è innanzitutto un algoritmo di secret sharing crittografico ideato da Adi Shamir nel 1979. Permette di dividere un segreto \\(S\\) (poi ci arriviamo a che cazpita si intende con un segreto) in \\(n\\) parti, chiamate shares (parti?), in maniera tale che, per ricostruire il segreto (aridajela) originale, sia necessario un numero minimo \\(k\\) (con \\(k \\leq n\\)) di queste shares. Questo schema è anche conosciuto come threshold scheme \\((k, n)\\).\nQuindi:\n\nUn segreto viene diviso in \\(n\\) shares.\nAlmeno \\(k\\) shares sono necessarie per ricostruire l’intero segreto (magia).\nCon meno di \\(k\\) shares, il segreto non può essere ricostruito (volevi eh!?), e non si ottiene alcuna informazione su di esso (proprietà di information-theoretically secure).\n\nA che serve:\n\nKey Management: Distribuire una master key crittografica tra più attori, in modo che un numero sufficiente di essi debba cooperare per utilizzarla.\nAccess Control: Dividere una secret key per l’accesso a un sistema o a dei dati tra più utenti.\nDistributed Storage: Distribuire i frammenti di un file criptato su più server, in modo che un certo numero di server debba essere accessibile per decriptare il file.\nSecure Multi-Party Computation (MPC): Come building block per protocolli più complessi.\nDividere i vocali della chat coi bro: Così andiamo tutti in galera, non solo io.\n\nIn sostanza: un segreto è qualcosa che vogliamo proteggere. Per esempio può essere una chiave privata (una password dai… senza fare tanto il fenomeno). Però vogliamo che non risieda in un unico punto bensì che risieda in tanti luoghi fisici diversi. Per esempio un pezzo lo nascondo a casa mia, un pezzo dal salumiere, un pezzo dal gommista eccetera eccetera eccetera.\nA sto punto però prima o poi la password mi serve e visto che io ho la memoria di un pesce rosso devo andare dal salumiere. Ma lo trovo chiuso. E adesso? sono fottuto. Mi serve la mia password per comprare i filmini con le donne nude, come faccio?\nEh come faccio… Con lo schema di Shamir’s non mi serve avere accesso a tutte le share, bensì solo a un gruppo di esse. Ed è qui appunto la magia.\nCome funziona?\nA livello puramente matematico, l’algoritmo si basa sull’interpolation polinomiale. L’idea chiave è che, dati \\(k\\) punti in un piano, con ascisse distinte, esiste uno e un solo polinomio di grado al più \\(k-1\\) che passa per tutti questi punti. Nel nostro caso, il grado del polinomio sarà esattamente \\(k-1\\).\nEsempio con polinomio di grado 2 (k=3):\nPer ricostruire un polinomio di grado 2 (una parabola), sono necessari \\(k=3\\) punti. Qui sotto puoi interagire con un grafico che mostra questo concetto. Muovi i punti e osserva come cambia la parabola. Il termine noto del polinomio (il punto in cui la parabola interseca l’asse y) rappresenta il segreto.\n\n\nCode\nviewof x1 = Inputs.range([-4, 4], {step: 0.5, value: -2, label: \"x1\"})\nviewof y1 = Inputs.range([-5, 5], {step: 0.5, value: 4, label: \"y1\"})\nviewof x2 = Inputs.range([-4, 4], {step: 0.5, value: 2, label: \"x2\"})\nviewof y2 = Inputs.range([-5, 5], {step: 0.5, value: -4, label: \"y2\"})\nviewof x3 = Inputs.range([-4, 4], {step: 0.5, value: 3, label: \"x3\"})\nviewof y3 = Inputs.range([-5, 5], {step: 0.5, value: 4, label: \"y3\"})\n\n// Calculate coefficients function - simplified version using Cramer's rule\n// for the specific case of quadratic regression through 3 points\nfunction calculateCoeffs(x1, y1, x2, y2, x3, y3) {\n  // Matrix A entries for system Ax = b where x are the coefficients\n  const x1_2 = x1 * x1, x2_2 = x2 * x2, x3_2 = x3 * x3;\n  \n  // Determinant of matrix A\n  const detA = x1_2 * (x2 - x3) - x1 * (x2_2 - x3_2) + (x2_2 * x3 - x3_2 * x2);\n  \n  // Using Cramer's rule to solve the system\n  const a = (y1 * (x2 - x3) - x1 * (y2 - y3) + (y2 * x3 - y3 * x2)) / detA;\n  \n  const b = (x1_2 * (y2 - y3) - y1 * (x2_2 - x3_2) + (x2_2 * y3 - x3_2 * y2)) / detA;\n  \n  const c = (x1_2 * (x2 * y3 - x3 * y2) - x1 * (x2_2 * y3 - x3_2 * y2) + y1 * (x2_2 * x3 - x3_2 * x2)) / detA;\n  \n  return [a, b, c];\n}\n\n// Create the plot\n{\n  const width = 800;\n  const height = 600;\n  const margin = {top: 60, right: 40, bottom: 60, left: 60};\n  \n  const svg = d3.create(\"svg\")\n    .attr(\"width\", width)\n    .attr(\"height\", height)\n    .attr(\"viewBox\", [0, 0, width, height])\n    .attr(\"style\", \"max-width: 100%; height: auto;\");\n  \n  const g = svg.append(\"g\")\n    .attr(\"transform\", `translate(${margin.left},${margin.top})`);\n  \n  // Set up scales\n  const xScale = d3.scaleLinear()\n    .domain([-10, 10])\n    .range([0, width - margin.left - margin.right]);\n  \n  const yScale = d3.scaleLinear()\n    .domain([-10, 10])\n    .range([height - margin.top - margin.bottom, 0]);\n  \n  // Add axes\n  g.append(\"g\")\n    .attr(\"transform\", `translate(0,${(height - margin.top - margin.bottom)/2})`)\n    .call(d3.axisBottom(xScale))\n    .attr(\"class\", \"axis\");\n  \n  g.append(\"g\")\n    .attr(\"transform\", `translate(${(width - margin.left - margin.right)/2},0)`)\n    .call(d3.axisLeft(yScale))\n    .attr(\"class\", \"axis\");\n  \n  // Add grid\n  g.append(\"g\")\n    .attr(\"class\", \"grid\")\n    .selectAll(\"line\")\n    .data(d3.range(-10, 10))\n    .join(\"line\")\n    .attr(\"x1\", d =&gt; xScale(d))\n    .attr(\"x2\", d =&gt; xScale(d))\n    .attr(\"y1\", 0)\n    .attr(\"y2\", height - margin.top - margin.bottom)\n    .attr(\"stroke\", \"#ddd\")\n    .attr(\"stroke-width\", 0.5);\n\n  g.append(\"g\")\n    .attr(\"class\", \"grid\")\n    .selectAll(\"line\")\n    .data(d3.range(-10, 10))\n    .join(\"line\")\n    .attr(\"y1\", d =&gt; yScale(d))\n    .attr(\"y2\", d =&gt; yScale(d))\n    .attr(\"x1\", 0)\n    .attr(\"x2\", width - margin.left - margin.right)\n    .attr(\"stroke\", \"#ddd\")\n    .attr(\"stroke-width\", 0.5);\n  \n  // Calculate parabola points\n  const coeffs = calculateCoeffs(x1, y1, x2, y2, x3, y3);\n  const xValues = d3.range(-5, 5.1, 0.1);\n  const parabola = xValues.map(x =&gt; ({\n    x: x,\n    y: coeffs[0] * x * x + coeffs[1] * x + coeffs[2]\n  }));\n  \n  // Draw parabola\n  const line = d3.line()\n    .x(d =&gt; xScale(d.x))\n    .y(d =&gt; yScale(d.y))\n    .curve(d3.curveNatural);\n  \n  g.append(\"path\")\n    .datum(parabola)\n    .attr(\"fill\", \"none\")\n    .attr(\"stroke\", \"blue\")\n    .attr(\"stroke-width\", 2)\n    .attr(\"d\", line);\n  \n  // Add points\n  const points = [\n    {x: x1, y: y1},\n    {x: x2, y: y2},\n    {x: x3, y: y3}\n  ];\n  \n  g.selectAll(\"circle\")\n    .data(points)\n    .join(\"circle\")\n    .attr(\"cx\", d =&gt; xScale(d.x))\n    .attr(\"cy\", d =&gt; yScale(d.y))\n    .attr(\"r\", 8)\n    .attr(\"fill\", \"red\");\n  \n  // Add equation and secret value\n  const equation = `y = ${coeffs[0].toFixed(2)}x² + ${coeffs[1].toFixed(2)}x + ${coeffs[2].toFixed(2)}`;\n  g.append(\"text\")\n    .attr(\"x\", (width - margin.left - margin.right) / 2)\n    .attr(\"y\", -10)\n    .attr(\"text-anchor\", \"middle\")\n    .text(equation);\n  \n  g.append(\"text\")\n    .attr(\"x\", (width - margin.left - margin.right) / 2)\n    .attr(\"y\", -30)\n    .attr(\"text-anchor\", \"middle\")\n    .text(`Secret: ${coeffs[2].toFixed(2)}`);\n  \n  return svg.node();\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFasi dell’algoritmo:\n\nSetup:\n\nSia \\(S\\) il segreto da condividere, rappresentato come un numero intero.\nSia \\(n\\) il numero di shares da generare.\nSia \\(k\\) il threshold, ovvero il numero minimo di shares necessarie per ricostruire il segreto.\nScegli un numero primo \\(p\\) maggiore di \\(S\\) e di \\(n\\). Tutte le operazioni aritmetiche saranno eseguite modulo \\(p\\) (in \\(\\mathbb{Z}_p\\)).\n\nShare Generation:\n\nScegli casualmente \\(k-1\\) coefficienti \\(a_1, a_2, \\dots, a_{k-1}\\), dove ogni \\(a_i\\) è un numero intero in \\(\\mathbb{Z}_p\\).\nCostruisci il polinomio \\(P(x)\\) di grado \\(k-1\\):\n\n\\[P(x) = S + a_1x + a_2x^2 + \\dots + a_{k-1}x^{k-1}\\]\n\nNotare che il termine noto del polinomio è il segreto \\(S\\) (\\(P(0) = S\\)).\nGenera \\(n\\) shares calcolando il valore del polinomio in \\(n\\) punti distinti non nulli. Ad esempio, si possono usare i punti \\(x = 1, 2, \\dots, n\\).\nLa share \\(i\\)-esima è la coppia \\((x_i, y_i)\\), dove \\(x_i = i\\) e \\(y_i = P(i)\\).\n\nSecret Reconstruction:\n\nPer ricostruire il segreto, sono necessarie almeno \\(k\\) shares \\((x_i, y_i)\\).\nUtilizza l’interpolation di Lagrange per ricostruire il polinomio \\(P(x)\\) a partire dai \\(k\\) punti. La formula di Lagrange è:\n\n\\[P(x) = \\sum_{i=1}^{k} y_i \\prod_{j=1, j \\neq i}^{k} \\frac{x - x_j}{x_i - x_j}\\]\n\nUna volta ricostruito il polinomio \\(P(x)\\), il segreto \\(S\\) può essere ottenuto valutando il polinomio in \\(x=0\\):\n\n\\[S = P(0)\\]\n\nEsempio (semplificato):\n\nSegreto: \\(S = 1234\\)\nNumero di shares: \\(n = 5\\)\nThreshold: \\(k = 3\\)\nNumero primo: \\(p = 1613\\) (maggiore di \\(S\\) e \\(n\\))\n\nShare Generation:\n\nScegliamo casualmente \\(k-1 = 2\\) coefficienti: \\(a_1 = 166\\), \\(a_2 = 94\\).\nIl polinomio è: \\(P(x) = 1234 + 166x + 94x^2\\).\nGeneriamo 5 shares:\n\n\\(P(1) = 1234 + 166 \\cdot 1 + 94 \\cdot 1^2 = 1494 \\pmod{1613}\\) -&gt; \\((1, 1494)\\)\n\\(P(2) = 1234 + 166 \\cdot 2 + 94 \\cdot 2^2 = 1942 \\equiv 329 \\pmod{1613}\\) -&gt; \\((2, 329)\\)\n\\(P(3) = 1234 + 166 \\cdot 3 + 94 \\cdot 3^2 = 2578 \\equiv 965 \\pmod{1613}\\) -&gt; \\((3, 965)\\)\n\\(P(4) = 1234 + 166 \\cdot 4 + 94 \\cdot 4^2 = 3402 \\equiv 176 \\pmod{1613}\\) -&gt; \\((4, 176)\\)\n\\(P(5) = 1234 + 166 \\cdot 5 + 94 \\cdot 5^2 = 4414 \\equiv 1188 \\pmod{1613}\\) -&gt; \\((5, 1188)\\)\n\n\nSecret Reconstruction:\nSupponiamo di avere le shares: \\((1, 1494)\\), \\((3, 965)\\), \\((5, 1188)\\).\n\nUsiamo l’interpolation di Lagrange per ricostruire il polinomio.\nDopo aver eseguito i calcoli (omessi per brevità), otteniamo \\(P(x) = 1234 + 166x + 94x^2\\).\nIl segreto è \\(S = P(0) = 1234\\).\n\nProprietà di sicurezza:\n\nInformation-Theoretically Secure: Con meno di \\(k\\) shares, non si ottiene alcuna informazione sul segreto. Questo perché qualsiasi valore del segreto è ugualmente probabile, dato un numero insufficiente di punti per determinare univocamente il polinomio.\nPerfect Secret Sharing: Ogni share è grande quanto il segreto originale.\n\nLimitazioni:\n\nShare Size: Ogni share ha la stessa dimensione del segreto. Questo può essere problematico se il segreto è molto grande, per esempio un file.\nDealer Trust: Il dealer (chi genera le shares) conosce il segreto e deve essere fidato. E dunque potenzialmente rende questo inusabile in contesti come blockchain dove si potrebbe volere uno schema meno lasco dove la chiave privata (il segreto) non viene mai veramente materializzata completamente.\nStatic Threshold: Il valore di \\(k\\) (il threshold) è fissato al momento della generazione delle shares. Ovvero se aumentano o diminuiscono i partecipanti bisogna rifare tutto sto giro da capo.\n\nConclusioni:\nWow. Che figata, loso. Lo Shamir’s Secret Sharing è alla fine un algoritmo decisamente potent per la condivisione di segreti o chiavi private. Nonostante alcune limitazioni, rimane uno schema fondamentale nel campo della crittografia e della sicurezza informatica."
  },
  {
    "objectID": "projects/prese-rnn.html#recurrent-neural-networks",
    "href": "projects/prese-rnn.html#recurrent-neural-networks",
    "title": "Of RNNs: Ising, Hopfield, Math and Modernity",
    "section": "Recurrent Neural Networks",
    "text": "Recurrent Neural Networks"
  },
  {
    "objectID": "projects/prese-rnn.html#physics-foundations",
    "href": "projects/prese-rnn.html#physics-foundations",
    "title": "Of RNNs: Ising, Hopfield, Math and Modernity",
    "section": "Physics Foundations",
    "text": "Physics Foundations\n\n1925: The Ising Model laid groundwork for understanding dynamic systems.\n\nConcept: A mathematical model of ferromagnetism in statistical mechanics.\nRelevance: Introduced ideas of interacting components and system states evolving over time, conceptually related to dynamic systems later explored in RNNs."
  },
  {
    "objectID": "projects/prese-rnn.html#hopfield-networks",
    "href": "projects/prese-rnn.html#hopfield-networks",
    "title": "Of RNNs: Ising, Hopfield, Math and Modernity",
    "section": "Hopfield Networks",
    "text": "Hopfield Networks\n\n1982: Hopfield Networks introduced associative memory structures.\n\nConcept: A type of recurrent neural network that serves as a content-addressable “associative memory” system.\nRelevance: Demonstrated the potential of recurrent connections for memory and pattern completion, a precursor to RNNs for sequential data."
  },
  {
    "objectID": "projects/prese-rnn.html#formalizing-recurrent-networks-and-training",
    "href": "projects/prese-rnn.html#formalizing-recurrent-networks-and-training",
    "title": "Of RNNs: Ising, Hopfield, Math and Modernity",
    "section": "Formalizing Recurrent Networks and Training",
    "text": "Formalizing Recurrent Networks and Training\n\n1986: Recurrent Neural Networks (RNNs) were formalized.\n\nConcept: Neural networks with loops, allowing them to process sequences of inputs by maintaining a hidden state that carries information across time steps.\nRelevance: The birth of the RNN as we know it, designed to handle sequential data."
  },
  {
    "objectID": "projects/prese-rnn.html#backpropation-through-time",
    "href": "projects/prese-rnn.html#backpropation-through-time",
    "title": "Of RNNs: Ising, Hopfield, Math and Modernity",
    "section": "Backpropation Through Time",
    "text": "Backpropation Through Time\n\n1986: Backpropagation Through Time (BPTT) was developed for training RNNs.\n\nConcept: An adaptation of backpropagation algorithm to train RNNs by unfolding the network through time and then applying standard backpropagation.\nRelevance: Provided a practical method to train RNNs, enabling them to learn from sequential data."
  },
  {
    "objectID": "projects/prese-rnn.html#addressing-the-vanishing-gradient",
    "href": "projects/prese-rnn.html#addressing-the-vanishing-gradient",
    "title": "Of RNNs: Ising, Hopfield, Math and Modernity",
    "section": "Addressing the Vanishing Gradient",
    "text": "Addressing the Vanishing Gradient\n\n1997: LSTM networks addressed the vanishing gradient problem.\n\nConcept: Long Short-Term Memory networks, a special type of RNN with memory cells and gates that regulate information flow, designed to mitigate the vanishing gradient problem.\nRelevance: A major breakthrough that allowed RNNs to learn long-range dependencies in sequences, significantly improving performance on tasks like NLP."
  },
  {
    "objectID": "projects/prese-rnn.html#simplifying-and-maintaining-performance",
    "href": "projects/prese-rnn.html#simplifying-and-maintaining-performance",
    "title": "Of RNNs: Ising, Hopfield, Math and Modernity",
    "section": "Simplifying and Maintaining Performance",
    "text": "Simplifying and Maintaining Performance\n\n2014: GRUs simplified RNN structures while maintaining performance.\n\nConcept: Gated Recurrent Units, a simplified version of LSTMs with fewer gates, offering comparable performance with fewer parameters.\nRelevance: Provided a more efficient alternative to LSTMs in many cases, making RNNs more accessible and faster to train."
  },
  {
    "objectID": "projects/prese-rnn.html#modern-advancements",
    "href": "projects/prese-rnn.html#modern-advancements",
    "title": "Of RNNs: Ising, Hopfield, Math and Modernity",
    "section": "Modern Advancements",
    "text": "Modern Advancements\n\n2020: Modern LSTMs evolved for advanced applications like NLP and forecasting.\n\nConcept: Continued research and development on LSTMs and related architectures, incorporating attention mechanisms, transformers, and other advancements for state-of-the-art performance.\nRelevance: Highlights the ongoing evolution of RNNs and their continued relevance in cutting-edge applications, especially in natural language processing and time series forecasting."
  },
  {
    "objectID": "projects/prese-rnn.html#backpropagation-the-math",
    "href": "projects/prese-rnn.html#backpropagation-the-math",
    "title": "Of RNNs: Ising, Hopfield, Math and Modernity",
    "section": "Backpropagation: The Math",
    "text": "Backpropagation: The Math\nTo understand BPTT works, let’s derive its mathematical formulation."
  },
  {
    "objectID": "projects/prese-rnn.html#unfolding-the-rnn",
    "href": "projects/prese-rnn.html#unfolding-the-rnn",
    "title": "Of RNNs: Ising, Hopfield, Math and Modernity",
    "section": "Unfolding the RNN",
    "text": "Unfolding the RNN\n\nConsider an RNN that processes a sequence of inputs \\[( x_1, x_2, \\ldots, x_T )\\] At each time step \\(t\\), the RNN maintains a hidden state \\(h_t\\) which is updated based on the current input \\(x_t\\) and the previous hidden state \\(h_{t-1}\\):\n\\[h_t = f(W_h h_{t-1} + W_x x_t + b)\\]\nwhere \\(W_h\\) and \\(W_x\\) are weight matrices, \\(b\\) is a bias vector, and \\(f\\) is an activation function (typically tanh or ReLU)."
  },
  {
    "objectID": "projects/prese-rnn.html#loss-function",
    "href": "projects/prese-rnn.html#loss-function",
    "title": "Of RNNs: Ising, Hopfield, Math and Modernity",
    "section": "Loss Function",
    "text": "Loss Function\nAssume we have a loss function \\(L\\) that depends on the outputs of the RNN at each time step. The total loss over the sequence is:\n\\[L = \\sum_{t=1}^T L_t(y_t, \\hat{y}_t)\\]\nwhere \\(y_t\\) is the true output and \\(\\hat{y}_t\\) is the predicted output at time step \\(t\\)."
  },
  {
    "objectID": "projects/prese-rnn.html#backpropagation-through-time",
    "href": "projects/prese-rnn.html#backpropagation-through-time",
    "title": "Of RNNs: Ising, Hopfield, Math and Modernity",
    "section": "Backpropagation Through Time",
    "text": "Backpropagation Through Time\nTo train the RNN, we need to compute the gradients of the loss with respect to the weights \\(W_h\\) and \\(W_x\\). BPTT involves unfolding the RNN through time and applying the chain rule of calculus to compute these gradients.\n\nForward Pass: Compute the hidden states \\(h_t\\) and the outputs \\(\\hat{y}_t\\) for \\(t = 1, 2, \\ldots, T\\).\nBackward Pass: Compute the gradients of the loss with respect to the hidden states and weights by propagating the error backwards through time.\nThe gradient of the loss with respect to the hidden state at time step \\(t\\) is:\n\\[\\frac{\\partial L}{\\partial h_t} = \\sum_{k=t}^T \\frac{\\partial L_k}{\\partial h_t}\\]\nUsing the chain rule, we can express this as:\n\\[\\frac{\\partial L_k}{\\partial h_t} = \\frac{\\partial L_k}{\\partial \\hat{y}_k} \\frac{\\partial \\hat{y}_k}{\\partial h_k} \\frac{\\partial h_k}{\\partial h_t}\\]\nThe gradient of the hidden state \\(h_k\\) with respect to \\(h_t\\) involves the recurrent connection:\n\\[\\frac{\\partial h_k}{\\partial h_t} = \\prod_{j=t+1}^k \\frac{\\partial h_j}{\\partial h_{j-1}}\\]\nFinally, the gradients of the loss with respect to the weights are computed by summing the contributions from each time step:\n\\[\\frac{\\partial L}{\\partial W_h} = \\sum_{t=1}^T \\frac{\\partial L}{\\partial h_t} \\frac{\\partial h_t}{\\partial W_h}\\]\n\\[\\frac{\\partial L}{\\partial W_x} = \\sum_{t=1}^T \\frac{\\partial L}{\\partial h_t} \\frac{\\partial h_t}{\\partial W_x}\\]"
  },
  {
    "objectID": "projects/prese-rnn.html#a-plot",
    "href": "projects/prese-rnn.html#a-plot",
    "title": "Of RNNs: Ising, Hopfield, Math and Modernity",
    "section": "A Plot",
    "text": "A Plot\n\nsunburst = {\n  const root = partition(flareData);\n  root.each(d =&gt; d.current = d);\n  const svg = d3.create(\"svg\")\n      .attr(\"viewBox\", [0, 0, width, width])\n      .style(\"font\", \"15px sans-serif\");\n\n  // ...remainder of implementation\n  \n  return svg.node();\n}"
  },
  {
    "objectID": "projects/connect4.html",
    "href": "projects/connect4.html",
    "title": "Connect 4",
    "section": "",
    "text": "Voglio applicare il metodo DPO a una rete RNN. Creerò una struttura composta da tre elementi che interagiscono tra loro:\n\nuna RNN che ha appreso solo le regole del gioco (banalmente sequenze di partite random)\nuna rete che apprende tramite DPO a discernere tra una mossa giusta e una sbagliata\nuna rete che apprende ad apprendere, che adatta i parametri della rete 1. in base alla situazione/stato corrente."
  },
  {
    "objectID": "projects/connect4.html#february-16th-2025",
    "href": "projects/connect4.html#february-16th-2025",
    "title": "Connect 4",
    "section": "February 16th, 2025",
    "text": "February 16th, 2025\nProgress: Successfully experimented with GRUs on the “adding problem” to understand RNN learning dynamics. Demonstrated that normalizing input/output to [0,1] significantly improves performance. Showed the model (with a hidden size of one) can learn to sum a variable number of masked inputs, even with variable sequence lengths (up to +/- 80% variation). This highlights the surprising capability of even tiny RNNs.\nTechnical Notes: - GRUs were used for all experiments. - Input/output normalization to [0,1] is essential for learning. - The model architecture is extremely small: a single-layer GRU with hidden size 1, followed by a linear layer. - The model successfully learned with a variable number of masked elements and variable sequence length. - The sequence length has little to no effect on the ability to learn.\nNext Steps: - Begin investigating the incorporation of the DPO loss. - Define the structure of the meta-learning network. Consider the size constraint (meta-network should be smaller than the main network?). - Start thinking about how to design signals for “flag” and “value” to sum. - Explore whether an Energy-Based Model (EBM) is suitable for the meta-learning component (though stability could a concern)."
  },
  {
    "objectID": "projects/spatial-spike-neural-networks-it.html",
    "href": "projects/spatial-spike-neural-networks-it.html",
    "title": "Spatial Spike Neural Networks",
    "section": "",
    "text": "Voglio provare a studiare un tipo di rete neurale di cui non ho trovato citazioni in ambito di ricerca. Di base Una rete neurale spiking (SNN) è un tipo di Neural Network che si avvicina maggiormente al funzionamento dei neuroni biologici. A differenza delle reti neurali tradizionali che trasmettono valori continui (ovvero fanno le moltiplicazioni direttamente), le SNN elaborano le informazioni attraverso “spike”, ovvero eventi discreti nel tempo, rendendole computazionalmente più efficienti. Queste reti codificano le informazioni nella temporizzazione e nella frequenza degli spike, consentendo loro di catturare dinamiche temporali complesse e potenzialmente imitare la capacità di apprendimento del cervello.\nL’elemento che voglio introdurre è quello di strutture (basate su grafi) che includano all’interno della rete anche il concetto di vicinanza dei neuroni della rete. Voglio inoltre provare a comprendere se i neuroni possono spostarsi all’interno della struttura a grafo.\nIn altre parole: do una struttura alla rete, come ad esempio delle strutture simili a quelle cristalline, e posiziono i neuroni. Man mano che apprendono pattern all’interno dei dati, li faccio muovere seguendo la struttura data, seguendo l’apprendimento Hebbiano\n\n\n\n\n🇮🇹 Mentre mi preparavo la camomilla con la melatonina ieri sera ho visualizzato per un istante la rete che ho in mente come una specie di struttura che si adatta all’input che riceve. Può essere realizzata una roba del genere? Può l’input guidare il movimento dei neuroni? Devo ancora capire bene cosa ho visto, ma quello che forse voglio realizzare è un grafo con i neuroni posizionati dove devono stare. Quando ricevono un input (che potrebbe essere un vettore latente) si riorganizzano seguendo i percorsi obbligati dalla struttura per far sì che neuroni che devono stare vicini restino vicini. Nulla vieta che i “neuroni” possano in realtà essere intere reti deep.\n🇬🇧 While I was preparing my chamomile with melatonin last night, I briefly visualized the network I have in mind as a kind of structure that adapts to the input it receives. Can such a thing be realized? Can the input guide the movement of the neurons? I still need to fully understand what I saw, but what I might want to create is a graph with neurons positioned where they need to be. When they receive an input (which could be a latent vector), they rearrange following the paths dictated by the structure to ensure that neurons that need to stay close remain close. Nothing prevents the “neurons” from actually being entire deep networks.\n\n\n\n🇮🇹 a colazione ho mangiato latte e cereali. Ho scaldato il latte nella pentolino, versato sui cereali caldo. Mangiato. Finto, metto nel lavandino la pentola e la ciotola dentro la pentola e butto un po’ d’acqua così, come mi ha insegnato la mamma, è più facile lavare. Se metto la ciotola dentro la pentola, all’interno della pentola l’acqua raggiunge prima il bordo superiore perché c’è la ciotola a fare volume, così posso usare meno acqua. Ovvero: avere qualcosa che riempe il volume permette di usare meno risorse. Può essere utile? boh.\n🇬🇧 for breakfast I had milk and cereal. I heated the milk in the saucepan, poured it hot over the cereal. Ate. Finished, I put the saucepan in the sink and the bowl inside the saucepan and I put a bit of water in so that, like my mum taught me, it’s easier to wash up. If I put the bowl inside the saucepan, inside the saucepan the water reaches the top edge sooner because the bowl is taking up volume, so I can use less water. Basically: having something that fills the volume means you can use fewer resources. Could that be useful? Dunno."
  },
  {
    "objectID": "projects/spatial-spike-neural-networks-it.html#log",
    "href": "projects/spatial-spike-neural-networks-it.html#log",
    "title": "Spatial Spike Neural Networks",
    "section": "",
    "text": "🇮🇹 Mentre mi preparavo la camomilla con la melatonina ieri sera ho visualizzato per un istante la rete che ho in mente come una specie di struttura che si adatta all’input che riceve. Può essere realizzata una roba del genere? Può l’input guidare il movimento dei neuroni? Devo ancora capire bene cosa ho visto, ma quello che forse voglio realizzare è un grafo con i neuroni posizionati dove devono stare. Quando ricevono un input (che potrebbe essere un vettore latente) si riorganizzano seguendo i percorsi obbligati dalla struttura per far sì che neuroni che devono stare vicini restino vicini. Nulla vieta che i “neuroni” possano in realtà essere intere reti deep.\n🇬🇧 While I was preparing my chamomile with melatonin last night, I briefly visualized the network I have in mind as a kind of structure that adapts to the input it receives. Can such a thing be realized? Can the input guide the movement of the neurons? I still need to fully understand what I saw, but what I might want to create is a graph with neurons positioned where they need to be. When they receive an input (which could be a latent vector), they rearrange following the paths dictated by the structure to ensure that neurons that need to stay close remain close. Nothing prevents the “neurons” from actually being entire deep networks.\n\n\n\n🇮🇹 a colazione ho mangiato latte e cereali. Ho scaldato il latte nella pentolino, versato sui cereali caldo. Mangiato. Finto, metto nel lavandino la pentola e la ciotola dentro la pentola e butto un po’ d’acqua così, come mi ha insegnato la mamma, è più facile lavare. Se metto la ciotola dentro la pentola, all’interno della pentola l’acqua raggiunge prima il bordo superiore perché c’è la ciotola a fare volume, così posso usare meno acqua. Ovvero: avere qualcosa che riempe il volume permette di usare meno risorse. Può essere utile? boh.\n🇬🇧 for breakfast I had milk and cereal. I heated the milk in the saucepan, poured it hot over the cereal. Ate. Finished, I put the saucepan in the sink and the bowl inside the saucepan and I put a bit of water in so that, like my mum taught me, it’s easier to wash up. If I put the bowl inside the saucepan, inside the saucepan the water reaches the top edge sooner because the bowl is taking up volume, so I can use less water. Basically: having something that fills the volume means you can use fewer resources. Could that be useful? Dunno."
  },
  {
    "objectID": "projects/adding-problem-part2.html",
    "href": "projects/adding-problem-part2.html",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 2",
    "section": "",
    "text": "In this blog post, I’ll complete the journey started in the previous post in which I introduced the problem and showed some interesting plots depicting weights and the learning dynamics that was behind it. I tried to explain the way that GRU works under the hood, step by step with a full example of a sequence going through the whole GRU. We saw step by step each and every transformation that the input is subjected to. In this blog post we will do something in my opinion more interesting that is analysing the weights. Now, this can actually mean everything so I want to split the discussion in two parts:\n\nWhat the weights actually mean? We saw a complete forward pass that showed more or less how the initial input is transformed into the hidden layer. But why those specific weights rather than other values?\nHow did we get there? It’s well known that weights are usually randomly initialized in networks, so how did we get from those random values to our values? Is the dynamics of the learning responsible of the specific values? Could have this been done differently? Could have the weights have followed different trajectories? In either case can it be proved?\n\nLet’s begin, this is gonna be a lot of fun! (And a lot of work!!)"
  },
  {
    "objectID": "projects/adding-problem-part2.html#introduction",
    "href": "projects/adding-problem-part2.html#introduction",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 2",
    "section": "",
    "text": "In this blog post, I’ll complete the journey started in the previous post in which I introduced the problem and showed some interesting plots depicting weights and the learning dynamics that was behind it. I tried to explain the way that GRU works under the hood, step by step with a full example of a sequence going through the whole GRU. We saw step by step each and every transformation that the input is subjected to. In this blog post we will do something in my opinion more interesting that is analysing the weights. Now, this can actually mean everything so I want to split the discussion in two parts:\n\nWhat the weights actually mean? We saw a complete forward pass that showed more or less how the initial input is transformed into the hidden layer. But why those specific weights rather than other values?\nHow did we get there? It’s well known that weights are usually randomly initialized in networks, so how did we get from those random values to our values? Is the dynamics of the learning responsible of the specific values? Could have this been done differently? Could have the weights have followed different trajectories? In either case can it be proved?\n\nLet’s begin, this is gonna be a lot of fun! (And a lot of work!!)"
  },
  {
    "objectID": "projects/adding-problem-part2.html#what-those-parameters-mean",
    "href": "projects/adding-problem-part2.html#what-those-parameters-mean",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 2",
    "section": "2 What those parameters mean?",
    "text": "2 What those parameters mean?\n\n\n\n\n\n\nNote\n\n\n\nI will use weights and parameters interchangeably. One could argue that they have different meanings (and it might even be true) but in our case maybe we can loosen a bit this detail in terminology and live happily anyway.\n\n\nSo: what those parameter mean?\nI tried to figure this out because I noticed one really weird thing looking at the last hidden layer of our walkthrough.\nLet’s refresh quickly what was the example, remember that the input sequence was:\n\\[\n\\text{x} =\n\\left[\n\\begin{bmatrix}\n12 \\\\ 0\n\\end{bmatrix}\n\\begin{bmatrix}\n37 \\\\ 1\n\\end{bmatrix}\n\\begin{bmatrix}\n12 \\\\ 0\n\\end{bmatrix}\n\\begin{bmatrix}\n21 \\\\ 1\n\\end{bmatrix}\n\\right]\n\\]\nwe normalized the numbers so that they were represented in hundreths, so basically what we had after the normalization was:\nThe input sequence is represented as a tensor:\n\\[\n\\text{x} =\n\\begin{bmatrix}\n0.12 & 0 \\\\\n0.37 & 1 \\\\\n0.12 & 0 \\\\\n0.21 & 1\n\\end{bmatrix}\n\\]\nMy architecture was given the smallest hidden layer as possible on purpose. What I was trying to do was pushin the GRU cell to make the best out of what it had, and my expectation was that at some point it would have learned to basically add to it’s hidden layer the value in position \\(0\\) iff the flaf in position \\(1\\) was equal to \\(true\\) (or \\(1\\) ok…) This didn’t happen and looking back at my original thought I feel a bit stupid about it. Let’s recall what a GRU cell is. Initially, for \\(t = 0\\), the hidden layer is \\(h_0 = 0\\).\nthen the cell uses this series of transformations to get tha hidden layer out:\n\\[\\begin{aligned}\nz_t &= \\sigma(W_z x_t + U_z h_{t-1} + b_z) \\\\\nr_t &= \\sigma(W_r x_t + U_r h_{t-1} + b_r) \\\\\n\\hat{h}_t &= \\phi(W_h x_t + U_h (r_t \\odot h_{t-1}) + b_h) \\\\\nh_t &= z_t \\odot h_{t-1} + (1 - z_t) \\odot \\hat{h}_t\n\\end{aligned}\\]\nif you look at the activation function this should already ring a bell, but let’s walk through step by step. Let’s assume we always have four elements and 2 of them are summed to the total. Let’s also assume that we normalize to the maximum each single value can reach (in our case \\(100\\)). Let’s have an extreme example now:\n\\[\n\\text{x} =\n\\begin{bmatrix}\n100 & 1 \\\\\n100 & 1 \\\\\n100 & 0 \\\\\n100 & 0\n\\end{bmatrix}\n\\]\nwhich after our normalization becomes:\n\\[\n\\text{x} =\n\\begin{bmatrix}\n1.0 & 1 \\\\\n1.0 & 1 \\\\\n1.0 & 0 \\\\\n1.0 & 0\n\\end{bmatrix}\n\\]\nNow: if my first hypothesis was right the hidden layer should have contained something like \\(1.0 + 1.0 = 2.0\\). But this could have never happened. Why? Look at the activation functions and also at how the new hidden cell is calculated\n\\[\nh_t = z_t \\odot h_{t-1} + (1 - z_t) \\odot \\hat{h}_t\n\\]\nBasically it’s a weighted sum of the old value of the cell and the new value of the cell. If the weight was either \\(0\\) or \\(1\\) in extreme cases this would have meant that either the old value was being kept as it was (ignoring totally the new value) or the converse: the old value was forgotten and the new was taking its place. So we might expect that the truth is in the middle, meaning that \\(z_t\\) was around \\(.5\\) so that it took half the information from the \\(t-1\\) step and half the information from the new value. But does this make sense? Why \\(\\frac{1}{2}\\)?\nIf we see at what the network is doing instead, we observe a cool and symmetrical (I LOVE SYMMETRIES!) behaviour. What is that? If you see the old post you’ll see that \\(z_t\\) takes on some nice values that are\n\\[\\begin{aligned}\n\nz_1 = 0.8275274634361267 \\\\\nz_2 = 0.15969596803188324 \\\\\nz_3 = 0.8254608511924744 \\\\\nz_4 = 0.1668987274169922 \\\\\n\\end{aligned}\\]\nBasically when the flag is \\(0\\), then \\(z_t \\ge .825\\) otherwise \\(z_t \\le .166\\). So we already observe a pattern. I love this so much! Let’s recall then how \\(z_t\\) is computed:\n\\[\nz_t = \\sigma(W_z x_t + U_z h_{t-1} + b_z)\n\\]\nwhere \\(\\sigma\\) is the sigmoid function. Recall that the sigmoid function squeezes its input in the range \\([0,1]\\) In the plot below we map \\(100\\) points to the sigmoid function (did you know that function, map and application are quite the same thing in math?)\n\n\nCode\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-10,10,100)\ny = 1/(1+np.exp(-x))\nsns.set_theme(style=\"darkgrid\")  # This gives the typical Seaborn look\n\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=x, y=y, linewidth=2)\nplt.title('Sigmoid Function')\nplt.xlabel('x')\nplt.ylabel('$\\sigma(x)$')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nSo now, let’s do something nice and get back our weights, that are gonna be useful for our next section.\n\n\nCode\nimport json\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm.notebook import trange, tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport os\n\nclass AddingProblemGRU(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(AddingProblemGRU, self).__init__()\n        self.gru = nn.GRU(\n            input_size, hidden_size, num_layers=1, batch_first=True\n        )\n        self.linear = nn.Linear(hidden_size, output_size)\n        self.init_weights()\n\n    def init_weights(self):\n        for name, param in self.gru.named_parameters():\n            if \"weight\" in name:\n                nn.init.orthogonal_(param)\n            elif \"bias\" in name:\n                nn.init.constant_(param, 0)\n        nn.init.xavier_uniform_(self.linear.weight)\n        nn.init.zeros_(self.linear.bias)\n\n    def forward(self, x):\n        out, hn = self.gru(x)\n        output = self.linear(out[:, -1, :])\n        return output, out\n\n\n# Reproducibility\nRANDOM_SEED = 37\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\ntorch.cuda.manual_seed_all(RANDOM_SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Hyperparameters\nDELTA = 0\nSEQ_LEN = 4\nHIGH = 100\nN_SAMPLES = 10000\nTRAIN_SPLIT = 0.8\nBATCH_SIZE = 256\nLEARNING_RATE = 1e-4\nWEIGHT_DECAY = 1e-5\nCLIP_VALUE = 2.0\nNUM_EPOCHS = 3000\nHIDDEN_SIZE = 1\nOUTPUT_SIZE = 1\nINPUT_SIZE = 2\n\ndef adding_problem_generator(N, seq_len=6, high=1, delta=0.6):\n    actual_seq_len = np.random.randint(\n        int(seq_len * (1 - delta)), int(seq_len * (1 + delta))\n    ) if delta &gt; 0 else seq_len\n    num_ones = np.random.randint(2, min(actual_seq_len - 1, 4))\n    X_num = np.random.randint(low=0, high=high, size=(N, actual_seq_len, 1))\n    X_mask = np.zeros((N, actual_seq_len, 1))\n    Y = np.ones((N, 1))\n    for i in range(N):\n        positions = np.random.choice(actual_seq_len, size=num_ones, replace=False)\n        X_mask[i, positions] = 1\n        Y[i, 0] = np.sum(X_num[i, positions])\n    X = np.append(X_num, X_mask, axis=2)\n    return X, Y\n\nX, Y = adding_problem_generator(N_SAMPLES, seq_len=SEQ_LEN, high=HIGH, delta=DELTA)\n\ntraining_len = int(TRAIN_SPLIT * N_SAMPLES)\ntrain_X = X[:training_len]\ntest_X = X[training_len:]\ntrain_Y = Y[:training_len]\ntest_Y = Y[training_len:]\n\ntrain_dataset = TensorDataset(\n    torch.tensor(train_X).float(), torch.tensor(train_Y).float()\n)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\ntest_dataset = TensorDataset(\n    torch.tensor(test_X).float(), torch.tensor(test_Y).float()\n)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# File paths for saved data\ntrain_losses_path = \"train_losses.json\"\ntest_losses_path = \"test_losses.json\"\nall_weights_path = \"all_weights.json\"\nmodel_save_path = (\n    f\"gru_adding_problem_model_epochs_{NUM_EPOCHS}_hidden_{HIDDEN_SIZE}.pth\"\n)\n\nFORCE_TRAIN = False\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ncriterion = nn.MSELoss()\ndef evaluate(model, data_loader, criterion, high):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for inputs, labels in data_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            inputs[:, :, 0] /= high\n            outputs, _ = model(inputs)\n            outputs = outputs * high\n            loss = criterion(outputs, labels)\n            total_loss += loss.item() * inputs.size(0)\n    return total_loss / len(data_loader.dataset)\n\n\n# Try to load data from files\nif FORCE_TRAIN == False and os.path.exists(train_losses_path) and os.path.exists(test_losses_path) and os.path.exists(all_weights_path) and os.path.exists(model_save_path):\n    with open(train_losses_path, \"r\") as f:\n        train_losses = json.load(f)\n    with open(test_losses_path, \"r\") as f:\n        test_losses = json.load(f)\n    with open(all_weights_path, \"r\") as f:\n        all_weights_loaded = json.load(f)\n\n    # Convert loaded weights (which are lists) back to numpy arrays\n    all_weights = []\n    for epoch_weights_list in all_weights_loaded:\n        epoch_weights_dict = {}\n        for name, weights_list in epoch_weights_list.items():\n            epoch_weights_dict[name] = np.array(weights_list)\n        all_weights.append(epoch_weights_dict)\n    #load model\n    model = AddingProblemGRU(\n    input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, output_size=OUTPUT_SIZE)\n    model.load_state_dict(torch.load(model_save_path))\n    model.to(device)\n\n\n\nelse:\n    model = AddingProblemGRU(\n        input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, output_size=OUTPUT_SIZE\n    )\n    model.to(device)\n\n    optimizer = torch.optim.Adam(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode=\"min\", factor=0.5, patience=5, min_lr=1e-6, verbose=False\n    )\n\n\n    \n\n    train_losses = []\n    test_losses = []\n    all_weights = []\n\n    for epoch in trange(NUM_EPOCHS, desc=\"Epoch\"):\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            inputs[:, :, 0] /= HIGH\n            labels_scaled = labels / HIGH\n            optimizer.zero_grad()\n            outputs, _ = model(inputs)\n            loss = criterion(outputs, labels_scaled)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_VALUE)\n            optimizer.step()\n            running_loss += loss.item() * inputs.size(0)\n\n        epoch_loss = running_loss / len(train_loader.dataset)\n        train_losses.append(epoch_loss)\n\n        if epoch % 49 == 0:\n            test_loss = evaluate(model, test_loader, criterion, HIGH)\n            test_losses.append(test_loss)\n            scheduler.step(test_loss)\n\n            weights_dict = {}\n            for name, param in model.named_parameters():\n                weights_dict[name] = param.data.cpu().numpy().copy()\n            all_weights.append(weights_dict)\n        else:\n            test_losses.append(None)\n\n    # Save data to files\n    with open(train_losses_path, \"w\") as f:\n        json.dump(train_losses, f)\n    with open(test_losses_path, \"w\") as f:\n        json.dump(test_losses, f)\n    # Convert weights to lists for JSON serialization\n    all_weights_serializable = [\n        {k: v.tolist() for k, v in epoch_weights.items()}\n        for epoch_weights in all_weights\n    ]\n    with open(all_weights_path, \"w\") as f:\n        json.dump(all_weights_serializable, f)\n\n    # Save Model\n    model_save_path = (\n        f\"gru_adding_problem_model_epochs_{NUM_EPOCHS}_hidden_{HIDDEN_SIZE}.pth\"\n    )\n    torch.save(model.state_dict(), model_save_path)\n\ninput_sequence = torch.tensor([\n    [12,0],\n    [37,1],\n    [12,0],\n    [21,1],\n]).float().unsqueeze(0)\n\n\ninput_sequence[:, :, 0] /= HIGH\n\n# Get weights from the last training epoch.  'all_weights' is populated by the loading/training section.\nlast_epoch_weights = model.state_dict() #all_weights[-1]\n\n# Extract the relevant weight matrices\nW_ih = torch.tensor(last_epoch_weights['gru.weight_ih_l0']).float()  # Input-to-hidden\nW_hh = torch.tensor(last_epoch_weights['gru.weight_hh_l0']).float()  # Hidden-to-hidden\nb_ih = torch.tensor(last_epoch_weights['gru.bias_ih_l0']).float()  # Input-to-hidden bias\nb_hh = torch.tensor(last_epoch_weights['gru.bias_hh_l0']).float()  # Hidden-to-hidden bias\nW_linear = torch.tensor(last_epoch_weights['linear.weight']).float() # Linear layer weights\nb_linear = torch.tensor(last_epoch_weights['linear.bias']).float()   # Linear layer bias"
  },
  {
    "objectID": "projects/adding-problem-part2.html#the-update-gate-at-t0",
    "href": "projects/adding-problem-part2.html#the-update-gate-at-t0",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 2",
    "section": "3 The Update Gate at \\(t=0\\)",
    "text": "3 The Update Gate at \\(t=0\\)\nLet’s start our analysis from the bottom up in the calculation of the updated value of the hidden state. For how PyTorch implements the GRU cell under the hood the \\(z_t\\) value is used in this way:\n\\[\n\\texttt{hy} = (1 - \\texttt{updategate}) * \\texttt{newgate} + \\texttt{updategate} * \\texttt{h}\n\\]\nThis means basically that the higher the value of \\(\\texttt{updategate}\\), the more we should keep in memory our previous value (sum?) and ignore the newgate (the update in memory, the term to be added to the sum?). This sums up with our previous observation: with flag \\(1\\), update gate was pretty low and when flag was \\(0\\) then \\(z_t\\) was pretty high. But how much each component is involved in this behaviour?\n\n\nCode\nx_1 = input_sequence[:, 0, :]\nh = torch.zeros(1, 1, 1)\n\n# Stable version (using torch.matmul)\ngi_stable = torch.matmul(x_1, W_ih.t())\ngh_stable = torch.matmul(h, W_hh.t())\n\n# Get gates\ni_r, i_z, i_n = gi_stable.chunk(3, dim=1)\nh_r, h_z, h_n = gh_stable.chunk(3, dim=2)\n\n# Apply gate operations with controlled precision\nresetgate = torch.sigmoid(i_r + h_r  + b_ih[0] + b_hh[0])\nupdategate = torch.sigmoid(i_z + h_z + b_ih[1] + b_hh[1])\nnewgate = torch.tanh(i_n + b_ih[2] + (resetgate * (h_n + b_hh[2])))\n\nhy = (1 - updategate) * newgate + updategate * h\nh_t = hy\n\n\nThese are the three components that go through the sigmoid for the update gate:\n\\[\\eqalign{\nb_{ih_z} +  b_{hh_z} &= 1.6185756921768188 \\cr\ni_z &= -0.05037130042910576; \\cr\nh_z &= -0.0; \\cr\ni_z + h_z + b_{ih_z} + b_{hh_z} &= 1.568204402923584 \\cr\nz_t &= 0.8275274634361267 \\cr\n}\n\\]\nThis is interesting. So let’s break this down a little bit.\nProbably it’s a good idea to verify how each term here is calculated and what happens. Let’s begin with seeing how \\(i_z\\) is calculated, using the original weight matrix.\n\\[\\eqalign{\ni_z &= x_t \\cdot W_{ih_z}^T\\cr\nW_{ih_z}^T &=\n\\begin{bmatrix}\n-0.4197608530521393 \\\\ -3.02486252784729\n\\end{bmatrix} \\cr\n}\n\\]\nWhat we do to obtain \\(i_z\\) is multiply our input row vector \\(x_0^T = \\left[\\begin{smallmatrix}0.12 \\\\ 0.0\\end{smallmatrix}\\right]\\) by the column vector \\(W_{ih_z}^T\\)\n\\[\n\\begin{bmatrix}0.12 & 0.0\\end{bmatrix}\\cdot \\begin{bmatrix}\n-0.4197608530521393 \\\\ -3.02486252784729\n\\end{bmatrix}\n\\]\nwhich becomes:\n\\[\\eqalign{\ni_z &= 0.12  \\cdot -0.4197608530521393 + 0.0 \\cdot  -3.02486252784729 \\cr\ni_z &= 0.12  \\cdot -0.4197608530521393 + \\cancel{0.0 \\cdot  -3.02486252784729} (\\texttt{flag}=0) \\cr\ni_z &= 0.12  \\cdot -0.4197608530521393 \\cr\ni_z &= -0.05037130042910576\n}\n\\]\nNow this conveys a super important information: when the \\(\\texttt{flag} = 0\\) then only the number has some importance in the final calculation because the flag cancels the second term of the sum as saw before. Now remember, this is only one of the three terms that go through the sigmoid at the end to obtain the final \\(z_t\\) term.\nSo let’s carry on with the second part of it, \\(h_z\\): basically it’s \\(0\\) since when our GRU cell sees the input for the first time its input state is \\(h=0\\), and whatever we multiply here stays zero.\n\\[\nh_z = -0.0\n\\]\nNow the last portion of our sum, the bias. Now, you should know that many ML scientists avoid using the bias term when the data is already centered or when the model inherently accounts for offsets, as it can be redundant and complicate interpretation. But in my case the bias was left there. And in the first calculation you can observe that without the bias term accounts super heavily on the final sum:\n\\[\\eqalign{\nb_{ih_z} &= 0.8092878460884094 \\cr\nb_{hh_z} &= 0.8092878460884094 \\cr\n}\n\\]\nSo the final sum is:\n\\[\n\\texttt{temp} = -0.05037130042910576 + 0 + 0.8092878460884094 + 0.8092878460884094;\n\\]\nI’ve always believed that a plot tells more then hundred numbers, so let’s plot a cumulative sum of the four terms to check how much each of them accounts for the gran total:\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.rcParams[\"text.usetex\"] = True\n\nitems = [r\"$i_z$\", r\"$h_z$\", r\"$b_{ih_z}$\", r\"$b_{hh_z}$\"]\nvalues = [(input_sequence[0][0][0] * W_ih[1][0]).item(), 0, (b_ih[1]).item(), (b_hh[1]).item()]\n\ncumsum_values = np.cumsum(values)\n\ndf = pd.DataFrame({\"Term\": items, \"Cumulative Sum\": cumsum_values})\n\nplt.figure(figsize=(8, 5))\nsns.barplot(x=\"Term\", y=\"Cumulative Sum\", data=df, color=\"skyblue\", alpha=0.7)\n\nfor i, (item, value) in enumerate(zip(items, values)):\n    plt.text(i, cumsum_values[i] - value / 2, f\"$+{value}$\", ha=\"center\", fontsize=12, color=\"black\")\n\nplt.ylabel(r\"\\textbf{Cumulative Sum}\", fontsize=12)\nplt.xlabel(r\"\\textbf{Terms}\", fontsize=12)\nplt.title(r\"\\textbf{Cumulative Sum Contribution}\", fontsize=14)\nplt.ylim(0, max(cumsum_values) + .5)  # Adjust y-limit\nplt.show()\n\n\n\n\n\n\n\n\n\nWow, so basically during the first iteration, we’re only dealing with bias terms. An important distinction to remember is that bias terms don’t depend on or connect to any individual data point being analyzed. Instead, they capture and convey overall patterns or information from the entire dataset as a whole.\nTo complete this first step, let’s do a sanity check. I plot here what would have happened after the first iteration to a number in the range \\([0, 100]\\) (remember normalized by a factor of \\(100\\), so in the final range of \\([0,1]\\)) There are 4 plots:\n\n\\(y_0\\) with bias: In our trained GRU cell (green line), when \\(\\sigma\\) is applied to an input flagged with \\(0\\), the values oscillate between \\([0.77, 0.83]\\).\n\\(y_1\\) with bias: In our trained GRU cell (red line), when \\(\\sigma\\) is applied to an input flagged with \\(1\\), the values oscillate between \\([0.14, 0.20]\\).\n\\(y_0\\) no bias: If we removed the bias term \\(b_z\\) from the GRU cell (blue line) and applied \\(\\sigma\\) to an input flagged with \\(0\\), the value stays around \\(0.5\\). The reason for the cell choosing such a strong bias remains unclear - it seems significant since it pushes \\(z_t\\) considerably higher.\n\\(y_1\\) no bias: If we removed the bias term \\(b_z\\) from the GRU cell (orange line), and applied \\(\\sigma\\) to an input flagged with \\(1\\), the values oscillate between \\(0.05\\). Again, the strong bias choice is super cool. One (crazy!) hypothesis: it pushes up \\(z_t\\) to act as a factor \\(\\frac{1}{4}\\), possibly because the network expects 4 items where 2 flags are in unknown positions and maintains this factor to account for missing items by adding \\(\\frac{1}{4}\\) of the current candidate.\n\nThe takeaway here is how the bias term strongly affects the outputs, particularly in pushing up \\(z_t\\). While we can hypothesize about the network’s strategy (especially regarding the \\(\\frac{1}{4}\\) factor), the exact reason for such a strong bias will be hopefully uncovered in the next sections.\n\n\nCode\nbz =  b_ih[1].item() + b_hh[1].item()\nx = np.linspace(0,1, 1000)\ny0 = x * W_ih[1][0].item()\ny1 = x * W_ih[1][0].item() + W_ih[1][1].item() \ny0_nb = 1 / (1 + np.exp(-(y0)))\ny1_nb = 1 / (1 + np.exp(-(y1)))\ny0_b = 1 / (1 + np.exp(-(y0 + bz)))\ny1_b = 1 / (1 + np.exp(-(y1 + bz)))\n\nsns.set_theme(style=\"darkgrid\")  # This gives the typical Seaborn look\n\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=x, y=y0_nb, linewidth=2, label='$y_0$ no bias')\nsns.lineplot(x=x, y=y1_nb, linewidth=2, label='$y_1$ no bias')\nsns.lineplot(x=x, y=y0_b, linewidth=2, label='$y_0$ with bias')\nsns.lineplot(x=x, y=y1_b, linewidth=2, label='$y_1$ with bias')\nplt.title('Sigmoid Function')\nplt.xlabel('x')\n\nplt.ylabel('$i_z$')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1"
  },
  {
    "objectID": "projects/adding-problem-part2.html#the-reset-gate-at-t0",
    "href": "projects/adding-problem-part2.html#the-reset-gate-at-t0",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 2",
    "section": "4 The Reset gate at \\(t=0\\)",
    "text": "4 The Reset gate at \\(t=0\\)\nI lied earlier. I said we were going bottom up, but we’re actually approaching this top down because next in our discussion is the reset gate. The reasons is easy to tell: the update gate value \\(z_t\\) acts on the previous \\(h_{t-1}\\) as well as on \\(\\hat{h}_t\\) (the candidate memory update). But to make a discussion about \\(\\hat{h}_t\\) we need to discuss the reset gate \\(r_t\\) first.\nLet’s recall again how both the reset gate and newgate are computed\n\\[\\eqalign{\nr_t &= \\sigma(W_r x_t + U_r h_{t-1} + b_r) \\\\\n\\hat{h}_t &= \\phi(W_h x_t + U_h (r_t \\odot h_{t-1}) + b_h) \\\\\n}\n\\]\nFirst of all, you might have noticed already in the previous post that there was a different activation function in one step, which is \\(\\phi\\). But what is \\(\\phi\\)? It is the \\(\\tanh\\), which yields a plot like this:\n\n\nCode\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-10,10,100)\ny = np.tanh(x)\nsns.set_theme(style=\"darkgrid\")  # This gives the typical Seaborn look\n\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=x, y=y, linewidth=2)\nplt.title('Sigmoid Function')\nplt.axis(True)\nplt.xlabel('x')\nplt.ylabel('$\\sigma(x)$')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nPreviously \\(\\sigma\\) was squeezing its input in the range \\([0, 1]\\), whereas \\(\\phi\\) maps its input to a value in the range \\([-1, 1]\\). The reason why the update gate employs this activation function is that the range ([-1, 1]) allows the gate to dynamically amplify, suppress, or invert features from the previous hidden state, enabling more nuanced control over the update mechanism compared to the purely additive or multiplicative behavior of \\([0, 1]\\). Moreover: this symmetry and bidirectional scaling can improve gradient flow during training and help the model learn richer representations by incorporating both positive and negative adjustments to the hidden state. In other words \\(\\sigma\\) was telling: “How much of this should I add to this other thing?”, whereas \\(\\phi\\) is telling “How much of this should I add or remove from this other thing?”\nNow, why is this important? Remember that the GRU cell during its computation tries to build an internal hidden state that conveys some information. Think of the hidden state as some sort of scratchpad, where you take notes as you read more data. Sometimes some of the new information should be added to the previous state, sometimes new data should be ignored. Sometimes new data instead needs to kinda remove information from the state, in order to have fresher information.\nImagine for the sake of example to be a detective, which is trying to solve a mystery. You might get new information as time flows. At some point you might even have a track on a suspect and build your knowledge on that. But then at some point, you find out that your suspect was just cheating on his wife and as such you need to forget about him, otherwise you’ll focus on something that’s not needed in your ivnestigation.\nHow does our network know what to forget, what to keep, what to update and so on is the task of machine learning. But we can surely observe what happened here!\nFirst we need to study the reset gate \\(r_t\\), similarly to how we did before!\nAgain, there are three components that go through the sigmoid for the update gate:\n\\[\\eqalign{\nb_{ih_r} +  b_{hh_r} &= 1.4853122234344482 \\cr\ni_r &= 0.027704410254955292;  \\cr\nh_r &= -0.0;  \\cr\ni_r + h_r + b_{ih_r} + b_{hh_r} &= 1.513016700744629 \\cr\nr_t &= 0.819507896900177 \\cr\n}\n\\]\nAs we observed before the bias is king again, because it accounts for the largest part of the activation. But we can observe something else here, let’s compare how the flag affects both the update (\\(z_t\\)) and reset (\\(r_t\\)) gate:\nUpdate gate \\(z_t\\) with \\(\\texttt{flag} = 0\\)\n\\[\\eqalign{\ni_z + h_z + b_{ih_z} + b_{hh_z} &= 1.568204402923584\\cr\nz_t &= 0.8275274634361267 \\cr\n}\\] and with \\(\\texttt{flag} = 1\\) \\[\\eqalign{\ni_z + h_z + b_{ih_z} + b_{hh_z} &= -1.456658124923706\\cr\nz   _t &= 0.18897898495197296 \\cr\n}\\]\nIn Figure 1 we saw how \\(z_t\\) was affected pretty heavily by the flag at position \\(1\\) of the input item, whereas in this case we observe that the reset gate is not affected too much about the flag. Basically it looks like if we have \\(\\texttt{flag} = 0\\)\n\\[\\eqalign{\ni_r + h_r + b_{ih_r} + b_{hh_r} &= 1.513016700744629\\cr\nr_t &= 0.819507896900177 \\cr\n}\\]\nor \\(\\texttt{flag} = 1\\)\n\\[\\eqalign{\ni_r + h_r + b_{ih_r} + b_{hh_r} &= 2.146040916442871\\cr\nr_t &= 0.8952982425689697 \\cr\n}\\]\nthe reset gate \\(r_t\\) keeps taking a pretty large value (remember that sigmoid is in the range \\([0,1]\\), so you can consider it as a percentage, meaning that the reset gate is always above \\(80\\%\\))\nWhy is this? Let’s take a look at the weights involved. Again being this the first step in the sequence the hidden state is not yer involved and we can safely (FOR NOW!) ignore it.\n\\[\\eqalign{\ni_r &= x_t \\cdot W_{ih_r}^T\\cr\nW_{ih_r}^T &=\n\\begin{bmatrix}\n0.2308700829744339 \\\\ 0.6330242156982422\n\\end{bmatrix} \\cr\n}\\]\nLet’s compare it with the update gate weights: \\[\\eqalign{\ni_z &= x_t \\cdot W_{ih_z}^T\\cr\nW_{ih_z}^T &=\n\\begin{bmatrix}\n-0.4197608530521393 \\\\ -3.02486252784729\n\\end{bmatrix} \\cr\n}\\]\nWe can take two key observations:\n\nThe update weights are both negative and reset are both positive\nThe difference in magnitude is noteworthy: in the update gate is heavily affected by the flag input, moving the activation along the \\(\\hat{y}\\), whereas the reset gate is not so strongly affected by it.\n\nLet’s plot them both here where \\(y_{0z}\\) and \\(y_{1z}\\) are the update gates when \\(\\texttt{flag} = 0\\) and \\(\\texttt{flag} = 1\\), and \\(y_{0r}\\) and \\(y_{1r}\\) are the reset gates in the same cases, respectively.\n\n\nCode\nbr =  b_ih[0].item() + b_hh[0].item()\nx = np.linspace(0,1, 1000)\ny0 = x * W_ih[1][0].item()\ny1 = x * W_ih[1][0].item() + W_ih[1][1].item() \ny0_r = x * W_ih[0][0].item()\ny1_r = x * W_ih[0][0].item() + W_ih[0][1].item() \n\ny0_b = 1 / (1 + np.exp(-(y0 + br)))\ny1_b = 1 / (1 + np.exp(-(y1 + br)))\ny0_r_b = 1 / (1 + np.exp(-(y0_r + br)))\ny1_r_b = 1 / (1 + np.exp(-(y1_r + br)))\n\nsns.set_theme(style=\"darkgrid\")\n\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=x, y=y0_b, linewidth=2, linestyle='--', color='blue', label='$y_{0z}$')\nsns.lineplot(x=x, y=y1_b, linewidth=2, linestyle='--', color='orange', label='$y_{1z}$')\nsns.lineplot(x=x, y=y0_r_b, linewidth=2, linestyle='-', color='green', label='$y_{0r}$')\nsns.lineplot(x=x, y=y1_r_b, linewidth=2, linestyle='-', color='red', label='$y_{1r}$')\n\nplt.title('Comparison between $z_t$ and $r_t$')\nplt.xlabel('x')\nplt.ylabel('$i_r$')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThis is all cool and stuff… But I forgot to mention that is also useless. Just kidding. But thre’s an actual catch here: at \\(t=0\\) this is acting only on the bias term because \\(r_t\\) is applied via a Hadamard product (element-wise multiplication) to the previous hidden state plus the bias. Its purpose is to control how much of it we want to remember. But at \\(t=0\\), \\(h_t = h_0 = 0\\), meaning \\(r_0\\) only acts on the bias term!"
  },
  {
    "objectID": "projects/adding-problem-part2.html#the-new-gate-at-t0",
    "href": "projects/adding-problem-part2.html#the-new-gate-at-t0",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 2",
    "section": "5 The New gate at \\(t=0\\)",
    "text": "5 The New gate at \\(t=0\\)\nYou will find in literature multiple ways to refer to it, but I think the most useful is: candidate \\(\\hat{h}_t\\). Basically, it weights some of the input \\(\\mathbf{x}_t\\) and some of the \\(h_{t-1}\\) value using the reset gate \\(r_t\\) to decide how much it should be brought to the activation. In other words, the candidate \\(\\hat{h}_t\\) represents a proposed new hidden state that combines the current input \\(\\mathbf{x}_t\\) and the previous hidden state \\(h_{t-1}\\), modulated by the reset gate \\(r_t\\).\nLet’s recall how the candidate hidden state \\(\\hat{h}_t\\) is computed:\n\\[\n\\hat{h}_t = \\phi(W_h \\mathbf{x}_t + b_{{ih}_n} + r_t \\odot  [(U_h \\cdot h_{t-1}) + b_{{hh}_n}])\n\\] where \\(\\phi = \\tanh(\\cdot)\\)\n\n\n\n\n\n\nNote\n\n\n\nUp until now we always treated the bias terms as a single value, summing them up. But now we need to make extra care! Previously we had 2 terms added up and each of them had its bias, so we could safely add them either during the $$ product or later when inside the \\(\\sigma\\) sigmoid. But now, the bias term is multipled to the reset gate as much as the projected hidden state.\n\n\nOne term cancels out, because at \\(t=0\\) \\(h_0 = 0\\):\n\\[\\eqalign{\n\\hat{h}_t &= \\phi(W_h \\mathbf{x}_t + b_{{ih}_n} + r_t \\odot  [\\cancel{(U_h \\cdot h_{t-1})} + b_{{hh}_n}])\\cr\n\\hat{h}_t &= \\phi(W_h \\mathbf{x}_t + b_{{ih}_n} + r_t \\odot  b_{{hh}_n})\n}\\]\nfrom the previous section we learnt that \\(r_t\\) has always values above \\(.8\\), which at \\(t=0\\) means that we carry on with us \\(80\\%\\) of the bias in the activatoin \\(\\phi\\).\nNow this leaves us with these terms: \\[\\eqalign{\nb_{ih_n}  &= 0.6560405492782593 \\cr\nb_{hh_n} &= 0.16526484489440918 \\cr\ni_n &= -0.03407169133424759;  \\cr\nh_t &= 0.0;  \\cr\nh_t + b_{hh_n} &= 0.16526484489440918;  \\cr\n\\hat{h}_t &= \\phi(i_n + b_{ih_n} + r_t \\odot b_{hh_n}) \\cr\n\\hat{h}_t &= \\phi(-0.03407169133424759 + 0.6560405492782593 + 0.819507896900177 \\odot 0.16526484489440918) \\cr\n\\hat{h}_t &= \\phi(0.7574047034149629) \\cr\n\\hat{h}_t &= 0.639545738697052 \\cr\n}\n\\]\nAgain, bias is king! It accounts for basically the whole activation. And again a reminder about it:\n\n\n\n\n\n\nImportant\n\n\n\nBias is not about a single data point. Instead, it reflects a systematic shift in the entire dataset or even during the learning process. It tells us something about the data as a whole, rather than just the point we’re observing. This is so important: no matter what data we input, the bias would remain the same!"
  },
  {
    "objectID": "projects/adding-problem-part2.html#final-step",
    "href": "projects/adding-problem-part2.html#final-step",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 2",
    "section": "6 Final step",
    "text": "6 Final step\nSo we’re finally ready for our final hidden state:\n\\[\\eqalign{\nh_t &= (1 - z_t) \\cdot  \\hat{h} + z_t \\cdot h_0 \\cr\nh_t &= 0.1724725365638733 \\cdot 0.639545738697052 + \\cancel{0.8275274634361267 \\cdot 0.0}\\cr\nh_t &= 0.11030407580169665 + \\cancel{0.0}\\cr\nh_t &= 0.11030407249927521\\cr\n}\\]\nNow: this is a Recurrent Neural Network, so theory tells us that it should be able to handle sequences of arbitrary length. But is it?\nLet’s try to just project into the final output through the linear layer:\n\\[\n\\text{x} =\n\\begin{bmatrix}\n12 & 0 \\\\\n\\end{bmatrix}\n\\]\n\\[\n\\hat{y} = 41.076526045799255 \\neq 0\n\\]\nWoooh! That’s waaay off. What if we let the flag be one instead?\n\\[\n\\text{x} =\n\\begin{bmatrix}\n12 & 1 \\\\\n\\end{bmatrix}\n\\]\n\\[\n\\hat{y} = 101.57540893554688 \\gg 12\n\\]\nWay off again!\nWait… let’s see hat happend if we put 4 elements, 2 of which are set to true?\n\\[\n\\text{x} =\n\\left[\n\\begin{bmatrix}\n75 \\\\ 1\n\\end{bmatrix}\n\\begin{bmatrix}\n38 \\\\ 1\n\\end{bmatrix}\n\\begin{bmatrix}\n12 \\\\ 0\n\\end{bmatrix}\n\\begin{bmatrix}\n12 \\\\ 0\n\\end{bmatrix}\n\\right]\n\\]\nThis shows your 2-dimensional temporal sequence in the format you wanted. Let me know if you’d like to adjust the spacing or format further.\n\\[\n\\hat{y} = 112.553955078125 \\approxeq 113 = y\n\\]\nMmmmh! That works much better. But why? It seems the network has learned three key things:\n\nIt’s a sum: The network has learned to sum only the numbers flagged with \\(1\\). Well, that was actually the main task.\n\nFixed length \\(n = 4\\): It expects an input sequence of length \\(n = 4\\).\n\nExactly two flagged items: It assumes that exactly two elements in the input are flagged with \\(1\\).\n\n\n\n\n\n\n\nFirst Step Insights: What Have We Learned?\n\n\n\nAlright, let’s pause and take stock. After dissecting the GRU’s very first move, some things are becoming clearer. We’re seeing how much those bias terms are driving the initial behavior – they’re the unsung heroes at \\(t=0\\)! And it’s fascinating how the flag in the input is already so specifically wired to control the update gate (but less so the reset gate, interesting!). Plus, we’re starting to suspect the network is already ‘assuming’ a certain kind of input – fixed length, maybe even expecting those two flagged numbers.\nBut remember our starting questions? We’re just scratching the surface of “what these weights mean.” And the big “WHY?” – “how did we even get these weights?” – is still a complete mystery! This first step analysis is cool, but it’s just the beginning. To really understand this GRU, we gotta dig deeper into how these weights learned to be this way. And that’s where the real fun begins…"
  },
  {
    "objectID": "projects/adding-problem-part2.html#coming-next",
    "href": "projects/adding-problem-part2.html#coming-next",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 2",
    "section": "7 Coming next…",
    "text": "7 Coming next…\nIn this part, we took a thorough investagion into the inner workings of our GRU cell, dissecting its very first iteration piece by piece. We carefully traced how the input and weights influenced the ( z_t ), ( r_t ), and ( ) gates, step by step, uncovering how activations evolved and what they actually meant. Along the way, we stumbled upon three key insights about what the network had learned.\nBut that is still just the what—now it’s time to ask why.\nWhy did the model learn to do this seemingly strange thing? Was it always heading in this direction, or did it explore different strategies earlier in training? Were the weights trying to do something entirely different at first?\nTo answer these questions, we’ll rewind the clock and analyze how the model’s weights evolved over time. Did they start off chaotic before settling into a structured pattern? Were different strategies competing before the final approach emerged?\nAnd then, we’ll take things a step further. Instead of just observing the learned weights, we’ll craft our own by hand—designing a set that does precisely what we expect. Then, we’ll compare our manually created weights with the network’s chosen ones. Did the network find a more efficient solution? Did it take shortcuts we wouldn’t have thought of? Or did it stumble upon an elegant trick that we can learn from?\nNext up: reverse-engineering learning itself. Let’s crack this thing open!\nI LOVE THIS SO MUCH!"
  },
  {
    "objectID": "projects/adding-problem-part1.html",
    "href": "projects/adding-problem-part1.html",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 1",
    "section": "",
    "text": "In this blog post, I want to report on some insights I discovered about the training dynamics of a GRU network applied to a toy problem called the “Adding Problem.” This is a benchmark task for recurrent neural networks. The Adding Problem, introduced by Le, Jaitly, and Hinton, tests a model’s ability to selectively remember and sum relevant information over a sequence. I trained a small GRU model, visualized its training progress, and examined how its weights evolve during training. This low-dimensional weight space allowed me to observe unique insights about the learning process. This work is part of a larger research project where I aim to leverage training dynamics to create a lightweight network. This network will use a combination of meta-learning and Energy-Based Models (EBM) to adjust weights based on input sequences, allowing it to leverage generic pre-training and adapt to different situations."
  },
  {
    "objectID": "projects/adding-problem-part1.html#introduction",
    "href": "projects/adding-problem-part1.html#introduction",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 1",
    "section": "",
    "text": "In this blog post, I want to report on some insights I discovered about the training dynamics of a GRU network applied to a toy problem called the “Adding Problem.” This is a benchmark task for recurrent neural networks. The Adding Problem, introduced by Le, Jaitly, and Hinton, tests a model’s ability to selectively remember and sum relevant information over a sequence. I trained a small GRU model, visualized its training progress, and examined how its weights evolve during training. This low-dimensional weight space allowed me to observe unique insights about the learning process. This work is part of a larger research project where I aim to leverage training dynamics to create a lightweight network. This network will use a combination of meta-learning and Energy-Based Models (EBM) to adjust weights based on input sequences, allowing it to leverage generic pre-training and adapt to different situations."
  },
  {
    "objectID": "projects/adding-problem-part1.html#setup-and-data-generation",
    "href": "projects/adding-problem-part1.html#setup-and-data-generation",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 1",
    "section": "2 Setup and Data Generation",
    "text": "2 Setup and Data Generation\nFirst, I import the necessary libraries and define the function to generate the Adding Problem dataset. The dataset consists of sequences of synthetic numbers and a binary mask. The model’s task is to sum the numbers indicated by the ’1’s in the mask. The sequence length, mask disposition, and even the number of masked elements are variable. I tried to introduce this variability as an additional source of randomness that will be fundamental in the final network trained for playing a table game.\n\n\nCode\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm.notebook import trange, tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport os\n\n# Reproducibility\nRANDOM_SEED = 37\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\ntorch.cuda.manual_seed_all(RANDOM_SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Hyperparameters\nDELTA = 0\nSEQ_LEN = 4\nHIGH = 100\nN_SAMPLES = 10000\nTRAIN_SPLIT = 0.8\nBATCH_SIZE = 256\nLEARNING_RATE = 1e-4\nWEIGHT_DECAY = 1e-5\nCLIP_VALUE = 2.0\nNUM_EPOCHS = 3000\nHIDDEN_SIZE = 1\nOUTPUT_SIZE = 1\nINPUT_SIZE = 2\n\ndef adding_problem_generator(N, seq_len=6, high=1, delta=0.6):\n    actual_seq_len = np.random.randint(\n        int(seq_len * (1 - delta)), int(seq_len * (1 + delta))\n    ) if delta &gt; 0 else seq_len\n    num_ones = np.random.randint(2, min(actual_seq_len - 1, 4))\n    X_num = np.random.randint(low=0, high=high, size=(N, actual_seq_len, 1))\n    X_mask = np.zeros((N, actual_seq_len, 1))\n    Y = np.ones((N, 1))\n    for i in range(N):\n        positions = np.random.choice(actual_seq_len, size=num_ones, replace=False)\n        X_mask[i, positions] = 1\n        Y[i, 0] = np.sum(X_num[i, positions])\n    X = np.append(X_num, X_mask, axis=2)\n    return X, Y\n\nX, Y = adding_problem_generator(N_SAMPLES, seq_len=SEQ_LEN, high=HIGH, delta=DELTA)\n\ntraining_len = int(TRAIN_SPLIT * N_SAMPLES)\ntrain_X = X[:training_len]\ntest_X = X[training_len:]\ntrain_Y = Y[:training_len]\ntest_Y = Y[training_len:]\n\ntrain_dataset = TensorDataset(\n    torch.tensor(train_X).float(), torch.tensor(train_Y).float()\n)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\ntest_dataset = TensorDataset(\n    torch.tensor(test_X).float(), torch.tensor(test_Y).float()\n)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
  },
  {
    "objectID": "projects/adding-problem-part1.html#model-definition-and-training",
    "href": "projects/adding-problem-part1.html#model-definition-and-training",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 1",
    "section": "3 Model Definition and Training",
    "text": "3 Model Definition and Training\nI defined my GRU model, which consists of a single GRU layer followed by a linear layer. I used orthogonal weight initialization for the GRU and Xavier initialization for the linear layer, following best practices for RNN training. I also defined the evaluation function and the main training loop. Crucially, I added code to store the model’s weights at different epochs. This allows me to analyze their evolution later.\n\n\nCode\nclass AddingProblemGRU(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(AddingProblemGRU, self).__init__()\n        self.gru = nn.GRU(\n            input_size, hidden_size, num_layers=1, batch_first=True\n        )\n        self.linear = nn.Linear(hidden_size, output_size)\n        self.init_weights()\n\n    def init_weights(self):\n        for name, param in self.gru.named_parameters():\n            if \"weight\" in name:\n                nn.init.orthogonal_(param)\n            elif \"bias\" in name:\n                nn.init.constant_(param, 0)\n        nn.init.xavier_uniform_(self.linear.weight)\n        nn.init.zeros_(self.linear.bias)\n\n    def forward(self, x):\n        out, hn = self.gru(x)\n        output = self.linear(out[:, -1, :])\n        return output, out\n\n\n\n\nCode\nimport json\n\n# File paths for saved data\ntrain_losses_path = \"train_losses.json\"\ntest_losses_path = \"test_losses.json\"\nall_weights_path = \"all_weights.json\"\nmodel_save_path = (\n    f\"gru_adding_problem_model_epochs_{NUM_EPOCHS}_hidden_{HIDDEN_SIZE}.pth\"\n)\n\nFORCE_TRAIN = False\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ncriterion = nn.MSELoss()\ndef evaluate(model, data_loader, criterion, high):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for inputs, labels in data_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            inputs[:, :, 0] /= high\n            outputs, _ = model(inputs)\n            outputs = outputs * high\n            loss = criterion(outputs, labels)\n            total_loss += loss.item() * inputs.size(0)\n    return total_loss / len(data_loader.dataset)\n\n\n# Try to load data from files\nif FORCE_TRAIN == False and os.path.exists(train_losses_path) and os.path.exists(test_losses_path) and os.path.exists(all_weights_path) and os.path.exists(model_save_path):\n    with open(train_losses_path, \"r\") as f:\n        train_losses = json.load(f)\n    with open(test_losses_path, \"r\") as f:\n        test_losses = json.load(f)\n    with open(all_weights_path, \"r\") as f:\n        all_weights_loaded = json.load(f)\n\n    # Convert loaded weights (which are lists) back to numpy arrays\n    all_weights = []\n    for epoch_weights_list in all_weights_loaded:\n        epoch_weights_dict = {}\n        for name, weights_list in epoch_weights_list.items():\n            epoch_weights_dict[name] = np.array(weights_list)\n        all_weights.append(epoch_weights_dict)\n    #load model\n    model = AddingProblemGRU(\n    input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, output_size=OUTPUT_SIZE)\n    model.load_state_dict(torch.load(model_save_path))\n    model.to(device)\n\n\n\nelse:\n    model = AddingProblemGRU(\n        input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, output_size=OUTPUT_SIZE\n    )\n    model.to(device)\n\n    optimizer = torch.optim.Adam(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode=\"min\", factor=0.5, patience=5, min_lr=1e-6, verbose=False\n    )\n\n\n    \n\n    train_losses = []\n    test_losses = []\n    all_weights = []\n\n    for epoch in trange(NUM_EPOCHS, desc=\"Epoch\"):\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            inputs[:, :, 0] /= HIGH\n            labels_scaled = labels / HIGH\n            optimizer.zero_grad()\n            outputs, _ = model(inputs)\n            loss = criterion(outputs, labels_scaled)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_VALUE)\n            optimizer.step()\n            running_loss += loss.item() * inputs.size(0)\n\n        epoch_loss = running_loss / len(train_loader.dataset)\n        train_losses.append(epoch_loss)\n\n        if epoch % 49 == 0:\n            test_loss = evaluate(model, test_loader, criterion, HIGH)\n            test_losses.append(test_loss)\n            scheduler.step(test_loss)\n\n            weights_dict = {}\n            for name, param in model.named_parameters():\n                weights_dict[name] = param.data.cpu().numpy().copy()\n            all_weights.append(weights_dict)\n        else:\n            test_losses.append(None)\n\n    # Save data to files\n    with open(train_losses_path, \"w\") as f:\n        json.dump(train_losses, f)\n    with open(test_losses_path, \"w\") as f:\n        json.dump(test_losses, f)\n    # Convert weights to lists for JSON serialization\n    all_weights_serializable = [\n        {k: v.tolist() for k, v in epoch_weights.items()}\n        for epoch_weights in all_weights\n    ]\n    with open(all_weights_path, \"w\") as f:\n        json.dump(all_weights_serializable, f)\n\n    # Save Model\n    model_save_path = (\n        f\"gru_adding_problem_model_epochs_{NUM_EPOCHS}_hidden_{HIDDEN_SIZE}.pth\"\n    )\n    torch.save(model.state_dict(), model_save_path)"
  },
  {
    "objectID": "projects/adding-problem-part1.html#training-metrics-visualization",
    "href": "projects/adding-problem-part1.html#training-metrics-visualization",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 1",
    "section": "4 Training Metrics Visualization",
    "text": "4 Training Metrics Visualization\nThe following two plots show the training and validation loss curves. These provide a visual indication of how well the model is learning and whether it’s overfitting (which would be indicated by the training loss continuing to decrease while the validation loss starts to increase). In my case, the training loss closely follows the validation loss, suggesting that I likely reached a global minimum.\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.plot(train_losses, label=\"Training Loss\")\n# plt.plot(range(0,NUM_EPOCHS,50), [x for x in test_losses if x is not None], label=\"Validation Loss\") #plot only not none values\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"MSE Loss\")\nplt.ylim(0, max(train_losses)*1.05)\nplt.title(\"Training and Validation Loss over Epochs\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\nplt.figure(figsize=(10, 6))\n# plt.plot(train_losses, label=\"Training Loss\")\nplt.plot(range(0,NUM_EPOCHS+1,50), [x for x in test_losses if x is not None][0:len(range(0,NUM_EPOCHS+1,50))], label=\"Validation Loss\") #plot only not none values\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"MSE Loss\")\nplt.title(\"Training and Validation Loss over Epochs\")\nplt.ylim(0, max([x for x in test_losses if x is not None])*1.05)\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\nTraining and Validation Loss\n\n\n\n\n\n\n\n\n\n\n\nIt’s important to note that if the model had overfit the data, then the weight analysis would have been nearly useless."
  },
  {
    "objectID": "projects/adding-problem-part1.html#weight-evolution-analysis",
    "href": "projects/adding-problem-part1.html#weight-evolution-analysis",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 1",
    "section": "5 Weight Evolution Analysis",
    "text": "5 Weight Evolution Analysis\nThis is the core of my analysis. I examine how the model’s weights change over the course of training. Because my GRU has a small number of parameters, I can visualize this evolution effectively. The PyTorch documentation helps us understand the names of the involved parameters:\n\nweight_ih_l[k] - the learnable input-hidden weights of the \\(k^{th}\\) layer (\\(W_{ir}\\)|\\(W_{iz}\\)|\\(W_{in}\\)), of shape \\((3 * \\text{hidden\\_size}, \\text{input\\_size})\\) for \\(k=0\\). In my example, with \\(\\text{hidden\\_size} = 1\\) and \\(\\text{input\\_size} = 2\\), this is \\((3*1, 2)\\), or \\((3, 2)\\).\nweight_hh_l[k] - the learnable hidden-hidden weights of the \\(k^{th}\\) layer (\\(W_{hr}\\)|\\(W_{hz}\\)|\\(W_{hn}\\)), of shape \\((3 * \\text{hidden\\_size}, \\text{hidden\\_size})\\). In my example, with \\(\\text{hidden\\_size} = 1\\), this is \\((3*1, 1)\\), or \\((3, 1)\\).\nbias_ih_l[k] - the learnable input-hidden bias of the \\(k^{th}\\) layer (\\(b_{ir}\\)|\\(b_{iz}\\)|\\(b_{in}\\)), of shape \\((3 * \\text{hidden\\_size})\\). In my example, with \\(\\text{hidden\\_size} = 1\\), this is \\((3*1)\\), or \\((3)\\).\nbias_hh_l[k] - the learnable hidden-hidden bias of the \\(k^{th}\\) layer (\\(b_{hr}\\)|\\(b_{hz}\\)|\\(b_{hn}\\)), of shape \\((3 * \\text{hidden\\_size})\\). In my example, with \\(\\text{hidden\\_size} = 1\\), this is \\((3*1)\\), or \\((3)\\).\n\nI focused on the key weight matrices:\n\ngru.weight_ih_l0: Input-to-hidden weights. In my example, this has size \\((3 \\times 2)\\).\ngru.weight_hh_l0: Hidden-to-hidden weights. In my example, this has size \\((3 \\times 1)\\).\nlinear.weight: Weights of the final linear layer. In my example, with \\(\\text{hidden\\_size} = 1\\) and \\(\\text{output\\_size} = 1\\), this has size \\((1 \\times 1)\\). I included this to allow for potentially adding an additional dimension to the hidden layer later.\n\nNow, here’s how GRUs work (it might look a little intimidating at first, but it’s actually quite straightforward):\nInitially, for \\(t = 0\\), the output vector is \\(h_0 = 0\\).\n\\[\\begin{aligned}\nz_t &= \\sigma(W_z x_t + U_z h_{t-1} + b_z) \\\\\nr_t &= \\sigma(W_r x_t + U_r h_{t-1} + b_r) \\\\\n\\hat{h}_t &= \\phi(W_h x_t + U_h (r_t \\odot h_{t-1}) + b_h) \\\\\nh_t &= (1 - z_t) \\odot h_{t-1} + z_t \\odot \\hat{h}_t\n\\end{aligned}\\]\nThe \\(W_{*}\\) weights are the first set of weights (i.e., gru.weight_ih_l0), while the \\(U_{*}\\) weights are the second set of weights (i.e., gru.weight_hh_l0). The bias terms are straightforward, so I won’t go into detail.\nI created plots showing the values of these weights at different stages of training. This visualization helps understand how the GRU learns to solve the adding problem. I used a heatmap to represent the weight matrices, making it easy to spot patterns and changes.\n\n\nCode\n# Select a subset of epochs for clearer visualization\nepochs_to_plot = [0, len(all_weights) // 4, len(all_weights) // 2, len(all_weights) - 1]\nselected_weights = [all_weights[i] for i in epochs_to_plot]\n\n# Plotting function for a single weight matrix\ndef plot_weight_matrix(weight_matrix, title, ax):\n    sns.heatmap(weight_matrix, annot=True, fmt=\".2f\", cmap=\"viridis\", cbar=True, ax=ax)\n    ax.set_title(title)\n\nfig, axes = plt.subplots(len(selected_weights), 3, figsize=(15, 5 * len(selected_weights)))\n\nfor i, epoch_weights in enumerate(selected_weights):\n  plot_weight_matrix(epoch_weights['gru.weight_ih_l0'], f'Epoch {epochs_to_plot[i]*50}: Input-to-Hidden', axes[i, 0])\n  plot_weight_matrix(epoch_weights['gru.weight_hh_l0'], f'Epoch {epochs_to_plot[i]*50}: Hidden-to-Hidden', axes[i, 1])\n  plot_weight_matrix(epoch_weights['linear.weight'], f'Epoch {epochs_to_plot[i]*50}: Linear Layer', axes[i, 2])\n\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nEvolution of GRU + Linear Weights"
  },
  {
    "objectID": "projects/adding-problem-part1.html#weight-dynamics-and-learned-strategy",
    "href": "projects/adding-problem-part1.html#weight-dynamics-and-learned-strategy",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 1",
    "section": "6 Weight Dynamics and Learned Strategy",
    "text": "6 Weight Dynamics and Learned Strategy\nThis section combines the detailed weight evolution visualizations (heatmaps and line plots) with the analysis of the GRU’s learned strategy. This provides a single, cohesive narrative.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Assuming all_weights is already loaded\n\nweight_types = ['gru.weight_ih_l0', 'gru.weight_hh_l0', 'linear.weight']\nweight_type_names = ['Input-to-Hidden', 'Hidden-to-Hidden', 'Linear Layer']\n\nfor i, weight_type in enumerate(weight_types):\n    evolution_data = []\n    for epoch_weights in all_weights:\n        weight_matrix = epoch_weights[weight_type]\n        if weight_type == 'gru.weight_ih_l0':\n            evolution_data.append(weight_matrix.flatten(order='F'))\n        else:\n            evolution_data.append(weight_matrix.flatten())\n\n    evolution_heatmap_data = np.array(evolution_data)\n\n    fig, ax = plt.subplots(figsize=(10, 8))\n    sns.heatmap(evolution_heatmap_data, cmap=\"viridis\", cbar=True, ax=ax, xticklabels=False)\n\n    ax.set_title(f'Heatmap of {weight_type_names[i]} Weights Over Epochs')\n    ax.set_ylabel('Epoch (x49)')\n\n    ax.axhline(2, color='#EE4B2B', linestyle='--', linewidth=2)\n    ax.axhline(10, color='#EE4B2B', linestyle='--', linewidth=2)\n    ax.axhline(15, color='#EE4B2B', linestyle='--', linewidth=2)\n\n\n    if weight_type == 'gru.weight_ih_l0':\n        ax.axvline(3, color='#EE4B2B')\n        ax.text(x=0.1, y=60, s=\"Input[0]\", color='#EE4B2B', fontsize=8,\n            ha='left', va='bottom', bbox=dict(facecolor='black', alpha=0.3))\n        ax.text(x=3.1, y=60, s=\"Input[1]\", color='#EE4B2B', fontsize=8,\n                ha='left', va='bottom', bbox=dict(facecolor='black', alpha=0.3))\n\n\n\n    ax.text(x=0.1, y=2-.2, s=\"Epoch 100\", color='#EE4B2B', fontsize=8,\n        ha='left', va='bottom')\n    ax.text(x=0.1, y=10-.2, s=\"Epoch 500\", color='#EE4B2B', fontsize=8,\n            ha='left', va='bottom')\n    ax.text(x=0.1, y=15-.2, s=\"Epoch 750\", color='#EE4B2B', fontsize=8,\n        ha='left', va='bottom')\n\n\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\nCombined Heatmap of Weight Evolution Over Epochs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo visualize weight changes directly, I used line plots to show the evolution of individual weights across sampled epochs. These plots complement the heatmaps by providing a more detailed view of how each weight changes in magnitude and direction. Plotting signed values reveals symmetries and patterns that emerge during training, even in a simple network like mine.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np  # Make sure numpy is imported\n\ndef plot_gru_weight_evolution(all_weights, title_prefix):\n    \"\"\"\n    Plots the evolution of GRU weight matrices (input-hidden, hidden-hidden)\n    separated by gate (Reset, Update, New).\n    \"\"\"\n\n    def plot_single_gate_weights(weight_values, gate_name, title):\n        \"\"\"Helper function to plot weights for a single gate.\"\"\"\n        df = pd.DataFrame(weight_values).T\n        df.columns = [f\"Epoch {i*49}\" for i in range(len(all_weights))]\n\n        plt.figure(figsize=(12, 6))\n        for i in range(df.shape[0]):\n            plt.plot(df.columns, df.iloc[i], label=f\"{gate_name} Weight {i}\")  # Add weight index to label\n\n        plt.title(title)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Weight Value\")\n        plt.xticks(rotation=90)\n        plt.legend()\n        plt.grid(True)\n        plt.axvline(x=\"Epoch 98\")\n        plt.axvline(x=\"Epoch 490\")\n        plt.axvline(x=\"Epoch 735\")\n        plt.tight_layout()\n        plt.show()\n\n    # --- Input-to-Hidden Weights ---\n    ih_weights = [all_weights[i][\"gru.weight_ih_l0\"] for i in range(len(all_weights))]\n    # Split into Reset, Update, and New gate weights (assuming standard GRU structure)\n    ih_reset_weights = [w[0:w.shape[0]//3, :].flatten() for w in ih_weights]\n    ih_update_weights = [w[w.shape[0]//3:2*w.shape[0]//3, :].flatten() for w in ih_weights]\n    ih_new_weights = [w[2*w.shape[0]//3:, :].flatten() for w in ih_weights]\n\n    plot_single_gate_weights(ih_reset_weights, \"Reset\", f\"{title_prefix} - Input-Hidden (Reset Gate)\")\n    plot_single_gate_weights(ih_update_weights, \"Update\", f\"{title_prefix} - Input-Hidden (Update Gate)\")\n    plot_single_gate_weights(ih_new_weights, \"New\", f\"{title_prefix} - Input-Hidden (New Gate)\")\n\n    # --- Hidden-to-Hidden Weights ---\n    hh_weights = [all_weights[i][\"gru.weight_hh_l0\"] for i in range(len(all_weights))]\n    # Split into Reset, Update, and New gate weights\n    hh_reset_weights = [w[0:w.shape[0]//3, :].flatten() for w in hh_weights]\n    hh_update_weights = [w[w.shape[0]//3:2*w.shape[0]//3, :].flatten() for w in hh_weights]\n    hh_new_weights = [w[2*w.shape[0]//3:, :].flatten() for w in hh_weights]\n\n    plot_single_gate_weights(hh_reset_weights, \"Reset\", f\"{title_prefix} - Hidden-Hidden (Reset Gate)\")\n    plot_single_gate_weights(hh_update_weights, \"Update\", f\"{title_prefix} - Hidden-Hidden (Update Gate)\")\n    plot_single_gate_weights(hh_new_weights, \"New\", f\"{title_prefix} - Hidden-Hidden (New Gate)\")\n\n\n\ndef plot_linear_weight_evolution(all_weights, title):\n    \"\"\"Plots the evolution of the linear layer weights.\"\"\"\n    weight_values = [all_weights[i][\"linear.weight\"].flatten() for i in range(len(all_weights))]\n\n    df = pd.DataFrame(weight_values).T\n    df.columns = [f\"Epoch {i*49}\" for i in range(len(all_weights))]\n\n    plt.figure(figsize=(12, 6))\n    for i in range(df.shape[0]):\n        plt.plot(df.columns, df.iloc[i], label=f\"Linear Weight {i}\")\n\n    plt.title(title)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Weight Value\")\n    plt.xticks(rotation=90)\n    plt.legend()\n    plt.grid(True)\n    plt.axvline(x=\"Epoch 98\")\n    plt.axvline(x=\"Epoch 490\")\n    plt.axvline(x=\"Epoch 735\")\n    plt.tight_layout()\n    plt.show()\n\n\n# Create the plots\nplot_gru_weight_evolution(all_weights, \"GRU Weight Evolution\")\nplot_linear_weight_evolution(all_weights, \"Linear Layer Weight Evolution\")\n\n\n\n\n\nWeight Dynamics Over Training"
  },
  {
    "objectID": "projects/adding-problem-part1.html#illustrative-example-forward-pass",
    "href": "projects/adding-problem-part1.html#illustrative-example-forward-pass",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 1",
    "section": "7 Illustrative Example: Forward Pass",
    "text": "7 Illustrative Example: Forward Pass\nTo further clarify how the trained GRU operates, I’ll walk through a single forward pass with a concrete example. This will demonstrate the roles of the input-to-hidden weights, hidden-to-hidden weights, and the final linear layer, as well as how the binary mask selects the relevant input values.\nExample Input:\nThe input sequence is represented as a tensor:\n\\[\n\\text{input\\_sequence} =\n\\begin{bmatrix}\n12 & 0 \\\\\n37 & 1 \\\\\n12 & 0 \\\\\n21 & 1\n\\end{bmatrix}\n\\]\nThe problem requires summing the values in the first dimension (column) only when the corresponding value in the second dimension (column) is 1 (the flag). In this example:\n\nThe second element, \\([37, 1]\\), has a flag of 1, so 37 is included in the sum.\nThe fourth element, \\([21, 1]\\), has a flag of 1, so 21 is included in the sum.\nThe first and third elements have flags of 0, so their first-dimension values (12 and 12) are excluded.\n\nTherefore, the expected output is \\(37 + 21 = 58\\).\n\n\nCode\ninput_sequence = torch.tensor([\n    [12,0],\n    [37,1],\n    [12,0],\n    [21,1],\n]).float().unsqueeze(0)\n\n\ninput_sequence[:, :, 0] /= HIGH\n\n\nAnd a quick sanity check applying my model. The result is: \\(57.80980587005615\\approx58\\)\nwhich is really close to the actual sum (\\(37 + 21 = 58\\))!!!!\nLoaded Weights (from the last training epoch):\nI used the weights from the last epoch of training, which I previously saved. This ensures that I’m using the fully trained model. Let’s get the weights:\n\n\nCode\n# Get weights from the last training epoch.  'all_weights' is populated by the loading/training section.\nlast_epoch_weights = model.state_dict() #all_weights[-1]\n\n# Extract the relevant weight matrices\nW_ih = torch.tensor(last_epoch_weights['gru.weight_ih_l0']).float()  # Input-to-hidden\nW_hh = torch.tensor(last_epoch_weights['gru.weight_hh_l0']).float()  # Hidden-to-hidden\nb_ih = torch.tensor(last_epoch_weights['gru.bias_ih_l0']).float()  # Input-to-hidden bias\nb_hh = torch.tensor(last_epoch_weights['gru.bias_hh_l0']).float()  # Hidden-to-hidden bias\nW_linear = torch.tensor(last_epoch_weights['linear.weight']).float() # Linear layer weights\nb_linear = torch.tensor(last_epoch_weights['linear.bias']).float()   # Linear layer bias\n\n\nNow, let’s walk through the GRU step-by-step for each element in our input sequence.\nFor each element, I calculate the Update Gate (\\(z_t\\)), Reset Gate (\\(r_t\\)), New Gate (\\(\\tilde{h}_t\\)), and Hidden State (\\(h_t\\)).\nI start by initializing the hidden state to zero:\n\n\nCode\nh_t = torch.zeros(1, 1, 1) # Initialize hidden state\n\n\n\\[\nh_t = 0.0\n\\]\n\n7.1 Step 1: Input \\([12, 0]\\)\nInput for Step 1: \\[\nx_1 = \\begin{bmatrix} 0.12 & 0 \\end{bmatrix}\n\\]\n\nUpdate Gate \\(z_1\\): \\[\nz_1 = \\sigma(x_1 W_{iz}^T + h_0 W_{hz}^T + b_{iz})\n\\]\n\n\nCode\nx_1 = input_sequence[:, 0, :]\nh = torch.zeros(1, 1, 1)\n\n# Stable version (using torch.matmul)\ngi_stable = torch.matmul(x_1, W_ih.t()) + b_ih\ngh_stable = torch.matmul(h, W_hh.t()) + b_hh\n\n# Get gates\ni_r, i_z, i_n = gi_stable.chunk(3, dim=1)\nh_r, h_z, h_n = gh_stable.chunk(3, dim=2)\n\n# Apply gate operations with controlled precision\nresetgate = torch.sigmoid(i_r + h_r)\nupdategate = torch.sigmoid(i_z + h_z)\nnewgate = torch.tanh(i_n + (resetgate * h_n))\n\nhy = (1 - updategate) * newgate + updategate * h\nh_t = hy\n\n\nUpdate Gate: \\[\nz_1 = 0.8275274634361267\n\\]\nReset Gate \\(r_1\\): \\[\nr_1 = \\sigma(x_1 W_{ir}^T + h_0 W_{hr}^T + b_{ir})\n\\]\nReset Gate: \\[\nr_1 = 0.819507896900177\n\\]\nNew Gate \\(\\tilde{h}_1\\): \\[\n\\tilde{h}_1 = \\tanh(x_1 W_{in}^T + (r_1 \\odot h_0) W_{hn}^T + b_{in})\n\\]\nNew Gate: \\[\n\\tilde{h}_1 = 0.639545738697052\n\\]\nHidden State \\(h_1\\): \\[\nh_1 = (1 - z_1) \\odot h_0 + z_1 \\odot \\tilde{h}_1\n\\]\nHidden State: \\[\nh_1 = 0.11030407249927521\n\\]\n\n\n\n7.2 Step 2: Input \\([37, 1]\\)\nInput for Step 2: \\[\nx_2 = \\begin{bmatrix} 0.37 & 1 \\end{bmatrix}\n\\]\n\nUpdate Gate \\(z_2\\): \\[\nz_2 = \\sigma(x_2 W_{iz}^T + h_1 W_{hz}^T + b_{iz})\n\\]\n\n\nCode\nx_2 = input_sequence[:, 1, :]\n\n# Stable version (using torch.matmul)\ngi_stable = torch.matmul(x_2, W_ih.t()) + b_ih\ngh_stable = torch.matmul(h_t, W_hh.t()) + b_hh\n\n# Get gates\ni_r, i_z, i_n = gi_stable.chunk(3, dim=1)\nh_r, h_z, h_n = gh_stable.chunk(3, dim=2)\n\n# Apply gate operations\nresetgate2 = torch.sigmoid(i_r + h_r)\nupdategate2 = torch.sigmoid(i_z + h_z)\nnewgate2 = torch.tanh(i_n + (resetgate2 * h_n))\n\nhy = (1 - updategate2) * newgate2 + updategate2 * h_t\nh_t = hy\n\n\nUpdate Gate: \\[\nz_2 = 0.15969596803188324\n\\]\nReset Gate \\(r_2\\): \\[\nr_2 = \\sigma(x_2 W_{ir}^T + h_1 W_{hr}^T + b_{ir})\n\\]\nReset Gate: \\[\nr_2 = 0.89918053150177\n\\]\nNew Gate \\(\\tilde{h}_2\\): \\[\n\\tilde{h}_2 = \\tanh(x_2 W_{in}^T + (r_2 \\odot h_1) W_{hn}^T + b_{in})\n\\]\nNew Gate: \\[\n\\tilde{h}_2 = -0.001833615591749549\n\\]\nHidden State \\(h_2\\): \\[\nh_2 = (1 - z_2) \\odot h_1 + z_2 \\odot \\tilde{h}_2\n\\]\nHidden State: \\[\nh_2 = 0.016074320301413536\n\\]\n\n\n\n7.3 Step 3: Input \\([12, 0]\\)\nInput for Step 3: \\[\nx_3 = \\begin{bmatrix} 0.12 & 0 \\end{bmatrix}\n\\]\n\nUpdate Gate \\(z_3\\): \\[\nz_3 = \\sigma(x_3 W_{iz}^T + h_2 W_{hz}^T + b_{iz})\n\\]\n\n\nCode\nx_3 = input_sequence[:, 2, :]\n\n# Stable version (using torch.matmul)\ngi_stable = torch.matmul(x_3, W_ih.t()) + b_ih\ngh_stable = torch.matmul(h_t, W_hh.t()) + b_hh\n\n# Get gates\ni_r, i_z, i_n = gi_stable.chunk(3, dim=1)\nh_r, h_z, h_n = gh_stable.chunk(3, dim=2)\n\n# Apply gate operations\nresetgate3 = torch.sigmoid(i_r + h_r)\nupdategate3 = torch.sigmoid(i_z + h_z)\nnewgate3 = torch.tanh(i_n + (resetgate3 * h_n))\n\nhy = (1 - updategate3) * newgate3 + updategate3 * h_t\nh_t = hy\n\n\nUpdate Gate: \\[\nz_3 = 0.8254608511924744\n\\]\nReset Gate \\(r_3\\): \\[\nr_3 = \\sigma(x_3 W_{ir}^T + h_2 W_{hr}^T + b_{ir})\n\\]\nReset Gate: \\[\nr_3 = 0.8191711902618408\n\\]\nNew Gate \\(\\tilde{h}_3\\): \\[\n\\tilde{h}_3 = \\tanh(x_3 W_{in}^T + (r_3 \\odot h_2) W_{hn}^T + b_{in})\n\\]\nNew Gate: \\[\n\\tilde{h}_3 = 0.648215115070343\n\\]\nHidden State \\(h_3\\): \\[\nh_3 = (1 - z_3) \\odot h_2 + z_3 \\odot \\tilde{h}_3\n\\]\nHidden State: \\[\nh_3 = 0.12640763819217682\n\\]\n\n\n\n7.4 Step 4: Input \\([21, 1]\\)\nInput for Step 4: \\[\nx_4 = \\begin{bmatrix} 0.21 & 1 \\end{bmatrix}\n\\]\n\nUpdate Gate \\(z_4\\): \\[\nz_4 = \\sigma(x_4 W_{iz}^T + h_3 W_{hz}^T + b_{iz})\n\\]\n\n\nCode\nx_4 = input_sequence[:, 3, :]\n\n\n# Stable version (using torch.matmul)\ngi_stable = torch.matmul(x_4, W_ih.t()) + b_ih\ngh_stable = torch.matmul(h_t, W_hh.t()) + b_hh\n\n# Get gates\ni_r, i_z, i_n = gi_stable.chunk(3, dim=1)\nh_r, h_z, h_n = gh_stable.chunk(3, dim=2)\n\n# Apply gate operations\nresetgate4 = torch.sigmoid(i_r + h_r)\nupdategate4 = torch.sigmoid(i_z + h_z)\nnewgate4 = torch.tanh(i_n + (resetgate4 * h_n))\n\nhy = (1 - updategate4) * newgate4 + updategate4 * h_t\nh_t = hy\n\n\nUpdate Gate: \\[\nz_4 = 0.1668987274169922\n\\]\nReset Gate \\(r_4\\): \\[\nr_4 = \\sigma(x_4 W_{ir}^T + h_3 W_{hr}^T + b_{ir})\n\\]\nReset Gate: \\[\nr_4 = 0.8955692052841187\n\\]\nNew Gate \\(\\tilde{h}_4\\): \\[\n\\tilde{h}_4 = \\tanh(x_4 W_{in}^T + (r_4 \\odot h_3) W_{hn}^T + b_{in})\n\\]\nNew Gate: \\[\n\\tilde{h}_4 = 0.05876209959387779\n\\]\nHidden State \\(h_4\\): \\[\nh_4 = (1 - z_4) \\odot h_3 + z_4 \\odot \\tilde{h}_4\n\\]\nHidden State: \\[\nh_4 = 0.07005205750465393\n\\]\n\n\n\n7.5 Step 5: Linear Layer\nFinally, the hidden state \\(h_4\\) is passed through the linear layer to produce the output.\n\\[\n\\text{output} = h_4 W_{\\text{linear}}^T + b_{\\text{linear}}\n\\]\n\n\nCode\noutput = h_t @ W_linear.T + b_linear\n\n\nLinear Layer output: \\[\n\\text{output} = 57.8097939491272\n\\]\nThe final result, as we can see, when approximated using the ceiling function, matches the expected result of 58."
  },
  {
    "objectID": "projects/adding-problem-part1.html#coming-next",
    "href": "projects/adding-problem-part1.html#coming-next",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 1",
    "section": "8 Coming next…",
    "text": "8 Coming next…\nThis post set some ground work for a more thorough analysis of the model’s weights. I’ve trained a simple model, and now it’s time to crack it open and see what makes it tick. Think of it like this: usually these models are black boxes, and the weights that are inside and make them move or somwhat foggy to us. I’m going to be taking a close look at what those gears and levers look like after all the training is done – are they big or small? Are they organized in any particular way? What do they do?\nBut it’s not enough just to look at them. I want to figure out what they actually do. Each weight contributes, in some way, to the final output of the model. I’ll be exploring how different weights, or groups of weights, affect the model’s predictions. I might even try tweaking them a bit to see what happens! This is like figuring out which lever controls which part of the machine.\nAnd finally, the really fun part: I’ll try to guess how they got that way. The training process is like a long and winding road, and the weights are constantly changing along the way. I’ll try to reconstruct that journey, piecing together clues from the final weights to understand the path the model took to learn. It’s a bit like detective work, trying to figure out the story behind the final result and I just love that! Get ready for some serious model investigation!"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Research Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nFeb 18, 2025\n\n\nAnalyzing GRU Training Dynamics on the Adding Problem - Part 1\n\n\nYour Name\n\n\n\n\nFeb 20, 2025\n\n\nAnalyzing GRU Training Dynamics on the Adding Problem - Part 2\n\n\nLuca Simonetti\n\n\n\n\nJan 30, 2025\n\n\nConnect 4\n\n\nLuca Simonetti\n\n\n\n\n \n\n\nHopfield Network Demo\n\n\n \n\n\n\n\nFeb 3, 2025\n\n\nOf RNNs: Ising, Hopfield, Math and Modernity\n\n\n \n\n\n\n\nJan 30, 2025\n\n\nSpatial Spike Neural Networks\n\n\nLuca Simonetti\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/hopfield.html#hopfield-network",
    "href": "projects/hopfield.html#hopfield-network",
    "title": "Hopfield Network Demo",
    "section": "Hopfield Network",
    "text": "Hopfield Network"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Luca Simonetti, yet another idiot",
    "section": "",
    "text": "Luca Simonetti\n\n\nFeb 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLuca Simonetti\n\n\nFeb 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLuca Simonetti\n\n\nFeb 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLuca Simonetti\n\n\nJan 30, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#blog",
    "href": "index.html#blog",
    "title": "Luca Simonetti, yet another idiot",
    "section": "",
    "text": "Luca Simonetti\n\n\nFeb 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLuca Simonetti\n\n\nFeb 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLuca Simonetti\n\n\nFeb 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLuca Simonetti\n\n\nJan 30, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Luca Simonetti, yet another idiot",
    "section": "Projects",
    "text": "Projects\n\n\n\n\n\n\n\n\n\n\nAnalyzing GRU Training Dynamics on the Adding Problem - Part 1\n\n\n\n\n\n\nYour Name\n\n\nFeb 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing GRU Training Dynamics on the Adding Problem - Part 2\n\n\n\n\n\n\nLuca Simonetti\n\n\nFeb 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConnect 4\n\n\n\n\n\n\nLuca Simonetti\n\n\nJan 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHopfield Network Demo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOf RNNs: Ising, Hopfield, Math and Modernity\n\n\n\n\n\n\n\n\n\nFeb 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial Spike Neural Networks\n\n\n\n\n\n\nLuca Simonetti\n\n\nJan 30, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/group-prime-order.html",
    "href": "blog/group-prime-order.html",
    "title": "Gruppi di Ordine Primo",
    "section": "",
    "text": "Una volta parlavo con un mio amico di Algebra astratta e ricordo che mi chiese se davvero esistesse qualcosa come l’algebra astratta. Secondo lui l’aggettivo astratta era ridondante, visto che considerava l’algebra di per sé già astratta. No. Non è così. Dovete sapere che ai matematici piace astrarre qualsiasi cosa. Ma c’è un motivo se è così ed è per dare delle definizioni formali, non assiomatiche di operazioni comuni per permettere (in algebre diverse da quelle che usiamo comunemente) di poter definire operazioni diverse ma con proprietà simili.\n\n“Cazzo vuol dire? Parla potabile Luca.”\n\nC’hai ragione, calmati.\nA volte fa comodo sapere che gli oggeti matematici con cui lavoriamo hanno delle proprietà definite, dimostrate da teoremi.\nPerché? Perché così si possono creare approcci nuovi, algoritmi, schemi e compagnia cantante senza dover dimostrare la loro fondatezza dalle basi, ma assumendo per esempio di partire con oggetti matematici noti.\nUn esempio di questo genere di oggetti matematici sono i gruppi. Esiste tutto un filone di matematica chiamato Teoria dei gruppi che si occupa di dimostrare, scoprire e studiare tutto ciò che riguarda i Gruppi.\nOk ma che caz sono ’sti benedetti gruppi? E perché ci servono?\nCalmati, di nuovo.\nPrima di arrivare al cuore dell’argomento di questo post, i gruppi di ordine primo, conviene costruire prima una base, partendo dalla definizione stessa di gruppo.\nUn gruppo è un insieme non vuoto, che possiamo chiamare come ci pare, ma chiamaremo \\(G\\) (che sta per \\(Giancarlo\\)), dotato di un’operazione binaria, che spesso indicheremo con \\(\\cdot\\) (ma potrebbe essere +, *, o altro a seconda del contesto o di come vi svegliate la mattina. E i matematici lo fanno.). Per essere un gruppo, questa coppia \\((G, \\cdot)\\) deve soddisfare quattro proprietà fondamentali, i cosiddetti assiomi di gruppo:\n\nChiusura: Per ogni coppia di elementi \\(a\\) e \\(b\\) appartenenti a \\(G\\), il risultato dell’operazione \\(a \\cdot b\\) deve essere ancora un elemento di \\(G\\). In simboli: se \\(a, b \\in G\\), allora \\(a \\cdot b \\in G\\). Questo significa che l’operazione non ci “porta fuori” dall’insieme \\(G\\).\nAssociatività: L’operazione deve essere associativa. Questo significa che per ogni terna di elementi \\(a, b, c\\) in \\(G\\), l’ordine in cui eseguiamo le operazioni non cambia il risultato: \\((a \\cdot b) \\cdot c = a \\cdot (b \\cdot c)\\).\nElemento Neutro (Identità): Esiste un elemento speciale in \\(G\\), che chiamiamo elemento neutro (o identità), solitamente indicato con \\(e\\) (o \\(1\\) in notazione moltiplicativa, \\(0\\) in notazione additiva). Questo elemento ha la proprietà che, per ogni elemento \\(a\\) in \\(G\\), \\(a \\cdot e = e \\cdot a = a\\). L’elemento neutro si comporta come un “non-operazione”.\nElemento Inverso: Per ogni elemento \\(a\\) in \\(G\\), esiste un altro elemento in \\(G\\), chiamato inverso di \\(a\\), che indicheremo con \\(a^{-1}\\) (o \\(-a\\) in notazione additiva). L’elemento inverso ha la proprietà che quando operato con \\(a\\) dà come risultato l’elemento neutro: \\(a \\cdot a^{-1} = a^{-1} \\cdot a = e\\).\n\nSe un gruppo soddisfa anche la proprietà commutativa, cioè se per ogni coppia di elementi \\(a, b \\in G\\), \\(a \\cdot b = b \\cdot a\\), allora il gruppo è detto gruppo abeliano (o commutativo). Se la proprietà commutativa non vale per tutti gli elementi, il gruppo è detto non-abeliano.\n\n\nIo ho già perso per strada metà dei lettori. Dico metà perché sono un inguaribile ottimista. In realtà se stai ancora leggendo forse sei l’unico che ha avuto il fegato di farlo. Quindi bravo.\nCome ricompnensa, vediamo alcuni esempi un po’ più concreti:\n\nI numeri interi con l’addizione (\\((\\mathbb{Z}, +)\\)): L’insieme dei numeri interi \\(\\mathbb{Z} = \\{..., -2, -1, 0, 1, 2, ...\\}\\) con l’operazione di addizione usuale (+) forma un gruppo abeliano.\n\nChiusura: La somma di due numeri interi è sempre un numero intero. 3 + 19 = 21 dove 3, 19 e 21 \\(\\in \\mathbb{Z}\\)\nAssociatività: L’addizione è associativa: \\((a + b) + c = a + (b + c)\\). Roba da seconda elementare, forza, su.\nElemento Neutro: L’elemento neutro è lo zero (0), poiché \\(a + 0 = 0 + a = a\\) per ogni intero \\(a\\).\nElemento Inverso: L’inverso di un intero \\(a\\) è \\(-a\\), poiché \\(a + (-a) = (-a) + a = 0\\).\nAbeliano: L’addizione è commutativa: \\(a + b = b + a\\).\n\nI numeri razionali non nulli con la moltiplicazione (\\((\\mathbb{Q} \\setminus \\{0\\}, \\cdot)\\)): L’insieme dei numeri razionali escluso lo zero, con l’operazione di moltiplicazione usuale (\\(\\cdot\\)), forma un gruppo abeliano.\n\nChiusura: Il prodotto di due numeri razionali non nulli è ancora un numero razionale non nullo.\nAssociatività: La moltiplicazione è associativa: \\((a \\cdot b) \\cdot c = a \\cdot (b \\cdot c)\\).\nElemento Neutro: L’elemento neutro è uno (1), poiché \\(a \\cdot 1 = 1 \\cdot a = a\\) per ogni razionale non nullo \\(a\\).\nElemento Inverso: L’inverso di un razionale non nullo \\(a = \\frac{p}{q}\\) (con \\(p, q \\neq 0\\)) è \\(a^{-1} = \\frac{q}{p}\\).\nAbeliano: La moltiplicazione è commutativa: \\(a \\cdot b = b \\cdot a\\).\n\nIl gruppo simmetrico \\(S_n\\) (per \\(n \\ge 3\\)): Questo è un esempio di gruppo non-abeliano. \\(S_n\\) è il gruppo delle permutazioni di \\(n\\) oggetti, con l’operazione di composizione di funzioni. Per \\(n \\ge 3\\), la composizione di permutazioni non è commutativa in generale.\n\nQuindi, riassumendo un pochino: un gruppo è semplicemente l’astrazione di quello che noi usiamo ogni giorno per contare quanti soldi ci sono rimasti sul conto e scoprire di essere molto povery. Per essere definito un gruppo ha bisogno di due elementi: un insieme (non vuoto altrimenti grazie al cazzo) e un’operazione binaria (operazione binaria=tra due operandi). Facile no? Ecco, adesso complichiamo le cose.\n\n\n\nInnanzitutto introduciamo un nuovo termine: l’ordine. L’ordine può essere definito sia sul gruppo nella sua interezza, sia solo sull’elemento.\n\nL’ordine di un gruppo \\(G\\), indicato con \\(|G|\\), non è altro che il numero di elementi contenuti nell’insieme \\(G\\). Se il numero di elementi è finito, si dice che il gruppo è finito, altrimenti è infinito (minchia, la fantasia dei matematici eh!?). Gli esempi \\((\\mathbb{Z}, +)\\) e \\((\\mathbb{Q} \\setminus \\{0\\}, \\cdot)\\) sono gruppi infiniti, mentre il gruppo simmetrico \\(S_n\\) è un gruppo finito di ordine \\(n!\\).\nL’ordine di un elemento \\(g\\) in un gruppo \\(G\\) è il più piccolo intero positivo \\(k\\) tale che \\(g^k = e\\), dove \\(e\\) è l’elemento neutro del gruppo e \\(g^k\\) indica l’operazione di \\(g\\) con se stesso \\(k\\) volte (ad esempio, in notazione moltiplicativa, \\(g^k = g \\cdot g \\cdot ... \\cdot g\\) (\\(k\\) volte); in notazione additiva, \\(kg = g + g + ... + g\\) (\\(k\\) volte)). Se tale intero positivo non esiste, si dice che l’elemento \\(g\\) ha ordine infinito.\n\nNah nah nah nah… ferma tutto. Il primo è chiaro e semplice. Il secondo non si capisce un tubo. Fammi un esempio.\nConsideriamo il gruppo \\((\\mathbb{Z}_4, +_4)\\), che è il gruppo degli interi modulo 4 sotto l’operazione di addizione modulo 4.\nEh!?\nL’elemento neutro in \\((\\mathbb{Z}_4, +_4)\\) è lo 0, perché per ogni elemento \\(a \\in \\mathbb{Z}_4\\), si ha \\(a +_4 0 = 0 +_4 a = a\\). Eh vabbè fin qua.\n\nCos’è \\(\\mathbb{Z}_4\\)? È l’insieme \\(\\{0, 1, 2, 3\\}\\). Facile, ok.\nCos’è \\(+_4\\)? È l’addizione modulo 4. Ad esempio, \\(2 +_4 3 = 5 \\pmod{4} = 1\\). Eh!?\n\nL’adidzione modulo 4 è semplicemente una addizione che quando “sfora” il 4 ricomincia da zero. Che uno può immaginare sia una roba incasinatissima e invece no, perché volendo è concettualmente simila a quello che fai dalla terza elementare quando fai le somme ma alle elementari lo chiamavi “riporto”1. \\(8+4\\) quanto fa? \\(12\\), embé?\nSì, ma come ci siamo arrivati a far comparire una seconda cifra lì? Arriviamoci passo passo: \\(8 + 1 = 9\\).\nOk e fin qua…\n\\(9 + 1\\)? Non ho più numeri a una cifra, allora cosa faccio? Ricomincio dallo zero, aggiungo uno davanti e continuo. Quindi: \\(9 + 1 = 10\\).\n\\(10 + 1 = 11\\).\n\\(11 + 1 = 12\\)\nL’addizione modulo \\(n\\) la facciamo dalla seconda elementare senza manco saperlo\nNell’esempio di sopra il \\(2\\) di \\(12\\) si ottiene in termini di addizione modulo 10 con: \\[8 +_{10} 4 = 12 \\pmod{10} = 2\\]\nOk, andiamo avanti.\nOra, prendiamo un elemento a caso da \\(\\mathbb{Z}_4\\), ad esempio l’elemento 2. Vogliamo trovare l’ordine dell’elemento 2. Dobbiamo trovare il più piccolo intero positivo \\(k\\) tale che \\(k \\cdot 2 = 0\\) (ricorda che in notazione additiva, \\(g^k\\) diventa \\(k \\cdot g\\)). Qui, l’operazione è l’addizione modulo 4, quindi stiamo cercando il più piccolo \\(k\\) tale che:\n\\(2 +_4 2 +_4 \\ldots +_4 2\\) (\\(k\\) volte) \\(= 0 \\pmod{4}\\)\nIn altre parole, stiamo cercando il più piccolo intero positivo \\(k\\) tale che \\(k \\times 2\\) sia un multiplo di 4. Vediamo un po’:\n\nPer \\(k = 1\\): \\(1 \\cdot 2 = 2 \\pmod{4} = 2 \\neq 0\\).\nPer \\(k = 2\\): \\(2 \\cdot 2 = 2 +_4 2 = 4 \\pmod{4} = 0\\).\n\nVAMOS! L’abbiamo trovato! Il più piccolo intero positivo \\(k\\) per cui \\(k \\cdot 2 = 0 \\pmod{4}\\) è \\(k = 2\\).\nQuindi, l’ordine dell’elemento 2 nel gruppo \\((\\mathbb{Z}_4, +_4)\\) è 2.\nProviamo con un altro elemento, ad esempio l’elemento 1. Vogliamo trovare il più piccolo intero positivo \\(k\\) tale che \\(k \\cdot 1 = 0 \\pmod{4}\\).\n\nPer \\(k = 1\\): \\(1 \\cdot 1 = 1 \\pmod{4} = 1 \\neq 0\\).\nPer \\(k = 2\\): \\(2 \\cdot 1 = 1 +_4 1 = 2 \\pmod{4} = 2 \\neq 0\\).\nPer \\(k = 3\\): \\(3 \\cdot 1 = 1 +_4 1 +_4 1 = 3 \\pmod{4} = 3 \\neq 0\\).\nPer \\(k = 4\\): \\(4 \\cdot 1 = 1 +_4 1 +_4 1 +_4 1 = 4 \\pmod{4} = 0\\).\n\nIl più piccolo intero positivo \\(k\\) per cui \\(k \\cdot 1 = 0 \\pmod{4}\\) è \\(k = 4\\).\nQuindi, l’ordine dell’elemento 1 nel gruppo \\((\\mathbb{Z}_4, +_4)\\) è 4.\nFantastico. Finora era tutto bello e semplice. Ora scendiamo un po’ nell’abisso."
  },
  {
    "objectID": "blog/group-prime-order.html#introduzione-ai-gruppi-le-fondamenta-dellalgebra-astratta",
    "href": "blog/group-prime-order.html#introduzione-ai-gruppi-le-fondamenta-dellalgebra-astratta",
    "title": "Gruppi di Ordine Primo",
    "section": "",
    "text": "Una volta parlavo con un mio amico di Algebra astratta e ricordo che mi chiese se davvero esistesse qualcosa come l’algebra astratta. Secondo lui l’aggettivo astratta era ridondante, visto che considerava l’algebra di per sé già astratta. No. Non è così. Dovete sapere che ai matematici piace astrarre qualsiasi cosa. Ma c’è un motivo se è così ed è per dare delle definizioni formali, non assiomatiche di operazioni comuni per permettere (in algebre diverse da quelle che usiamo comunemente) di poter definire operazioni diverse ma con proprietà simili.\n\n“Cazzo vuol dire? Parla potabile Luca.”\n\nC’hai ragione, calmati.\nA volte fa comodo sapere che gli oggeti matematici con cui lavoriamo hanno delle proprietà definite, dimostrate da teoremi.\nPerché? Perché così si possono creare approcci nuovi, algoritmi, schemi e compagnia cantante senza dover dimostrare la loro fondatezza dalle basi, ma assumendo per esempio di partire con oggetti matematici noti.\nUn esempio di questo genere di oggetti matematici sono i gruppi. Esiste tutto un filone di matematica chiamato Teoria dei gruppi che si occupa di dimostrare, scoprire e studiare tutto ciò che riguarda i Gruppi.\nOk ma che caz sono ’sti benedetti gruppi? E perché ci servono?\nCalmati, di nuovo.\nPrima di arrivare al cuore dell’argomento di questo post, i gruppi di ordine primo, conviene costruire prima una base, partendo dalla definizione stessa di gruppo.\nUn gruppo è un insieme non vuoto, che possiamo chiamare come ci pare, ma chiamaremo \\(G\\) (che sta per \\(Giancarlo\\)), dotato di un’operazione binaria, che spesso indicheremo con \\(\\cdot\\) (ma potrebbe essere +, *, o altro a seconda del contesto o di come vi svegliate la mattina. E i matematici lo fanno.). Per essere un gruppo, questa coppia \\((G, \\cdot)\\) deve soddisfare quattro proprietà fondamentali, i cosiddetti assiomi di gruppo:\n\nChiusura: Per ogni coppia di elementi \\(a\\) e \\(b\\) appartenenti a \\(G\\), il risultato dell’operazione \\(a \\cdot b\\) deve essere ancora un elemento di \\(G\\). In simboli: se \\(a, b \\in G\\), allora \\(a \\cdot b \\in G\\). Questo significa che l’operazione non ci “porta fuori” dall’insieme \\(G\\).\nAssociatività: L’operazione deve essere associativa. Questo significa che per ogni terna di elementi \\(a, b, c\\) in \\(G\\), l’ordine in cui eseguiamo le operazioni non cambia il risultato: \\((a \\cdot b) \\cdot c = a \\cdot (b \\cdot c)\\).\nElemento Neutro (Identità): Esiste un elemento speciale in \\(G\\), che chiamiamo elemento neutro (o identità), solitamente indicato con \\(e\\) (o \\(1\\) in notazione moltiplicativa, \\(0\\) in notazione additiva). Questo elemento ha la proprietà che, per ogni elemento \\(a\\) in \\(G\\), \\(a \\cdot e = e \\cdot a = a\\). L’elemento neutro si comporta come un “non-operazione”.\nElemento Inverso: Per ogni elemento \\(a\\) in \\(G\\), esiste un altro elemento in \\(G\\), chiamato inverso di \\(a\\), che indicheremo con \\(a^{-1}\\) (o \\(-a\\) in notazione additiva). L’elemento inverso ha la proprietà che quando operato con \\(a\\) dà come risultato l’elemento neutro: \\(a \\cdot a^{-1} = a^{-1} \\cdot a = e\\).\n\nSe un gruppo soddisfa anche la proprietà commutativa, cioè se per ogni coppia di elementi \\(a, b \\in G\\), \\(a \\cdot b = b \\cdot a\\), allora il gruppo è detto gruppo abeliano (o commutativo). Se la proprietà commutativa non vale per tutti gli elementi, il gruppo è detto non-abeliano.\n\n\nIo ho già perso per strada metà dei lettori. Dico metà perché sono un inguaribile ottimista. In realtà se stai ancora leggendo forse sei l’unico che ha avuto il fegato di farlo. Quindi bravo.\nCome ricompnensa, vediamo alcuni esempi un po’ più concreti:\n\nI numeri interi con l’addizione (\\((\\mathbb{Z}, +)\\)): L’insieme dei numeri interi \\(\\mathbb{Z} = \\{..., -2, -1, 0, 1, 2, ...\\}\\) con l’operazione di addizione usuale (+) forma un gruppo abeliano.\n\nChiusura: La somma di due numeri interi è sempre un numero intero. 3 + 19 = 21 dove 3, 19 e 21 \\(\\in \\mathbb{Z}\\)\nAssociatività: L’addizione è associativa: \\((a + b) + c = a + (b + c)\\). Roba da seconda elementare, forza, su.\nElemento Neutro: L’elemento neutro è lo zero (0), poiché \\(a + 0 = 0 + a = a\\) per ogni intero \\(a\\).\nElemento Inverso: L’inverso di un intero \\(a\\) è \\(-a\\), poiché \\(a + (-a) = (-a) + a = 0\\).\nAbeliano: L’addizione è commutativa: \\(a + b = b + a\\).\n\nI numeri razionali non nulli con la moltiplicazione (\\((\\mathbb{Q} \\setminus \\{0\\}, \\cdot)\\)): L’insieme dei numeri razionali escluso lo zero, con l’operazione di moltiplicazione usuale (\\(\\cdot\\)), forma un gruppo abeliano.\n\nChiusura: Il prodotto di due numeri razionali non nulli è ancora un numero razionale non nullo.\nAssociatività: La moltiplicazione è associativa: \\((a \\cdot b) \\cdot c = a \\cdot (b \\cdot c)\\).\nElemento Neutro: L’elemento neutro è uno (1), poiché \\(a \\cdot 1 = 1 \\cdot a = a\\) per ogni razionale non nullo \\(a\\).\nElemento Inverso: L’inverso di un razionale non nullo \\(a = \\frac{p}{q}\\) (con \\(p, q \\neq 0\\)) è \\(a^{-1} = \\frac{q}{p}\\).\nAbeliano: La moltiplicazione è commutativa: \\(a \\cdot b = b \\cdot a\\).\n\nIl gruppo simmetrico \\(S_n\\) (per \\(n \\ge 3\\)): Questo è un esempio di gruppo non-abeliano. \\(S_n\\) è il gruppo delle permutazioni di \\(n\\) oggetti, con l’operazione di composizione di funzioni. Per \\(n \\ge 3\\), la composizione di permutazioni non è commutativa in generale.\n\nQuindi, riassumendo un pochino: un gruppo è semplicemente l’astrazione di quello che noi usiamo ogni giorno per contare quanti soldi ci sono rimasti sul conto e scoprire di essere molto povery. Per essere definito un gruppo ha bisogno di due elementi: un insieme (non vuoto altrimenti grazie al cazzo) e un’operazione binaria (operazione binaria=tra due operandi). Facile no? Ecco, adesso complichiamo le cose.\n\n\n\nInnanzitutto introduciamo un nuovo termine: l’ordine. L’ordine può essere definito sia sul gruppo nella sua interezza, sia solo sull’elemento.\n\nL’ordine di un gruppo \\(G\\), indicato con \\(|G|\\), non è altro che il numero di elementi contenuti nell’insieme \\(G\\). Se il numero di elementi è finito, si dice che il gruppo è finito, altrimenti è infinito (minchia, la fantasia dei matematici eh!?). Gli esempi \\((\\mathbb{Z}, +)\\) e \\((\\mathbb{Q} \\setminus \\{0\\}, \\cdot)\\) sono gruppi infiniti, mentre il gruppo simmetrico \\(S_n\\) è un gruppo finito di ordine \\(n!\\).\nL’ordine di un elemento \\(g\\) in un gruppo \\(G\\) è il più piccolo intero positivo \\(k\\) tale che \\(g^k = e\\), dove \\(e\\) è l’elemento neutro del gruppo e \\(g^k\\) indica l’operazione di \\(g\\) con se stesso \\(k\\) volte (ad esempio, in notazione moltiplicativa, \\(g^k = g \\cdot g \\cdot ... \\cdot g\\) (\\(k\\) volte); in notazione additiva, \\(kg = g + g + ... + g\\) (\\(k\\) volte)). Se tale intero positivo non esiste, si dice che l’elemento \\(g\\) ha ordine infinito.\n\nNah nah nah nah… ferma tutto. Il primo è chiaro e semplice. Il secondo non si capisce un tubo. Fammi un esempio.\nConsideriamo il gruppo \\((\\mathbb{Z}_4, +_4)\\), che è il gruppo degli interi modulo 4 sotto l’operazione di addizione modulo 4.\nEh!?\nL’elemento neutro in \\((\\mathbb{Z}_4, +_4)\\) è lo 0, perché per ogni elemento \\(a \\in \\mathbb{Z}_4\\), si ha \\(a +_4 0 = 0 +_4 a = a\\). Eh vabbè fin qua.\n\nCos’è \\(\\mathbb{Z}_4\\)? È l’insieme \\(\\{0, 1, 2, 3\\}\\). Facile, ok.\nCos’è \\(+_4\\)? È l’addizione modulo 4. Ad esempio, \\(2 +_4 3 = 5 \\pmod{4} = 1\\). Eh!?\n\nL’adidzione modulo 4 è semplicemente una addizione che quando “sfora” il 4 ricomincia da zero. Che uno può immaginare sia una roba incasinatissima e invece no, perché volendo è concettualmente simila a quello che fai dalla terza elementare quando fai le somme ma alle elementari lo chiamavi “riporto”1. \\(8+4\\) quanto fa? \\(12\\), embé?\nSì, ma come ci siamo arrivati a far comparire una seconda cifra lì? Arriviamoci passo passo: \\(8 + 1 = 9\\).\nOk e fin qua…\n\\(9 + 1\\)? Non ho più numeri a una cifra, allora cosa faccio? Ricomincio dallo zero, aggiungo uno davanti e continuo. Quindi: \\(9 + 1 = 10\\).\n\\(10 + 1 = 11\\).\n\\(11 + 1 = 12\\)\nL’addizione modulo \\(n\\) la facciamo dalla seconda elementare senza manco saperlo\nNell’esempio di sopra il \\(2\\) di \\(12\\) si ottiene in termini di addizione modulo 10 con: \\[8 +_{10} 4 = 12 \\pmod{10} = 2\\]\nOk, andiamo avanti.\nOra, prendiamo un elemento a caso da \\(\\mathbb{Z}_4\\), ad esempio l’elemento 2. Vogliamo trovare l’ordine dell’elemento 2. Dobbiamo trovare il più piccolo intero positivo \\(k\\) tale che \\(k \\cdot 2 = 0\\) (ricorda che in notazione additiva, \\(g^k\\) diventa \\(k \\cdot g\\)). Qui, l’operazione è l’addizione modulo 4, quindi stiamo cercando il più piccolo \\(k\\) tale che:\n\\(2 +_4 2 +_4 \\ldots +_4 2\\) (\\(k\\) volte) \\(= 0 \\pmod{4}\\)\nIn altre parole, stiamo cercando il più piccolo intero positivo \\(k\\) tale che \\(k \\times 2\\) sia un multiplo di 4. Vediamo un po’:\n\nPer \\(k = 1\\): \\(1 \\cdot 2 = 2 \\pmod{4} = 2 \\neq 0\\).\nPer \\(k = 2\\): \\(2 \\cdot 2 = 2 +_4 2 = 4 \\pmod{4} = 0\\).\n\nVAMOS! L’abbiamo trovato! Il più piccolo intero positivo \\(k\\) per cui \\(k \\cdot 2 = 0 \\pmod{4}\\) è \\(k = 2\\).\nQuindi, l’ordine dell’elemento 2 nel gruppo \\((\\mathbb{Z}_4, +_4)\\) è 2.\nProviamo con un altro elemento, ad esempio l’elemento 1. Vogliamo trovare il più piccolo intero positivo \\(k\\) tale che \\(k \\cdot 1 = 0 \\pmod{4}\\).\n\nPer \\(k = 1\\): \\(1 \\cdot 1 = 1 \\pmod{4} = 1 \\neq 0\\).\nPer \\(k = 2\\): \\(2 \\cdot 1 = 1 +_4 1 = 2 \\pmod{4} = 2 \\neq 0\\).\nPer \\(k = 3\\): \\(3 \\cdot 1 = 1 +_4 1 +_4 1 = 3 \\pmod{4} = 3 \\neq 0\\).\nPer \\(k = 4\\): \\(4 \\cdot 1 = 1 +_4 1 +_4 1 +_4 1 = 4 \\pmod{4} = 0\\).\n\nIl più piccolo intero positivo \\(k\\) per cui \\(k \\cdot 1 = 0 \\pmod{4}\\) è \\(k = 4\\).\nQuindi, l’ordine dell’elemento 1 nel gruppo \\((\\mathbb{Z}_4, +_4)\\) è 4.\nFantastico. Finora era tutto bello e semplice. Ora scendiamo un po’ nell’abisso."
  },
  {
    "objectID": "blog/group-prime-order.html#gruppi-ciclici-e-generatori-la-chiave-per-i-gruppi-di-ordine-primo",
    "href": "blog/group-prime-order.html#gruppi-ciclici-e-generatori-la-chiave-per-i-gruppi-di-ordine-primo",
    "title": "Gruppi di Ordine Primo",
    "section": "Gruppi Ciclici e Generatori: La Chiave per i Gruppi di Ordine Primo",
    "text": "Gruppi Ciclici e Generatori: La Chiave per i Gruppi di Ordine Primo\nUn tipo speciale di gruppo, che poi è l’argomento per cui siamo qui tutti riuniti quest’oggi, è il gruppo ciclico.\nUn gruppo ciclico è (formalmente) un gruppo che può essere generato da un singolo elemento. Ovvero, un gruppo \\(G\\) è ciclico se esiste un elemento \\(g \\in G\\) tale che ogni elemento di \\(G\\) può essere espresso come una “potenza” di \\(g\\) (dove “potenza” significa ripetere l’operazione di gruppo). Questo elemento \\(g\\) è chiamato generatore del gruppo \\(G\\).\nOra: avete notato una cosa importante? Abbiamo usato il termini “potenza” ma specificando che vuol dire “ripere l’operazione di gruppo”. Intendevo questo quando parlavo di Algebra astratta. Non è altro che prendere l’algebra che usiamo tutti i giorni e semplicemente astrarre dei concetti e dimostrare le proprietà di quell’oggetto a prescindere che l’operazione sia l’addizione o la moltiplicazione.\nFormalmente, un gruppo \\(G\\) è ciclico se esiste un elemento \\(g \\in G\\) tale che:\n\\(G = \\{g^k \\mid k \\in \\mathbb{Z} \\}\\) (in notazione moltiplicativa)\noppure\n\\(G = \\{kg \\mid k \\in \\mathbb{Z} \\}\\) (in notazione additiva)\ndove \\(\\mathbb{Z}\\) rappresenta l’insieme dei numeri interi.\nConviene fare un esempio, che sento già la puzza di bruciato. Consideriamo il gruppo degli interi modulo 5 (perché se facciamo sempre con quel cazzo di 4 pensate che funziona solo col 4) con l’addizione, \\((\\mathbb{Z}_5, +) = \\{[0], [1], [2], [3], [4]\\}\\). Questo è un gruppo ciclico di ordine 5. L’elemento [1] è un generatore di \\(\\mathbb{Z}_5\\), poiché:\n\n\\([1]^1 = [1]\\)\n\\([1]^2 = [1] + [1] = [2]\\)\n\\([1]^3 = [1] + [1] + [1] = [3]\\)\n\\([1]^4 = [1] + [1] + [1] + [1] = [4]\\)\n\\([1]^5 = [1] + [1] + [1] + [1] + [1] = [5] \\equiv [0] \\pmod{5}\\) (elemento neutro)\n\\([1]^6 = [1] + [1] + [1] + [1] + [1] + [1] = [6] \\equiv [1] \\pmod{5}\\), e così via.\n\nQuindi, tornando a \\(\\mathbb{Z}_5 = \\{[0], [1], [2], [3], [4]\\}\\), è chiaro che partendo da [1] e sommandolo a se stesso un po’ di volte, possiamo ottenere tutti gli altri elementi. Infatti: [1], [1]+[1]=[2], [1]+[1]+[1]=[3], [1]+[1]+[1]+[1]=[4], e [1] sommato 5 volte ci ridà [0] (cioè [5] che modulo 5 fa [0]). Figo no? Questo significa che [1] è un generatore di \\(\\mathbb{Z}_5\\).\nMa non solo [1]! Anche [2] è un generatore di \\(\\mathbb{Z}_5\\). Proviamo: [2], [2]+[2]=[4], [2]+[2]+[2]=[6]=[1] (modulo 5), [2]+[2]+[2]+[2]=[8]=[3] (modulo 5), e [2] sommato 5 volte fa [10]=[0] (modulo 5). Anche con [2] abbiamo generato tutti gli elementi di \\(\\mathbb{Z}_5\\)!\nOra, uno potrebbe chiedersi: ma vale sempre? È vero che [2] è un generatore di \\(\\mathbb{Z}_n\\) per ogni \\(n\\) dispari? Si lascia la dimostrazione come esercizio al lettore.\nNaaah! Scherzo. In realtà lo schema della dimostrazione potrebbe essere abbastanza intuitivo.\nL’idea è che se \\(n\\) è dispari, allora 2 e \\(n\\) sono coprimi, cioè non hanno fattori comuni (a parte 1, ovviamente). E questo è un dettaglio FONDAMENTALE.\nCerchiamo di capirlo meglio. Quando sommiamo [2] a se stesso un po’ di volte in \\(\\mathbb{Z}_n\\), otteniamo i multipli di [2]: [2], [4], [6], [8], [10], e così via, sempre modulo \\(n\\). Se 2 e \\(n\\) fossero “amici” e avessero un fattore comune, diciamo \\(d &gt; 1\\), allora i multipli di 2 “modulo \\(n\\)” si ripeterebbero prima di coprire tutti gli elementi di \\(\\mathbb{Z}_n\\). Ma se 2 e \\(n\\) sono coprimi, allora i multipli di 2 “modulo \\(n\\)” continuano a sforare e a dare resti diversi fino a quando non abbiamo generato tutti gli elementi da [0] a \\([n-1]\\).\nPer esempio, prendiamo \\(n=9\\) (dispari). Partiamo da [2]: [2], [4], [6], [8], [10]=[1] (modulo 9), [12]=[3] (modulo 9), [14]=[5] (modulo 9), [16]=[7] (modulo 9), [18]=[0] (modulo 9). E voilà! Abbiamo ottenuto [0], [1], [2], [3], [4], [5], [6], [7], [8], cioè tutti gli elementi di \\(\\mathbb{Z}_9\\).\nSe invece prendessimo un \\(n\\) pari, tipo \\(n=6\\), e provassimo con [2] in \\(\\mathbb{Z}_6\\): [2], [4], [6]=[0] (modulo 6), [8]=[2] (modulo 6)… Vediamo che otteniamo solo [0], [2], [4]. Non generiamo tutto \\(\\mathbb{Z}_6\\). Questo succede perché 2 e 6 non sono coprimi (hanno il fattore comune 2).\n\\[\\blacksquare\\]"
  },
  {
    "objectID": "blog/group-prime-order.html#gruppi-di-ordine-primo-un-risultato-fondamentale",
    "href": "blog/group-prime-order.html#gruppi-di-ordine-primo-un-risultato-fondamentale",
    "title": "Gruppi di Ordine Primo",
    "section": "Gruppi di Ordine Primo: Un Risultato Fondamentale",
    "text": "Gruppi di Ordine Primo: Un Risultato Fondamentale\nOra arriviamo al cuore del nostro articolo: i gruppi di ordine primo. Tutti sapete cos’è un numero primo, altrimenti vi vengo a cercare e a strappare la licenza media di persona.\nComunque.\nFormalmente (perché un po’ di formalità ci vuole, sennò sembra il bar dello sport): Un numero primo è un intero positivo maggiore di 1 che ha solo due divisori positivi: 1 e se stesso. Esempi di numeri primi sono 2, 3, 5, 7, 11, 13, ecc.\nUn risultato fondamentale e notevole in teoria dei gruppi afferma (e rieccoci alla questione algebra astratta!):\nTeorema: Ogni gruppo di ordine primo è ciclico.\nCazzo vuol dire? (Cit.)\nQuesto teorema ci dice che se prendiamo un gruppo \\(G\\) il cui ordine \\(|G|\\) è un numero primo \\(q\\), allora \\(G\\) è necessariamente un gruppo ciclico. Questo significa che esiste almeno un elemento \\(g \\in G\\) che genera tutto il gruppo \\(G\\).\nIl che è notevole. Vuol dire che possiamo prendere un gruppo \\(G\\) di pordine \\(q\\) (ricordate? prima per esempio avevamo ordine \\(4\\) oppure \\(5\\)) e possiamo avere un elemento all’interno del gruppo che genera tutti gli altri, dato un numero \\(k\\).\n\nNon riesco a capire perché sei così eccitato Luca. Spiegamelo, prima che ti faccio male\n\nCalma e sangue freddo.\nRiprendiamo l’esempio di \\(\\mathbb{Z}_5\\) e del generatore [1]. Se io vi dico che un certo elemento nel gruppo è [3], e vi dico che il generatore è [1], voi sapete subito che per ottenere [3] da [1] devo fare:\n\\[\n[1] + [1] + [1] = [3] \\qquad \\text{oppure} \\qquad 3 \\cdot [1] = [3]\n\\]\nMa potrei averlo ottenuto anche facendo molte più operazioni, tipo:\n\\[\n8 \\cdot [1] = [1] + [1] + [1] + [1] + [1] + [1] + [1] + [1] = [8] = [3] \\pmod{5}\n\\]\no anche\n\\[\n90901291213 \\cdot [1] = [90901291213] = [3] \\pmod{5}\n\\]\nCi sono infiniti modi per descrivere lo stesso elemento [3] usando il generatore [1]! Se io vi dico solo che il risultato è [3], che l’ordine del gruppo ciclico è 5 e il generatore è [1], voi non potete sapere quante volte ho applicato l’operazione, cioè quale esponente \\(k\\) ho usato tra gli infiniti possibili. Ed è proprio su questa “ignoranza” che si basa una valanga di algoritmi di crittografia moderni!\n“Cose semplici da verificare, ma difficili da calcolare”: Verificare che \\(90901291213 \\cdot [1] = [3] \\pmod{5}\\) è facilissimo. Basta fare la divisione e vedere il resto. Ma se io vi dessi solo [3], [1] e 5, e vi chiedessi di trovare quel numero enorme \\(k=90901291213\\) (o uno simile), sarebbe praticamente impossibile farlo in tempi ragionevoli, soprattutto se l’ordine del gruppo fosse un numero primo enorme di centinaia di cifre!\nEcco perché questo teorema sui gruppi di ordine primo, che all’inizio sembrava solo un giochino matematico, è in realtà fondamentale per la sicurezza delle nostre comunicazioni online, delle nostre transazioni bancarie, e di un sacco di altre cose che usiamo tutti i giorni. La matematica astratta che salva il mondo, ragazzi! Chi l’avrebbe mai detto? Come? Banalmente, l’idea è che se io uso un numero segreto \\(k\\) per “criptare” in un certo modo con un certo algoritmo, anche se vi mostro il risultato “criptato”, indovinare \\(k\\) sarebbe un disastro.\nDetto questo: ora dimostrazione formale del teorema e tutti a nanna.\nDimostrazione (Schema):\nSia \\(G\\) un gruppo di ordine primo \\(q\\). Consideriamo un elemento \\(g \\in G\\) diverso dall’elemento neutro \\(e\\). Consideriamo il sottogruppo ciclico generato da \\(g\\), che indichiamo con \\(\\langle g \\rangle = \\{g^k \\mid k \\in \\mathbb{Z} \\}\\). Per il teorema di Lagrange, l’ordine di un sottogruppo deve dividere l’ordine del gruppo. Quindi, l’ordine di \\(\\langle g \\rangle\\), cioè \\(|\\langle g \\rangle|\\), deve dividere l’ordine di \\(G\\), che è \\(q\\). Poiché \\(q\\) è primo, i divisori positivi di \\(q\\) sono solo 1 e \\(q\\).\nL’ordine di \\(\\langle g \\rangle\\) non può essere 1, perché \\(g \\neq e\\), quindi \\(\\langle g \\rangle\\) contiene almeno due elementi (\\(e\\) e \\(g\\)). Pertanto, l’unica possibilità è che \\(|\\langle g \\rangle| = q\\). Ma se l’ordine del sottogruppo generato da \\(g\\) è uguale all’ordine del gruppo \\(G\\), e \\(\\langle g \\rangle\\) è un sottogruppo di \\(G\\), allora deve essere che \\(\\langle g \\rangle = G\\). Questo significa che \\(G\\) è generato dall’elemento \\(g\\), e quindi \\(G\\) è ciclico.\nAltre Conseguenze e Implicazioni:\nQuesto teorema ha importanti conseguenze. Se sappiamo che un gruppo ha ordine primo \\(q\\), sappiamo immediatamente che:\n\nÈ ciclico: Esiste un generatore \\(g \\in G\\) tale che ogni elemento di \\(G\\) è una potenza di \\(g\\).\nÈ abeliano: Tutti i gruppi ciclici sono abeliani. Quindi, ogni gruppo di ordine primo è abeliano.\nStruttura semplice: I gruppi ciclici sono tra i gruppi più semplici da comprendere e studiare. Il teorema ci dice che i gruppi di ordine primo, nonostante la loro apparente “rarità” (ci sono infiniti numeri primi), hanno una struttura algebrica molto ben definita e semplice.\n\nGeneratore \\(g\\):\nAbbiamo visto che un gruppo \\(G\\) di ordine primo \\(q\\) è ciclico e ha un generatore \\(g\\). Questo generatore \\(g\\) è un elemento di \\(G\\) tale che “ripetendo” l’operazione di gruppo con \\(g\\) (con se stesso), possiamo ottenere tutti gli elementi di \\(G\\).\nAd esempio, se \\(G\\) è un gruppo di ordine 7 (7 è primo), allora esiste un elemento \\(g \\in G\\) tale che:\n\\(G = \\{e, g, g^2, g^3, g^4, g^5, g^6\\}\\)\ndove \\(e\\) è l’elemento neutro e \\(g^7 = e\\). Gli esponenti sono presi modulo 7.\nUnicità (a meno di isomorfismo):\nUn altro risultato importante (che non dimostreremo qui perché oggettivamente mi sono scassato le balle pure io) è che esiste, a meno di isomorfismo, un solo gruppo ciclico di ordine \\(n\\) per ogni intero positivo \\(n\\). In particolare, per ogni numero primo \\(q\\), esiste un solo gruppo di ordine \\(q\\), a meno di isomorfismo. Questo gruppo è isomorfo al gruppo ciclico \\(\\mathbb{Z}_q = (\\mathbb{Z}/q\\mathbb{Z}, +)\\), cioè gli interi modulo \\(q\\) con l’addizione.\nEsempio:\nSe consideriamo un gruppo \\(G\\) di ordine 3. Poiché 3 è un numero primo, sappiamo che \\(G\\) è ciclico e abeliano. Esiste un elemento \\(g \\in G\\) tale che \\(G = \\{e, g, g^2\\}\\) e \\(g^3 = e\\). La struttura di \\(G\\) è completamente determinata da questa proprietà. Ogni gruppo di ordine 3 è “essenzialmente lo stesso” (isomorfo) a \\(\\mathbb{Z}_3 = \\{[0], [1], [2]\\}\\) con l’addizione modulo 3."
  },
  {
    "objectID": "blog/group-prime-order.html#conclusione-la-bellezza-e-la-potenza-dei-gruppi-di-ordine-primo",
    "href": "blog/group-prime-order.html#conclusione-la-bellezza-e-la-potenza-dei-gruppi-di-ordine-primo",
    "title": "Gruppi di Ordine Primo",
    "section": "Conclusione: La Bellezza e la Potenza dei Gruppi di Ordine Primo**",
    "text": "Conclusione: La Bellezza e la Potenza dei Gruppi di Ordine Primo**\nE con questo, siamo arrivati alla fine di questo viaggione nell’abisso dell’algebra astratta. Spero che abbiate capito perché questo teorema, all’apparenza così semplice, è in realtà un risultato fondamentale e potente. Ci dice che i gruppi con un numero primo di elementi non sono solo un’astrazione matematica, ma hanno una struttura incredibilmente ordinata e prevedibile: sono ciclici, sono abeliani, e sono, in un certo senso, i gruppi più “semplici” che possiamo immaginare.\nMa non è tanto la loro semplicità che deve entusiasmare. Come abbiamo visto, proprio questa struttura “semplice” e ben definita dei gruppi ciclici di ordine primo è alla base di tecnologie complesse come la crittografia moderna. La prossima volta che fate un pagamento online su onlyfans o che inviate un messaggio criptato che tanto la vostra fidanzata scoprirà comunque, pensateci: dietro le quinte, c’è l’algebra astratta, ci sono i gruppi di ordine primo, e c’è un sacco di matematica fighissima al servizio della vostra sicureza.\nLa teoria dei gruppi è piena di altri risultati sorprendenti e connessioni inaspettate. Questo teorema sui gruppi di ordine primo è solo un piccolo assaggio della bellezza e della potenza di questa branca della matematica. Se siete arrivati fin qui, complimenti, sinceramente!\n\nFIN"
  },
  {
    "objectID": "blog/group-prime-order.html#footnotes",
    "href": "blog/group-prime-order.html#footnotes",
    "title": "Gruppi di Ordine Primo",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nin realtà l’analogia è un po’ azzardata e pericolante, visto che nelle operazioni modulo \\(n\\) si resta sempre all’interno dell’insieme iniziale. Il “riporto” nelle somme in base 10 è legato alle potenze di 10 (unità, decine, centinaia, ecc.), mentre l’addizione modulo 4 è legata ai resti della divisione per 4. Non sono esattamente la stessa cosa, anche se condividono un’idea di “superamento di una soglia” e di “ripartenza”.↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Sono Luca, e come dico spesso: solo un povero cog**one. Scherzi a parte, sono un programmatore con 16 anni di esperienza. Ho iniziato a sviluppare piccoli siti per privati nel novembre 2009, poi ho lavorato per cinque anni come full stack developer presso Digitalfun S.r.l.\nNel novembre 2014, mentre lavoravo ancora in DF, ho co-fondato una startup con mio cugino e un terzo socio. Due mesi dopo mi sono trasferito a Londra in cerca di opportunità, dove ho lavorato per un anno come backend developer in SaveWaterSaveMoney Ltd, utilizzando Symfony.\nNel luglio 2016 sono tornato in Italia e ho iniziato a lavorare in Nohup S.r.l. come full stack developer, ricoprendo il ruolo di R&D Team Leader nell’ultimo anno.\nMi sono laureato presso l’Università degli Studi Guglielmo Marconi, un’università telematica. Attualmente, sono uno studente part-time del corso magistrale in Artificial Intelligence all’Università degli Studi di Udine. Dopo la mia esperienza alla Marconi, ho deciso di affrontare il percorso magistrale con un ritmo più sostenibile.\nSono appassionato di tecnologia, scienza e insegnamento. Il mio più grande sogno nel cassetto è diventare insegnante di materie scientifiche.\n\n\n\n\nI’m Luca, and as I often say: just a poor dum**ss. Jokes aside, I’ve been a programmer for 16 years. I started developing small websites for private clients in November 2009, then spent five years as a full stack developer at Digitalfun S.r.l.\nIn November 2014, while still working at DF, I co-founded a startup with my cousin and a third partner. Two months later, I moved to London to seek new opportunities and worked for a year as a backend developer at SaveWaterSaveMoney Ltd, using Symfony.\nIn July 2016, I returned to Italy and joined Nohup S.r.l. as a full stack developer, eventually becoming the R&D Team Leader in my final year.\nI earned my degree from Università degli Studi Guglielmo Marconi, an online university. Currently, I’m a part-time Master’s student in Artificial Intelligence at the University of Udine. After my experience at Marconi, I decided to take the Master’s program at a more sustainable pace.\nI’m passionate about technology, science, and teaching. My biggest dream is to become a teacher in scientific subjects."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About",
    "section": "",
    "text": "Sono Luca, e come dico spesso: solo un povero cog**one. Scherzi a parte, sono un programmatore con 16 anni di esperienza. Ho iniziato a sviluppare piccoli siti per privati nel novembre 2009, poi ho lavorato per cinque anni come full stack developer presso Digitalfun S.r.l.\nNel novembre 2014, mentre lavoravo ancora in DF, ho co-fondato una startup con mio cugino e un terzo socio. Due mesi dopo mi sono trasferito a Londra in cerca di opportunità, dove ho lavorato per un anno come backend developer in SaveWaterSaveMoney Ltd, utilizzando Symfony.\nNel luglio 2016 sono tornato in Italia e ho iniziato a lavorare in Nohup S.r.l. come full stack developer, ricoprendo il ruolo di R&D Team Leader nell’ultimo anno.\nMi sono laureato presso l’Università degli Studi Guglielmo Marconi, un’università telematica. Attualmente, sono uno studente part-time del corso magistrale in Artificial Intelligence all’Università degli Studi di Udine. Dopo la mia esperienza alla Marconi, ho deciso di affrontare il percorso magistrale con un ritmo più sostenibile.\nSono appassionato di tecnologia, scienza e insegnamento. Il mio più grande sogno nel cassetto è diventare insegnante di materie scientifiche.\n\n\n\n\nI’m Luca, and as I often say: just a poor dum**ss. Jokes aside, I’ve been a programmer for 16 years. I started developing small websites for private clients in November 2009, then spent five years as a full stack developer at Digitalfun S.r.l.\nIn November 2014, while still working at DF, I co-founded a startup with my cousin and a third partner. Two months later, I moved to London to seek new opportunities and worked for a year as a backend developer at SaveWaterSaveMoney Ltd, using Symfony.\nIn July 2016, I returned to Italy and joined Nohup S.r.l. as a full stack developer, eventually becoming the R&D Team Leader in my final year.\nI earned my degree from Università degli Studi Guglielmo Marconi, an online university. Currently, I’m a part-time Master’s student in Artificial Intelligence at the University of Udine. After my experience at Marconi, I decided to take the Master’s program at a more sustainable pace.\nI’m passionate about technology, science, and teaching. My biggest dream is to become a teacher in scientific subjects."
  }
]