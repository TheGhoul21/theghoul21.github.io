[
  {
    "objectID": "blog/blockchain.html",
    "href": "blog/blockchain.html",
    "title": "Blockchain: Non Solo Criptovalute",
    "section": "",
    "text": "Se non hai sentito parlare di Bitcoin e altre criptovalute vuol dire che fino ad oggi hai vissuto in una caverna, perch√© i media ormai hanno scartavetrato con questo argomento. Ecco, le criptovalute, spesso, si nascondono dietro a una tecnologia chiamata blockchain. Ma che cavolo √® veramente una blockchain? E serve solo alle valute virtuali? In questo articolo voglio addentrarmi un po‚Äô alla materia, scoprendo anche i suoi lati pi√π oscuri e magari meno noti in modo (spero) semplice e chiaro, senza bisogno di essere esperti di informatica o finanza.\n\n\nFirst things first‚Ä¶\nImmaginiamo di avere un registro pubblico, come un grande quaderno condiviso tra tantissime persone. Per esempio uno di questi registri potrebbe essere il vostro conto in banca, o il registro del catasto, o il registro delle nascite di un comune o il vostro personalissimo e dettagliatissimo resoconto delle maleducate che incontrate nei vostri felicissimi sabati sera.\nNormalmente, un registro √® gestito da una sola entit√†, una banca, un governo, voi, ecc. La blockchain invece √® diversa: √® un registro distribuito.\n\nEcco che comincia: che cazzo vuol dire distribuito??\n\nCalma.\nSignifica che questo ‚Äúquaderno‚Äù non √® in un unico posto (la banca, il comune, un server ecc.), ma √® copiato e tenuto aggiornato contemporaneamente da migliaia o milioni di computer in tutto il mondo.\n\nEh vabb√® capirai, non √® gi√† cos√¨? Eh? Facebook non ha milioni di computer? EHH??\n\nNon √® la stessa cosa. Nel caso di aziende come google, facebook ecc il fatto di avere una copia distribuita dei dati su tanti server fa comunque sempre capo ad un‚Äôunica autorit√† che √® appunto la multinazionale. In questo caso parliamo di persone comuni: io, te, la maleducata di venerd√¨ scorso.\n\nScusa, COSA? Tu vorresti distribuire i conti correnti della gente sui computer di tutta la gente? Ma sei rincoglionito?\n\nNo.¬†Calmati. Ci arriviamo.\nIntanto‚Ä¶ un altro po‚Äô di nomenclatura.\nOgni ‚Äúpagina‚Äù di questo quaderno √® chiamata blocco. In ogni blocco vengono scritte delle transazioni, ovvero delle informazioni. Una volta che una pagina (un blocco) √® piena, viene sigillata in modo speciale, usando una specie di ‚Äúimpronta digitale‚Äù matematica. Questa impronta digitale √® collegata alla pagina precedente, formando una catena (in inglese ‚Äúchain‚Äù) di blocchi: ecco perch√© si chiama blockchain, ‚Äúcatena di blocchi‚Äù.\n\nS√¨ ma perch√©?\n\nEh perch√©‚Ä¶ Perch√© se √® su tutti i server del mondo, di gente comune, bisogna trovare una maniera per far s√¨ che sia impossibile alterarla. Se qualche malintenzionato avesse il documento sul proprio computer e lo alterasse, comunicando agli altri la modifica potrebbe rubare i soldi della gente per esempio, o appropriarsi della casa di qualcuno. Ma se la pagina √® firmata allora √® impossibile farlo, perch√© nessuno riconoscerebbe la nuova firma (la firma non pu√≤ essere alterata da un solo partecipante, visto che viene generata da tutti).\nRiassunto intermedio:\nDunque, ricapitolandolo:\n\nDistribuita: Non controllata da uno solo, ma da tanti, tutti.\nTrasparente (ma non sempre, ci arriviamo): Tutti i partecipanti (o chi ha accesso alla blockchain, che pu√≤ essere pubblica o privata) possono vedere le ‚Äúpagine‚Äù (i blocchi) e le transazioni.\nImmutabile: Una volta scritta una ‚Äúpagina‚Äù (blocco) e sigillata, √® quasi impossibile modificarla o cancellarla. Questo √® dovuto all‚Äôuso dell‚Äô‚Äúimpronta digitale‚Äù e alla catena di blocchi. Dico quasi impossibile, perch√© se uno ha una botta di fortuna terrificante potrebbe alterare una pagina. Ma con botta di fortuna, intendo che √® pi√π probabile che Salvini diventi comunista.\n\nFacciamo un esempio pratico, che altrimenti vi perdete‚Ä¶\nPensiamo ad un gruppo di amici che vogliono tenere traccia di chi presta soldi a chi. Invece di usare foglietti volanti che si possono perdere o modificare di nascosto, decidono di usare un quaderno pubblico.\n\nQuaderno Pubblico: Mettono un quaderno al centro del tavolo, a disposizione di tutti.\nNuova Transazione (Blocco): Se Mariangela presta 10 euro a Gianfranco, tutti gli amici vedono Mariangela scrivere nel quaderno: ‚ÄúBlocco 1: Mariangela presta 10‚Ç¨ a Gianfranco‚Äù. Questo √® il primo ‚Äúblocco‚Äù.\nBlocco Successivo: Poi, se Gianfranco restituisce 5 euro a Mariangela, scrivono un nuovo ‚Äúblocco‚Äù sotto il precedente: ‚ÄúBlocco 2: Gianfranco restituisce 5‚Ç¨ a Mariangela‚Äù.\nCatena e Impronta Digitale (Semplificata): Immagina che ogni volta che scrivono un blocco, oltre al contenuto della transazione, scrivono anche un breve riassunto del blocco precedente (l‚Äôimpronta digitale semplificata). Cos√¨, il Blocco 2 riassumerebbe il Blocco 1. Se qualcuno volesse modificare il Blocco 1 dopo, dovrebbe cambiare anche il riassunto nel Blocco 2, e poi in tutti i blocchi successivi. Diventa complicatissimo e evidente che c‚Äô√® stato un tentativo di manipolazione.\n\nIn una blockchain vera, questo ‚Äúriassunto‚Äù (l‚Äô‚Äúimpronta digitale‚Äù, tecnicamente chiamata hash) √® molto complesso e generato da calcoli matematici avanzati, rendendo le modifiche praticamente impossibili senza essere scoperti.\nL‚Äôosservatore pi√π attento (nessuno‚Ä¶) avr√† notato una cosa particolare &gt; Ma Luca, perch√© cazzo dici ‚Äúpraticamente impossibile‚Äù? Vuol dire che √® possibile?\nS√¨. Come detto prima la probabilit√† di riuscirci √® talmente bassa che si pu√≤ dormire sonni molto tranquilli. Nel senso che: √® pi√π probabile avere un incidente d‚Äôauto, ma vedo che la gente non si barrica in casa per questo motivo.\n\n\n\nOk, le criptovalute come Bitcoin usano la blockchain. Ma la blockchain √® solo criptovaluta o c‚Äô√® altro? Beh che domanda del cavolo‚Ä¶ √® scritto nel titolo Luca.\nUn paio di esempi pratici e concreti:\n\nTracciamento della Filiera Alimentare e Prodotti: Immagina di comprare una bottiglia d‚Äôolio d‚Äôoliva. Grazie alla blockchain, potresti scansionare un codice sulla bottiglia e vedere tutta la storia di quell‚Äôolio: da quale uliveto provengono le olive, in quale frantoio sono state lavorate, quando √® stata imbottigliata, quando √® arrivata al supermercato. Questo rende la filiera pi√π trasparente e aiuta a combattere frodi e prodotti contraffatti. Funziona per il cibo, ma anche per vestiti, medicine, pezzi di ricambio per auto‚Ä¶ qualsiasi cosa compriate per soddisfare i vostri desideri animali. Putridi bestie.\nGestione dell‚ÄôIdentit√† Digitale: Oggi abbiamo tantissime password e identit√† digitali sparse ovunque (social media, banche, siti web‚Ä¶). La blockchain potrebbe permettere di creare un‚Äôunica identit√† digitale sicura e controllata da noi stessi. Invece di dare i nostri dati a ogni sito, potremmo usare la nostra ‚Äúidentit√† blockchain‚Äù per autenticarci, scegliendo quali informazioni condividere e con chi.\nContratti Intelligenti (Smart Contracts): Sono programmi che si attivano automaticamente quando si verificano certe condizioni. Immagina per esempio che l‚Äôolivaio di prima si accorda con il frantumaio. Gli dice: ‚Äúascolta capo, quando mi arrivano i prossimi 3 quintali di olive te li mando per 200 euro, fatta?‚Äù E il frantumaio che sa che l‚Äôolivaio √® un gran raccontaballe gli dice ‚Äúeh no capo‚Ä¶ che l‚Äôultima volta non mi hai mandato un bel niente‚Äù. Allora stipulano uno smart contract che si attiva in automatico, non appena arrivano le olive vengono consegnate. Nello smart contract potrebbe per esempio essere inclusa una penalit√† per cui se l‚Äôolivaio si rifiuta di consegnare le olive paga in automatico una multa. Non una multa che dice ‚Äús√¨ vabb√® sticazzi hai capito tutto, non pago niente‚Äù bens√¨ una multa che viene prelevata in automatico dal conto del signor olivaio.\n\nQuesti sono solo alcuni esempi. Le applicazioni della blockchain sono potenzialmente infinite e ne vengono fuori ogni giorno come i funghi.\n\n\n\nCome ogni tecnologia potente, ahim√©, anche la blockchain ha i suoi lati negativi e presenta delle sfide. √à importante conoscerli per avere un quadro completo e non farsi abbagliare solo dai vantaggi.\n\nDecentralizzazione e Attivit√† Illegali: Uno dei punti di forza della blockchain, la decentralizzazione, pu√≤ anche essere un punto debole. Se non c‚Äô√® un‚Äôautorit√† centrale di controllo, diventa pi√π difficile intervenire contro attivit√† illegali. Per esempio:\n\nCrimine Organizzato: Organizzazioni criminali potrebbero usare blockchain e criptovalute per riciclare denaro sporco, finanziare attivit√† illegali, o condurre transazioni non tracciabili. Nessuno saprebbe mai chi ha fatto che cosa.\nEvasione Fiscale: √à pi√π complicato per le autorit√† fiscali monitorare transazioni che avvengono al di fuori dei sistemi bancari tradizionali. Ma se l‚Äôintero sistema fosse gestito da blockchain sarebbe impossibile evadere le tasse visto che ogni transazione sarebbe tracciata e avrebbe delle fee.\nMercati Neri Online: Le blockchain possono facilitare la creazione di mercati online per beni e servizi illegali, come droghe o armi, rendendo pi√π difficile per le forze dell‚Äôordine smantellarli.\n\nOVVIAMENTE: Questo non significa che la blockchain √® fatta per il crimine. Significa che, come qualsiasi strumento, pu√≤ essere usata per scopi positivi e negativi. √à come internet: √® una risorsa fantastica se vi mandate le foto di gattini e i reel della gente che fa incidenti stradali. Ma pu√≤ essere fonte di disinformazione oltre che dei reel di Salvini che mangia le ciliegie (oggi ce l‚Äôho con Salvini perdonatemi).\nMancanza di Regolamentazione: Essendo una tecnologia decentralizzata, la blockchain e le criptovalute non sono regolate. Questo pu√≤ creare problemi:\n\nProtezione dei Consumatori: Se investi in criptovalute e perdi tutto, spesso non c‚Äô√® un‚Äôautorit√† a cui puoi rivolgerti per essere risarcito. Mancano le tutele che esistono nel sistema finanziario tradizionale.\nInstabilit√† e Speculazione: Il valore delle criptovalute √® estremamente volatile, con forti oscillazioni di prezzo. Questo le rende rischiose per gli investimenti e pu√≤ creare bolle speculative.\n\nScalabilit√† e Consumo Energetico: Alcune blockchain, come quella di Bitcoin, hanno problemi di scalabilit√†. Significa che possono gestire un numero limitato di transazioni al secondo. Se troppe persone vogliono usare la blockchain nello stesso momento, le transazioni possono diventare lente e costose. Inoltre, alcune blockchain (come ancora Bitcoin, anche se ci sono alternative pi√π efficienti) consumano molta energia elettrica per funzionare, il che ha un impatto ambientale negativo. Ovviamente questo non vale per tutte le blockchain: sono nati nel tempo metodi decisamente pi√π economici e meno dispendiosi in termini di energia consumata che risolvono parzialmente o totalmente questi problemi.\nRischi per la Sicurezza: Anche se la blockchain √® progettata per essere sicura, non √® immune da attacchi.\n\nAttacchi ‚Äú51%‚Äù: In alcune blockchain, se un singolo soggetto riuscisse a controllare pi√π del 50% della potenza di calcolo della rete, potrebbe teoricamente manipolare le transazioni. Anche se molto difficile, questo tipo di attacco √® un rischio teorico che deve sempre e comunque essere preso in considerazione. Ad esempio una soluzione √® garantirsi di avere il controllo sul 51% fitantoch√© la diffusione della blockchain √® alta a sufficienza da mitigare i rischi.\nSicurezza dei ‚ÄúPortafogli‚Äù Digitali (Wallet): Per usare criptovalute e interagire con la blockchain, si usano dei ‚Äúportafogli‚Äù digitali. Se questi portafogli non sono protetti adeguatamente (es. password deboli, phishing), si rischia di perdere i propri fondi. O anche (ed √® capitato‚Ä¶ ) non essendoci un‚Äôautorit√† centrale a cui fare riferimento se perdete la vostra ‚Äúpassword‚Äù siete fotttuti. Se vi dimenticate la password dell‚Äôaccount facebook dove chattate cone le escort, chiedete a facebook il reset della password e facebook gestisce tutto. Nel mondo blockchain se perdete la vostra chiave privata (\\(\\approx\\) password), basta, that‚Äôs it, siete tagliati fuori dal vostro portafoglio. Anche per questo punto ci sono soluzioni come i portafogli gestiti da autorit√† centrali. Il che per√≤ fa perdere un po‚Äô di senso alla blokchain.\n\n\n\n\n\nLa blockchain √® una tecnologia potenzialmente rivoluzionaria. Non √® assolutamente sinonimo di ‚Äúcriptovalute‚Äù e potrebbe trasformare molti settori attualmente gestiti in maniera tradizionale, dalla finanza alla logistica, dalla sanit√† alla pubblica amministrazione. Offre vantaggi importanti come la trasparenza, la sicurezza, la decentralizzazione e l‚Äôimmutabilit√† dei dati.\nPer√≤, √® fondamentale essere consapevoli anche dei rischi che comporta. La decentralizzazione, che √® un punto di forza, pu√≤ anche essere usata per attivit√† illegali. La mancanza di regolamentazione e la complessit√† tecnica sono ostacoli da superare. Come per ogni innovazione, √® importante un approccio equilibrato: sfruttare le potenzialit√† della blockchain, affrontando i rischi con consapevolezza e lavorando per creare un futuro digitale pi√π sicuro e trasparente per tutti.\nLa blockchain √® ancora in fase di sviluppo. Il futuro dipender√† da come sapremo guidare questa tecnologia e farla evolvere in modo responsabile e sostenibile, massimizzando i benefici e minimizzando i rischi.\n\n\n\nUn‚Äôultima riflessione. Avrai notato che io, Luca Simonetti, sono l‚Äôautore di questo articolo. L‚Äôho scritto, l‚Äôho pubblicato (ipoteticamente, eh!). Ora, immagina che tra un‚Äôora io veda la foto della tipa mezza gnuda su instagram, o che cambi idea su qualcosa che ho scritto, o che semplicemente trovi un errore di battitura che mi infastidisce. Posso tranquillamente tornare qui e modificare il cazzo che voglio. Posso cambiare intere frasi, aggiungere o togliere paragrafi, stravolgere il senso di quello che ho detto. E nessuno potrebbe contestare pi√π di tanto la cosa, perch√© l‚Äôautorit√† che gestisce questo contenuto (io, in questo caso) ho il pieno controllo e posso cambiarlo a mio piacimento.\nChi ha letto questo articolo ora, e magari lo rilegge tra qualche ora, potrebbe trovarlo diverso. Non c‚Äô√® un modo per dimostrare quale fosse la versione originale, se non affidandosi alla mia parola (o a servizi esterni che archiviano pagine web, ma che sono comunque ‚Äúesterni‚Äù e non parte integrante dell‚Äôarticolo stesso).\nEcco, questa √® la grande differenza con un‚Äôinformazione registrata su una blockchain. Una volta che un blocco di informazioni viene scritto e ‚Äúsigillato‚Äù sulla catena, non pu√≤ essere modificato retroattivamente in modo furtivo. Ogni modifica lascerebbe una traccia evidente e la versione originale resterebbe sempre verificabile e consultabile da chiunque. Questo articolo, invece, √® soggetto al capriccio del suo autore (ovvero il mio!) e alla natura effimera del web tradizionale. Un piccolo esempio concreto per capire ancora meglio la potenza (e la differenza!) della blockchain.\nSpoiler: questo ultimo paragrafo l‚Äôho aggiunto dopo aver pubblicato la versione originale dell‚Äôarticolo. E se nesusno l‚Äôavesse letto prima e se io non avessi scritto questa roba qui, nessuno l‚Äôavrebbe mai saputo."
  },
  {
    "objectID": "blog/blockchain.html#che-cos√®-in-parole-semplici-una-blockchain",
    "href": "blog/blockchain.html#che-cos√®-in-parole-semplici-una-blockchain",
    "title": "Blockchain: Non Solo Criptovalute",
    "section": "",
    "text": "First things first‚Ä¶\nImmaginiamo di avere un registro pubblico, come un grande quaderno condiviso tra tantissime persone. Per esempio uno di questi registri potrebbe essere il vostro conto in banca, o il registro del catasto, o il registro delle nascite di un comune o il vostro personalissimo e dettagliatissimo resoconto delle maleducate che incontrate nei vostri felicissimi sabati sera.\nNormalmente, un registro √® gestito da una sola entit√†, una banca, un governo, voi, ecc. La blockchain invece √® diversa: √® un registro distribuito.\n\nEcco che comincia: che cazzo vuol dire distribuito??\n\nCalma.\nSignifica che questo ‚Äúquaderno‚Äù non √® in un unico posto (la banca, il comune, un server ecc.), ma √® copiato e tenuto aggiornato contemporaneamente da migliaia o milioni di computer in tutto il mondo.\n\nEh vabb√® capirai, non √® gi√† cos√¨? Eh? Facebook non ha milioni di computer? EHH??\n\nNon √® la stessa cosa. Nel caso di aziende come google, facebook ecc il fatto di avere una copia distribuita dei dati su tanti server fa comunque sempre capo ad un‚Äôunica autorit√† che √® appunto la multinazionale. In questo caso parliamo di persone comuni: io, te, la maleducata di venerd√¨ scorso.\n\nScusa, COSA? Tu vorresti distribuire i conti correnti della gente sui computer di tutta la gente? Ma sei rincoglionito?\n\nNo.¬†Calmati. Ci arriviamo.\nIntanto‚Ä¶ un altro po‚Äô di nomenclatura.\nOgni ‚Äúpagina‚Äù di questo quaderno √® chiamata blocco. In ogni blocco vengono scritte delle transazioni, ovvero delle informazioni. Una volta che una pagina (un blocco) √® piena, viene sigillata in modo speciale, usando una specie di ‚Äúimpronta digitale‚Äù matematica. Questa impronta digitale √® collegata alla pagina precedente, formando una catena (in inglese ‚Äúchain‚Äù) di blocchi: ecco perch√© si chiama blockchain, ‚Äúcatena di blocchi‚Äù.\n\nS√¨ ma perch√©?\n\nEh perch√©‚Ä¶ Perch√© se √® su tutti i server del mondo, di gente comune, bisogna trovare una maniera per far s√¨ che sia impossibile alterarla. Se qualche malintenzionato avesse il documento sul proprio computer e lo alterasse, comunicando agli altri la modifica potrebbe rubare i soldi della gente per esempio, o appropriarsi della casa di qualcuno. Ma se la pagina √® firmata allora √® impossibile farlo, perch√© nessuno riconoscerebbe la nuova firma (la firma non pu√≤ essere alterata da un solo partecipante, visto che viene generata da tutti).\nRiassunto intermedio:\nDunque, ricapitolandolo:\n\nDistribuita: Non controllata da uno solo, ma da tanti, tutti.\nTrasparente (ma non sempre, ci arriviamo): Tutti i partecipanti (o chi ha accesso alla blockchain, che pu√≤ essere pubblica o privata) possono vedere le ‚Äúpagine‚Äù (i blocchi) e le transazioni.\nImmutabile: Una volta scritta una ‚Äúpagina‚Äù (blocco) e sigillata, √® quasi impossibile modificarla o cancellarla. Questo √® dovuto all‚Äôuso dell‚Äô‚Äúimpronta digitale‚Äù e alla catena di blocchi. Dico quasi impossibile, perch√© se uno ha una botta di fortuna terrificante potrebbe alterare una pagina. Ma con botta di fortuna, intendo che √® pi√π probabile che Salvini diventi comunista.\n\nFacciamo un esempio pratico, che altrimenti vi perdete‚Ä¶\nPensiamo ad un gruppo di amici che vogliono tenere traccia di chi presta soldi a chi. Invece di usare foglietti volanti che si possono perdere o modificare di nascosto, decidono di usare un quaderno pubblico.\n\nQuaderno Pubblico: Mettono un quaderno al centro del tavolo, a disposizione di tutti.\nNuova Transazione (Blocco): Se Mariangela presta 10 euro a Gianfranco, tutti gli amici vedono Mariangela scrivere nel quaderno: ‚ÄúBlocco 1: Mariangela presta 10‚Ç¨ a Gianfranco‚Äù. Questo √® il primo ‚Äúblocco‚Äù.\nBlocco Successivo: Poi, se Gianfranco restituisce 5 euro a Mariangela, scrivono un nuovo ‚Äúblocco‚Äù sotto il precedente: ‚ÄúBlocco 2: Gianfranco restituisce 5‚Ç¨ a Mariangela‚Äù.\nCatena e Impronta Digitale (Semplificata): Immagina che ogni volta che scrivono un blocco, oltre al contenuto della transazione, scrivono anche un breve riassunto del blocco precedente (l‚Äôimpronta digitale semplificata). Cos√¨, il Blocco 2 riassumerebbe il Blocco 1. Se qualcuno volesse modificare il Blocco 1 dopo, dovrebbe cambiare anche il riassunto nel Blocco 2, e poi in tutti i blocchi successivi. Diventa complicatissimo e evidente che c‚Äô√® stato un tentativo di manipolazione.\n\nIn una blockchain vera, questo ‚Äúriassunto‚Äù (l‚Äô‚Äúimpronta digitale‚Äù, tecnicamente chiamata hash) √® molto complesso e generato da calcoli matematici avanzati, rendendo le modifiche praticamente impossibili senza essere scoperti.\nL‚Äôosservatore pi√π attento (nessuno‚Ä¶) avr√† notato una cosa particolare &gt; Ma Luca, perch√© cazzo dici ‚Äúpraticamente impossibile‚Äù? Vuol dire che √® possibile?\nS√¨. Come detto prima la probabilit√† di riuscirci √® talmente bassa che si pu√≤ dormire sonni molto tranquilli. Nel senso che: √® pi√π probabile avere un incidente d‚Äôauto, ma vedo che la gente non si barrica in casa per questo motivo."
  },
  {
    "objectID": "blog/blockchain.html#blockchain-non-solo-criptovalute-1",
    "href": "blog/blockchain.html#blockchain-non-solo-criptovalute-1",
    "title": "Blockchain: Non Solo Criptovalute",
    "section": "",
    "text": "Ok, le criptovalute come Bitcoin usano la blockchain. Ma la blockchain √® solo criptovaluta o c‚Äô√® altro? Beh che domanda del cavolo‚Ä¶ √® scritto nel titolo Luca.\nUn paio di esempi pratici e concreti:\n\nTracciamento della Filiera Alimentare e Prodotti: Immagina di comprare una bottiglia d‚Äôolio d‚Äôoliva. Grazie alla blockchain, potresti scansionare un codice sulla bottiglia e vedere tutta la storia di quell‚Äôolio: da quale uliveto provengono le olive, in quale frantoio sono state lavorate, quando √® stata imbottigliata, quando √® arrivata al supermercato. Questo rende la filiera pi√π trasparente e aiuta a combattere frodi e prodotti contraffatti. Funziona per il cibo, ma anche per vestiti, medicine, pezzi di ricambio per auto‚Ä¶ qualsiasi cosa compriate per soddisfare i vostri desideri animali. Putridi bestie.\nGestione dell‚ÄôIdentit√† Digitale: Oggi abbiamo tantissime password e identit√† digitali sparse ovunque (social media, banche, siti web‚Ä¶). La blockchain potrebbe permettere di creare un‚Äôunica identit√† digitale sicura e controllata da noi stessi. Invece di dare i nostri dati a ogni sito, potremmo usare la nostra ‚Äúidentit√† blockchain‚Äù per autenticarci, scegliendo quali informazioni condividere e con chi.\nContratti Intelligenti (Smart Contracts): Sono programmi che si attivano automaticamente quando si verificano certe condizioni. Immagina per esempio che l‚Äôolivaio di prima si accorda con il frantumaio. Gli dice: ‚Äúascolta capo, quando mi arrivano i prossimi 3 quintali di olive te li mando per 200 euro, fatta?‚Äù E il frantumaio che sa che l‚Äôolivaio √® un gran raccontaballe gli dice ‚Äúeh no capo‚Ä¶ che l‚Äôultima volta non mi hai mandato un bel niente‚Äù. Allora stipulano uno smart contract che si attiva in automatico, non appena arrivano le olive vengono consegnate. Nello smart contract potrebbe per esempio essere inclusa una penalit√† per cui se l‚Äôolivaio si rifiuta di consegnare le olive paga in automatico una multa. Non una multa che dice ‚Äús√¨ vabb√® sticazzi hai capito tutto, non pago niente‚Äù bens√¨ una multa che viene prelevata in automatico dal conto del signor olivaio.\n\nQuesti sono solo alcuni esempi. Le applicazioni della blockchain sono potenzialmente infinite e ne vengono fuori ogni giorno come i funghi."
  },
  {
    "objectID": "blog/blockchain.html#non-√®-tutto-oro-ci√≤-che-luccica.",
    "href": "blog/blockchain.html#non-√®-tutto-oro-ci√≤-che-luccica.",
    "title": "Blockchain: Non Solo Criptovalute",
    "section": "",
    "text": "Come ogni tecnologia potente, ahim√©, anche la blockchain ha i suoi lati negativi e presenta delle sfide. √à importante conoscerli per avere un quadro completo e non farsi abbagliare solo dai vantaggi.\n\nDecentralizzazione e Attivit√† Illegali: Uno dei punti di forza della blockchain, la decentralizzazione, pu√≤ anche essere un punto debole. Se non c‚Äô√® un‚Äôautorit√† centrale di controllo, diventa pi√π difficile intervenire contro attivit√† illegali. Per esempio:\n\nCrimine Organizzato: Organizzazioni criminali potrebbero usare blockchain e criptovalute per riciclare denaro sporco, finanziare attivit√† illegali, o condurre transazioni non tracciabili. Nessuno saprebbe mai chi ha fatto che cosa.\nEvasione Fiscale: √à pi√π complicato per le autorit√† fiscali monitorare transazioni che avvengono al di fuori dei sistemi bancari tradizionali. Ma se l‚Äôintero sistema fosse gestito da blockchain sarebbe impossibile evadere le tasse visto che ogni transazione sarebbe tracciata e avrebbe delle fee.\nMercati Neri Online: Le blockchain possono facilitare la creazione di mercati online per beni e servizi illegali, come droghe o armi, rendendo pi√π difficile per le forze dell‚Äôordine smantellarli.\n\nOVVIAMENTE: Questo non significa che la blockchain √® fatta per il crimine. Significa che, come qualsiasi strumento, pu√≤ essere usata per scopi positivi e negativi. √à come internet: √® una risorsa fantastica se vi mandate le foto di gattini e i reel della gente che fa incidenti stradali. Ma pu√≤ essere fonte di disinformazione oltre che dei reel di Salvini che mangia le ciliegie (oggi ce l‚Äôho con Salvini perdonatemi).\nMancanza di Regolamentazione: Essendo una tecnologia decentralizzata, la blockchain e le criptovalute non sono regolate. Questo pu√≤ creare problemi:\n\nProtezione dei Consumatori: Se investi in criptovalute e perdi tutto, spesso non c‚Äô√® un‚Äôautorit√† a cui puoi rivolgerti per essere risarcito. Mancano le tutele che esistono nel sistema finanziario tradizionale.\nInstabilit√† e Speculazione: Il valore delle criptovalute √® estremamente volatile, con forti oscillazioni di prezzo. Questo le rende rischiose per gli investimenti e pu√≤ creare bolle speculative.\n\nScalabilit√† e Consumo Energetico: Alcune blockchain, come quella di Bitcoin, hanno problemi di scalabilit√†. Significa che possono gestire un numero limitato di transazioni al secondo. Se troppe persone vogliono usare la blockchain nello stesso momento, le transazioni possono diventare lente e costose. Inoltre, alcune blockchain (come ancora Bitcoin, anche se ci sono alternative pi√π efficienti) consumano molta energia elettrica per funzionare, il che ha un impatto ambientale negativo. Ovviamente questo non vale per tutte le blockchain: sono nati nel tempo metodi decisamente pi√π economici e meno dispendiosi in termini di energia consumata che risolvono parzialmente o totalmente questi problemi.\nRischi per la Sicurezza: Anche se la blockchain √® progettata per essere sicura, non √® immune da attacchi.\n\nAttacchi ‚Äú51%‚Äù: In alcune blockchain, se un singolo soggetto riuscisse a controllare pi√π del 50% della potenza di calcolo della rete, potrebbe teoricamente manipolare le transazioni. Anche se molto difficile, questo tipo di attacco √® un rischio teorico che deve sempre e comunque essere preso in considerazione. Ad esempio una soluzione √® garantirsi di avere il controllo sul 51% fitantoch√© la diffusione della blockchain √® alta a sufficienza da mitigare i rischi.\nSicurezza dei ‚ÄúPortafogli‚Äù Digitali (Wallet): Per usare criptovalute e interagire con la blockchain, si usano dei ‚Äúportafogli‚Äù digitali. Se questi portafogli non sono protetti adeguatamente (es. password deboli, phishing), si rischia di perdere i propri fondi. O anche (ed √® capitato‚Ä¶ ) non essendoci un‚Äôautorit√† centrale a cui fare riferimento se perdete la vostra ‚Äúpassword‚Äù siete fotttuti. Se vi dimenticate la password dell‚Äôaccount facebook dove chattate cone le escort, chiedete a facebook il reset della password e facebook gestisce tutto. Nel mondo blockchain se perdete la vostra chiave privata (\\(\\approx\\) password), basta, that‚Äôs it, siete tagliati fuori dal vostro portafoglio. Anche per questo punto ci sono soluzioni come i portafogli gestiti da autorit√† centrali. Il che per√≤ fa perdere un po‚Äô di senso alla blokchain."
  },
  {
    "objectID": "blog/blockchain.html#quindi",
    "href": "blog/blockchain.html#quindi",
    "title": "Blockchain: Non Solo Criptovalute",
    "section": "",
    "text": "La blockchain √® una tecnologia potenzialmente rivoluzionaria. Non √® assolutamente sinonimo di ‚Äúcriptovalute‚Äù e potrebbe trasformare molti settori attualmente gestiti in maniera tradizionale, dalla finanza alla logistica, dalla sanit√† alla pubblica amministrazione. Offre vantaggi importanti come la trasparenza, la sicurezza, la decentralizzazione e l‚Äôimmutabilit√† dei dati.\nPer√≤, √® fondamentale essere consapevoli anche dei rischi che comporta. La decentralizzazione, che √® un punto di forza, pu√≤ anche essere usata per attivit√† illegali. La mancanza di regolamentazione e la complessit√† tecnica sono ostacoli da superare. Come per ogni innovazione, √® importante un approccio equilibrato: sfruttare le potenzialit√† della blockchain, affrontando i rischi con consapevolezza e lavorando per creare un futuro digitale pi√π sicuro e trasparente per tutti.\nLa blockchain √® ancora in fase di sviluppo. Il futuro dipender√† da come sapremo guidare questa tecnologia e farla evolvere in modo responsabile e sostenibile, massimizzando i benefici e minimizzando i rischi."
  },
  {
    "objectID": "blog/blockchain.html#post-scriptum-e-questo-articolo",
    "href": "blog/blockchain.html#post-scriptum-e-questo-articolo",
    "title": "Blockchain: Non Solo Criptovalute",
    "section": "",
    "text": "Un‚Äôultima riflessione. Avrai notato che io, Luca Simonetti, sono l‚Äôautore di questo articolo. L‚Äôho scritto, l‚Äôho pubblicato (ipoteticamente, eh!). Ora, immagina che tra un‚Äôora io veda la foto della tipa mezza gnuda su instagram, o che cambi idea su qualcosa che ho scritto, o che semplicemente trovi un errore di battitura che mi infastidisce. Posso tranquillamente tornare qui e modificare il cazzo che voglio. Posso cambiare intere frasi, aggiungere o togliere paragrafi, stravolgere il senso di quello che ho detto. E nessuno potrebbe contestare pi√π di tanto la cosa, perch√© l‚Äôautorit√† che gestisce questo contenuto (io, in questo caso) ho il pieno controllo e posso cambiarlo a mio piacimento.\nChi ha letto questo articolo ora, e magari lo rilegge tra qualche ora, potrebbe trovarlo diverso. Non c‚Äô√® un modo per dimostrare quale fosse la versione originale, se non affidandosi alla mia parola (o a servizi esterni che archiviano pagine web, ma che sono comunque ‚Äúesterni‚Äù e non parte integrante dell‚Äôarticolo stesso).\nEcco, questa √® la grande differenza con un‚Äôinformazione registrata su una blockchain. Una volta che un blocco di informazioni viene scritto e ‚Äúsigillato‚Äù sulla catena, non pu√≤ essere modificato retroattivamente in modo furtivo. Ogni modifica lascerebbe una traccia evidente e la versione originale resterebbe sempre verificabile e consultabile da chiunque. Questo articolo, invece, √® soggetto al capriccio del suo autore (ovvero il mio!) e alla natura effimera del web tradizionale. Un piccolo esempio concreto per capire ancora meglio la potenza (e la differenza!) della blockchain.\nSpoiler: questo ultimo paragrafo l‚Äôho aggiunto dopo aver pubblicato la versione originale dell‚Äôarticolo. E se nesusno l‚Äôavesse letto prima e se io non avessi scritto questa roba qui, nessuno l‚Äôavrebbe mai saputo."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "L‚Äôs blog",
    "section": "",
    "text": "Hashing: Che Cos‚Äô√® ‚Äôsto Hashing?\n\n\n\n\n\n\ncomputer science\n\n\n\n\n\n\n\n\n\nFeb 7, 2025\n\n\nLuca Simonetti\n\n\n\n\n\n\n\n\n\n\n\n\nBlockchain: Non Solo Criptovalute\n\n\n\n\n\n\ncomputer science\n\n\n\n\n\n\n\n\n\nFeb 2, 2025\n\n\nLuca Simonetti\n\n\n\n\n\n\n\n\n\n\n\n\nGruppi di Ordine Primo\n\n\n\n\n\n\nmath\n\n\ncryptography\n\n\nalgebra\n\n\n\n\n\n\n\n\n\nFeb 1, 2025\n\n\nLuca Simonetti\n\n\n\n\n\n\n\n\n\n\n\n\nShamir‚Äôs Secret Sharing\n\n\n\n\n\n\nprogramming\n\n\nweb development\n\n\n\n\n\n\n\n\n\nJan 30, 2025\n\n\nLuca Simonetti\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/hashing.html",
    "href": "blog/hashing.html",
    "title": "Hashing: Che Cos‚Äô√® ‚Äôsto Hashing?",
    "section": "",
    "text": "Se hai mai sentito parlare di password, sicurezza informatica o anche solo di blockchain (di cui abbiamo parlato la settimana scorsa, se te lo sei perso sei un pirla), probabilmente ti sei imbattuto in questa parola: hashing. Ma che cavolo √® sta roba? Sembra il nome di una nuova droga sintetica o di un ballo di gruppo degli anni ‚Äô80. In realt√†, √® una figata (nel senso buono, non come quando ti dicono ‚Äúche figata le tue Crocs üòê‚Äù). In questo articolo voglio spiegare (spero) in parole semplici cosa √® l‚Äôhashing, perch√© √® importante e perch√© conviene smettere di usare password di merda come ‚Äúpassword123‚Äù.\n\n\nPartiamo dalle basi, come quando a scuola di cucina ti spiegano come si le uova fritte (eppure, ahim√©, anche riesce a fare un disastro‚Ä¶).\nImmagina di avere una di quelle macchinette per fare la carne trita, hai presente? Ci metti dentro un pezzo di carne, giri la manovella e dall‚Äôaltra parte esce carne trita. Ecco, l‚Äôhashing √® un po‚Äô come quella macchinetta, ma per i dati.\n\nAh, figo, quindi se ci metto dentro un PDF esce‚Ä¶ un PDF tritato?\n\nMinchia, sei simpatico, ti uccider√≤ per ultimo. No, non proprio. Non trita i dati nel senso che li spezzetta. Ma in un certo senso li trasforma. La macchinetta dell‚Äôhashing prende qualsiasi tipo di dato (un testo, un‚Äôimmagine, un video, un file intero, la tua lista di maleducate) e lo trasforma in una stringa (una sequnza, ma in gergo si chiama stringa) di caratteri di lunghezza fissa, che all‚Äôapparenza √® completamente casuale. Questa stringa (sequenza) di caratteri √® chiamata hash (o ‚Äúvalore hash‚Äù, o ‚Äúdigest‚Äù, insomma, chiamala come cazzo ti pare, basta che ci capiamo).\n\nOk, ok, ci sono quasi. Ma perch√© dovrebbe servirmi trasformare i miei dati in una stringa di caratteri a caso? Che me ne faccio? Ci tappezzo il cesso?\n\nCalma, non fare il fenomeno, che cazzo hai mangiato a colazione?! L‚Äôhash ha delle propriet√† molto particolari che lo rendono decisamente utile. In ordine (o no?):\n\n√à Deterministico: Se prendi lo stesso dato e lo passi alla stessa ‚Äúmacchinetta‚Äù (funzione di hash), otterrai sempre lo stesso identico hash. Sempre. Non importa quante volte lo fai, il risultato sar√† sempre lo stesso. √à come se la macchinetta avesse una memoria di ferro.\n\nQuindi se faccio l‚Äôhash della parola ‚Äúciao‚Äù ottengo sempre lo stesso hash?\n\nEsatto. Prova. Ci sono un sacco di siti online che fanno hash. Cerca ‚Äúonline hash calculator‚Äù, mettici ‚Äúciao‚Äù e usa l‚Äôalgoritmo SHA-256 (uno dei pi√π comuni). Vedrai che ti uscir√† sempre la stessa stringa di caratteri (tipo b133a0c0e9bee3be20163d2ad31d6248db292aa6dcb1ee087a2aa50e0fc75ae2, ma non ti fissare con questa stringa, √® solo un esempio).\n√à One-Way (A Senso Unico): Questa √® la parte veramente interessante e importante. √à praticamente impossibile (e dico praticamente perch√© nella vita nulla √® impossibile al 100%, tranne forse te che te ne stai a casa il sabato sera invece di andare a far baldoria con Salvini) tornare indietro dall‚Äôhash al dato originale. Cio√®, se hai l‚Äôhash, non puoi in alcun modo (se non per forza bruta) ricostruire il dato di partenza. √à come la macchinetta della carne trita: puoi fare la carne trita dalla carne, ma non puoi rifare il pezzo di carne dalla carne trita. Chiaro no?\n\nS√¨, s√¨, ho capito. Quindi se io faccio l‚Äôhash della mia password ‚Äúpassword123‚Äù, anche se qualcuno ruba l‚Äôhash, non pu√≤ risalire alla mia password?\n\nYES! Finalmente hai detto una roba giusta, porca troia. Ed √® proprio per questo che l‚Äôhashing √® fondamentale per la sicurezza delle password. Quando ti registri su un sito web e scegli una password, il sito non salva la tua password originale (sarebbe da cretini, se lo fanno prendi un martello). Bens√¨, salva solo l‚Äôhash della tua password. Quando poi fai il login e inserisci la tua password, il sito calcola l‚Äôhash della password che hai inserito e lo confronta con l‚Äôhash che ha salvato. Se i due hash coincidono, significa che hai inserito la password giusta, senza che il sito abbia mai dovuto conoscere la tua password in chiaro. Cos√¨ l‚Äôunico che in teoria sa la tua password sei tu (e la tua morosa‚Ä¶).\n√à Resistente alle Collisioni (Idealmente, come sempre): Questa √® un po‚Äô pi√π complicata, ma cercher√≤ di semplificare al massimo. Idealmente (ovvero nei mondi con le fate, le lucine eccetera), √® estremamente improbabile che due dati diversi producano lo stesso hash. Si chiama ‚Äúcollisione‚Äù quando due dati diversi producono lo stesso hash. Una buona funzione di hash √® progettata per rendere le collisioni praticamente impossibili (di nuovo, praticamente). √à come dire che √® quasi impossibile che due pezzi di carne diversi, messi nella stessa macchinetta, producano esattamente la stessa carne trita. Capito no? Pi√π o meno dai‚Ä¶\n\nPi√π o meno‚Ä¶ Ma quindi le collisioni possono succedere?\n\nS√¨, in teoria (e badate, che le cose che possono succedere in teoria anche con probabilit√† infime sono tante. Tipo che morite in un incidente d‚Äôauto, cos√¨, per dire.) possono succedere, ma con le funzioni di hash moderne e robuste, la probabilit√† √® talmente bassa che √® come vincere la lotteria 10 volte di fila mentre ti cade un meteorite in testa e Salvini dice una roba sensata. Insomma, per scopi pratici, possiamo dire che le collisioni sono trascurabili. Esistono per√≤ funzioni di hash pi√π vecchie e meno sicure che sono pi√π vulnerabili alle collisioni, ma c‚Äô√® gi√† troppa carne al fuoco (pun intended).\n\nRicapitolandolo:\nQuindi, ricapitolando, l‚Äôhashing √®:\n\nUna macchinetta per i dati: Prende qualsiasi dato e lo trasforma in una stringa di caratteri di lunghezza fissa (hash).\nDeterministico: Stesso dato, stesso hash, sempre.\nOne-Way: Impossibile (praticamente) tornare indietro dall‚Äôhash al dato originale.\nResistente alle Collisioni: Improbabile (molto improbabile) che due dati diversi producano lo stesso hash.\n\n\n\n\nOk, abbiamo capito che cos‚Äô√® l‚Äôhashing e le sue propriet√† magiche. Ma in pratica, a cosa serve? Oltre a proteggere le password (che √® gi√† una figata non da poco), l‚Äôhashing ha un sacco di altre applicazioni. Eccone alcune:\n\nVerifica dell‚ÄôIntegrit√† dei Dati: Immagina di scaricare un file enorme da internet, tipo un film in HD o un videogioco. Come fai a essere sicuro che il file che hai scaricato sia integro e non sia stato corrotto durante il download? Semplice: il sito web da cui hai scaricato il file di solito fornisce anche l‚Äôhash del file. Tu, dopo aver scaricato il file, puoi calcolare l‚Äôhash del file scaricato e confrontarlo con l‚Äôhash fornito dal sito. Se i due hash coincidono, significa che il file √® integro al 100%. Se non coincidono, significa che c‚Äô√® stato un problema durante il download e il file √® corrotto (o peggio, potrebbe essere stato manomesso da qualcuno). √à come avere un sigillo di garanzia digitale sul file. Figata, no?\nBlockchain e Criptovalute: Se hai letto l‚Äôarticolo sulla blockchain (e se non l‚Äôhai fatto, vergognati, tanto.), sai che l‚Äôhashing √® un ingrediente fondamentale della blockchain. Viene usato per creare l‚Äô‚Äúimpronta digitale‚Äù di ogni blocco e per collegare i blocchi in una catena immutabile. Senza l‚Äôhashing, la blockchain non esisterebbe. Quindi, se ti piacciono le criptovalute (o se le odi, non importa), sappi che devi ringraziare (o maledire) l‚Äôhashing.\nFirme Digitali: L‚Äôhashing √® anche usato nelle firme digitali per garantire l‚Äôautenticit√† e l‚Äôintegrit√† dei documenti digitali. Quando firmi digitalmente un documento, in realt√† stai firmando l‚Äôhash del documento, non il documento intero. Questo rende la firma molto pi√π efficiente e sicura.\nRicerca Efficiente di Dati: L‚Äôhashing pu√≤ essere usato per creare delle ‚Äútabelle hash‚Äù (o ‚Äúhash map‚Äù), che sono delle strutture dati che permettono di cercare dati in modo molto veloce. √à un po‚Äô come avere un indice di un libro: invece di dover sfogliare tutto il libro per trovare un‚Äôinformazione, puoi consultare l‚Äôindice e trovarla subito.\n\n\n\n\nAnche l‚Äôhashing, come tutte le cose belle della vita, tipo me, ha i suoi limiti:\n\nCollisioni (anche se improbabili): Abbiamo detto che le collisioni sono molto improbabili, ma non impossibili. In teoria, un attaccante potrebbe cercare di trovare due dati diversi che producono lo stesso hash (un ‚Äúattacco di collisione‚Äù). Se ci riuscisse, potrebbe usare questa collisione per scopi malevoli, tipo sostituire un file legittimo con uno fasullo che ha lo stesso hash. Per fortuna, trovare collisioni per le funzioni di hash moderne e robuste √® estremamente difficile e costoso, ma √® un rischio teorico da tenere presente.\nAttacchi di ‚ÄúRainbow Table‚Äù e ‚ÄúBrute Force‚Äù alle Password: Anche se l‚Äôhashing rende difficile risalire alla password originale dall‚Äôhash, non √® una protezione infallibile al 100%. Esistono tecniche come gli attacchi di ‚Äúrainbow table‚Äù e ‚Äúbrute force‚Äù che possono essere usate per cercare di ‚Äúindovinare‚Äù le password a partire dagli hash. Le ‚Äúrainbow table‚Äù sono delle tabelle precalcolate che contengono gli hash di milioni di password comuni. Gli attacchi ‚Äúbrute force‚Äù invece consistono nel provare a calcolare l‚Äôhash di tutte le possibili password finch√© non si trova una corrispondenza con l‚Äôhash rubato. Per difendersi da questi attacchi, √® fondamentale usare password lunghe, complesse e diverse per ogni sito. E smettetela di usare ‚Äúpassword123‚Äù, che cazzo!\nNon √® Crittografia: √à importante capire che l‚Äôhashing non √® crittografia. La crittografia serve per nascondere i dati, rendendoli illeggibili a chi non ha la chiave giusta. L‚Äôhashing serve per creare un‚Äôimpronta digitale dei dati, per verificarne l‚Äôintegrit√† e l‚Äôautenticit√†. E‚Äô importante non confondere le due cose. √à come confondere un lucchetto (crittografia) con un sigillo di ceralacca (hashing). Entrambi servono per la sicurezza, ma in modi diversi.\n\n\n\n\nL‚Äôhashing √® una tecnologia che sta alla base nel mondo digitale di oggi e di un sacco di cose che usiamo tutti i giorni, dalle password alla blockchain, dalla verifica dell‚Äôintegrit√† dei file alle firme digitali. √à una tecnologia potente e versatile, ma √® importante conoscerne anche i limiti e le potenziali vulnerabilit√†.\nLa prossima volta che sentirai parlare di hashing pensa alla macchinetta per la carne trita dei dati, alla one-way, e alla sicurezza che pu√≤ garantire. E soprattutto, smettila di usare password di merda! Grazie per l‚Äôattenzione e alla prossima settimana con un altro argomento nerd e (spero) interessante.\n\n\n\nGiusto per fare i fighi e per farti capire ancora meglio la potenza dell‚Äôhashing, ho calcolato l‚Äôhash SHA-256 di questo articolo (in formato testo semplice, senza HTML o altre formattazioni). Ecco l‚Äôhash:\n31e452bbf5134ac1d0d0a3a5897359b83eaccf6a46aed7d4323ddb51ec2d76f0\nOra, se io modificassi anche solo una virgola di questo articolo, anche solo uno spazio bianco, anche solo una lettera, l‚Äôhash cambierebbe completamente. Questo dimostra quanto anche una piccola modifica al dato originale cambi radicalmente l‚Äôhash. √à proprio questa propriet√† che rende l‚Äôhashing cos√¨ utile per la verifica dell‚Äôintegrit√† dei dati. E anche per farmi sentire un grande (imbecille) a scrivere un post scriptum del genere. Ciao stelline!"
  },
  {
    "objectID": "blog/hashing.html#che-cos√®-in-parole-povere-lhashing",
    "href": "blog/hashing.html#che-cos√®-in-parole-povere-lhashing",
    "title": "Hashing: Che Cos‚Äô√® ‚Äôsto Hashing?",
    "section": "",
    "text": "Partiamo dalle basi, come quando a scuola di cucina ti spiegano come si le uova fritte (eppure, ahim√©, anche riesce a fare un disastro‚Ä¶).\nImmagina di avere una di quelle macchinette per fare la carne trita, hai presente? Ci metti dentro un pezzo di carne, giri la manovella e dall‚Äôaltra parte esce carne trita. Ecco, l‚Äôhashing √® un po‚Äô come quella macchinetta, ma per i dati.\n\nAh, figo, quindi se ci metto dentro un PDF esce‚Ä¶ un PDF tritato?\n\nMinchia, sei simpatico, ti uccider√≤ per ultimo. No, non proprio. Non trita i dati nel senso che li spezzetta. Ma in un certo senso li trasforma. La macchinetta dell‚Äôhashing prende qualsiasi tipo di dato (un testo, un‚Äôimmagine, un video, un file intero, la tua lista di maleducate) e lo trasforma in una stringa (una sequnza, ma in gergo si chiama stringa) di caratteri di lunghezza fissa, che all‚Äôapparenza √® completamente casuale. Questa stringa (sequenza) di caratteri √® chiamata hash (o ‚Äúvalore hash‚Äù, o ‚Äúdigest‚Äù, insomma, chiamala come cazzo ti pare, basta che ci capiamo).\n\nOk, ok, ci sono quasi. Ma perch√© dovrebbe servirmi trasformare i miei dati in una stringa di caratteri a caso? Che me ne faccio? Ci tappezzo il cesso?\n\nCalma, non fare il fenomeno, che cazzo hai mangiato a colazione?! L‚Äôhash ha delle propriet√† molto particolari che lo rendono decisamente utile. In ordine (o no?):\n\n√à Deterministico: Se prendi lo stesso dato e lo passi alla stessa ‚Äúmacchinetta‚Äù (funzione di hash), otterrai sempre lo stesso identico hash. Sempre. Non importa quante volte lo fai, il risultato sar√† sempre lo stesso. √à come se la macchinetta avesse una memoria di ferro.\n\nQuindi se faccio l‚Äôhash della parola ‚Äúciao‚Äù ottengo sempre lo stesso hash?\n\nEsatto. Prova. Ci sono un sacco di siti online che fanno hash. Cerca ‚Äúonline hash calculator‚Äù, mettici ‚Äúciao‚Äù e usa l‚Äôalgoritmo SHA-256 (uno dei pi√π comuni). Vedrai che ti uscir√† sempre la stessa stringa di caratteri (tipo b133a0c0e9bee3be20163d2ad31d6248db292aa6dcb1ee087a2aa50e0fc75ae2, ma non ti fissare con questa stringa, √® solo un esempio).\n√à One-Way (A Senso Unico): Questa √® la parte veramente interessante e importante. √à praticamente impossibile (e dico praticamente perch√© nella vita nulla √® impossibile al 100%, tranne forse te che te ne stai a casa il sabato sera invece di andare a far baldoria con Salvini) tornare indietro dall‚Äôhash al dato originale. Cio√®, se hai l‚Äôhash, non puoi in alcun modo (se non per forza bruta) ricostruire il dato di partenza. √à come la macchinetta della carne trita: puoi fare la carne trita dalla carne, ma non puoi rifare il pezzo di carne dalla carne trita. Chiaro no?\n\nS√¨, s√¨, ho capito. Quindi se io faccio l‚Äôhash della mia password ‚Äúpassword123‚Äù, anche se qualcuno ruba l‚Äôhash, non pu√≤ risalire alla mia password?\n\nYES! Finalmente hai detto una roba giusta, porca troia. Ed √® proprio per questo che l‚Äôhashing √® fondamentale per la sicurezza delle password. Quando ti registri su un sito web e scegli una password, il sito non salva la tua password originale (sarebbe da cretini, se lo fanno prendi un martello). Bens√¨, salva solo l‚Äôhash della tua password. Quando poi fai il login e inserisci la tua password, il sito calcola l‚Äôhash della password che hai inserito e lo confronta con l‚Äôhash che ha salvato. Se i due hash coincidono, significa che hai inserito la password giusta, senza che il sito abbia mai dovuto conoscere la tua password in chiaro. Cos√¨ l‚Äôunico che in teoria sa la tua password sei tu (e la tua morosa‚Ä¶).\n√à Resistente alle Collisioni (Idealmente, come sempre): Questa √® un po‚Äô pi√π complicata, ma cercher√≤ di semplificare al massimo. Idealmente (ovvero nei mondi con le fate, le lucine eccetera), √® estremamente improbabile che due dati diversi producano lo stesso hash. Si chiama ‚Äúcollisione‚Äù quando due dati diversi producono lo stesso hash. Una buona funzione di hash √® progettata per rendere le collisioni praticamente impossibili (di nuovo, praticamente). √à come dire che √® quasi impossibile che due pezzi di carne diversi, messi nella stessa macchinetta, producano esattamente la stessa carne trita. Capito no? Pi√π o meno dai‚Ä¶\n\nPi√π o meno‚Ä¶ Ma quindi le collisioni possono succedere?\n\nS√¨, in teoria (e badate, che le cose che possono succedere in teoria anche con probabilit√† infime sono tante. Tipo che morite in un incidente d‚Äôauto, cos√¨, per dire.) possono succedere, ma con le funzioni di hash moderne e robuste, la probabilit√† √® talmente bassa che √® come vincere la lotteria 10 volte di fila mentre ti cade un meteorite in testa e Salvini dice una roba sensata. Insomma, per scopi pratici, possiamo dire che le collisioni sono trascurabili. Esistono per√≤ funzioni di hash pi√π vecchie e meno sicure che sono pi√π vulnerabili alle collisioni, ma c‚Äô√® gi√† troppa carne al fuoco (pun intended).\n\nRicapitolandolo:\nQuindi, ricapitolando, l‚Äôhashing √®:\n\nUna macchinetta per i dati: Prende qualsiasi dato e lo trasforma in una stringa di caratteri di lunghezza fissa (hash).\nDeterministico: Stesso dato, stesso hash, sempre.\nOne-Way: Impossibile (praticamente) tornare indietro dall‚Äôhash al dato originale.\nResistente alle Collisioni: Improbabile (molto improbabile) che due dati diversi producano lo stesso hash."
  },
  {
    "objectID": "blog/hashing.html#ma-a-che-cazzo-serve-veramente-lhashing",
    "href": "blog/hashing.html#ma-a-che-cazzo-serve-veramente-lhashing",
    "title": "Hashing: Che Cos‚Äô√® ‚Äôsto Hashing?",
    "section": "",
    "text": "Ok, abbiamo capito che cos‚Äô√® l‚Äôhashing e le sue propriet√† magiche. Ma in pratica, a cosa serve? Oltre a proteggere le password (che √® gi√† una figata non da poco), l‚Äôhashing ha un sacco di altre applicazioni. Eccone alcune:\n\nVerifica dell‚ÄôIntegrit√† dei Dati: Immagina di scaricare un file enorme da internet, tipo un film in HD o un videogioco. Come fai a essere sicuro che il file che hai scaricato sia integro e non sia stato corrotto durante il download? Semplice: il sito web da cui hai scaricato il file di solito fornisce anche l‚Äôhash del file. Tu, dopo aver scaricato il file, puoi calcolare l‚Äôhash del file scaricato e confrontarlo con l‚Äôhash fornito dal sito. Se i due hash coincidono, significa che il file √® integro al 100%. Se non coincidono, significa che c‚Äô√® stato un problema durante il download e il file √® corrotto (o peggio, potrebbe essere stato manomesso da qualcuno). √à come avere un sigillo di garanzia digitale sul file. Figata, no?\nBlockchain e Criptovalute: Se hai letto l‚Äôarticolo sulla blockchain (e se non l‚Äôhai fatto, vergognati, tanto.), sai che l‚Äôhashing √® un ingrediente fondamentale della blockchain. Viene usato per creare l‚Äô‚Äúimpronta digitale‚Äù di ogni blocco e per collegare i blocchi in una catena immutabile. Senza l‚Äôhashing, la blockchain non esisterebbe. Quindi, se ti piacciono le criptovalute (o se le odi, non importa), sappi che devi ringraziare (o maledire) l‚Äôhashing.\nFirme Digitali: L‚Äôhashing √® anche usato nelle firme digitali per garantire l‚Äôautenticit√† e l‚Äôintegrit√† dei documenti digitali. Quando firmi digitalmente un documento, in realt√† stai firmando l‚Äôhash del documento, non il documento intero. Questo rende la firma molto pi√π efficiente e sicura.\nRicerca Efficiente di Dati: L‚Äôhashing pu√≤ essere usato per creare delle ‚Äútabelle hash‚Äù (o ‚Äúhash map‚Äù), che sono delle strutture dati che permettono di cercare dati in modo molto veloce. √à un po‚Äô come avere un indice di un libro: invece di dover sfogliare tutto il libro per trovare un‚Äôinformazione, puoi consultare l‚Äôindice e trovarla subito."
  },
  {
    "objectID": "blog/hashing.html#non-√®-tutto-rose-e-fiori-o-hash-e-fiori-in-questo-caso",
    "href": "blog/hashing.html#non-√®-tutto-rose-e-fiori-o-hash-e-fiori-in-questo-caso",
    "title": "Hashing: Che Cos‚Äô√® ‚Äôsto Hashing?",
    "section": "",
    "text": "Anche l‚Äôhashing, come tutte le cose belle della vita, tipo me, ha i suoi limiti:\n\nCollisioni (anche se improbabili): Abbiamo detto che le collisioni sono molto improbabili, ma non impossibili. In teoria, un attaccante potrebbe cercare di trovare due dati diversi che producono lo stesso hash (un ‚Äúattacco di collisione‚Äù). Se ci riuscisse, potrebbe usare questa collisione per scopi malevoli, tipo sostituire un file legittimo con uno fasullo che ha lo stesso hash. Per fortuna, trovare collisioni per le funzioni di hash moderne e robuste √® estremamente difficile e costoso, ma √® un rischio teorico da tenere presente.\nAttacchi di ‚ÄúRainbow Table‚Äù e ‚ÄúBrute Force‚Äù alle Password: Anche se l‚Äôhashing rende difficile risalire alla password originale dall‚Äôhash, non √® una protezione infallibile al 100%. Esistono tecniche come gli attacchi di ‚Äúrainbow table‚Äù e ‚Äúbrute force‚Äù che possono essere usate per cercare di ‚Äúindovinare‚Äù le password a partire dagli hash. Le ‚Äúrainbow table‚Äù sono delle tabelle precalcolate che contengono gli hash di milioni di password comuni. Gli attacchi ‚Äúbrute force‚Äù invece consistono nel provare a calcolare l‚Äôhash di tutte le possibili password finch√© non si trova una corrispondenza con l‚Äôhash rubato. Per difendersi da questi attacchi, √® fondamentale usare password lunghe, complesse e diverse per ogni sito. E smettetela di usare ‚Äúpassword123‚Äù, che cazzo!\nNon √® Crittografia: √à importante capire che l‚Äôhashing non √® crittografia. La crittografia serve per nascondere i dati, rendendoli illeggibili a chi non ha la chiave giusta. L‚Äôhashing serve per creare un‚Äôimpronta digitale dei dati, per verificarne l‚Äôintegrit√† e l‚Äôautenticit√†. E‚Äô importante non confondere le due cose. √à come confondere un lucchetto (crittografia) con un sigillo di ceralacca (hashing). Entrambi servono per la sicurezza, ma in modi diversi."
  },
  {
    "objectID": "blog/hashing.html#quindi",
    "href": "blog/hashing.html#quindi",
    "title": "Hashing: Che Cos‚Äô√® ‚Äôsto Hashing?",
    "section": "",
    "text": "L‚Äôhashing √® una tecnologia che sta alla base nel mondo digitale di oggi e di un sacco di cose che usiamo tutti i giorni, dalle password alla blockchain, dalla verifica dell‚Äôintegrit√† dei file alle firme digitali. √à una tecnologia potente e versatile, ma √® importante conoscerne anche i limiti e le potenziali vulnerabilit√†.\nLa prossima volta che sentirai parlare di hashing pensa alla macchinetta per la carne trita dei dati, alla one-way, e alla sicurezza che pu√≤ garantire. E soprattutto, smettila di usare password di merda! Grazie per l‚Äôattenzione e alla prossima settimana con un altro argomento nerd e (spero) interessante."
  },
  {
    "objectID": "blog/hashing.html#post-scriptum-hash-di-sto-articolo",
    "href": "blog/hashing.html#post-scriptum-hash-di-sto-articolo",
    "title": "Hashing: Che Cos‚Äô√® ‚Äôsto Hashing?",
    "section": "",
    "text": "Giusto per fare i fighi e per farti capire ancora meglio la potenza dell‚Äôhashing, ho calcolato l‚Äôhash SHA-256 di questo articolo (in formato testo semplice, senza HTML o altre formattazioni). Ecco l‚Äôhash:\n31e452bbf5134ac1d0d0a3a5897359b83eaccf6a46aed7d4323ddb51ec2d76f0\nOra, se io modificassi anche solo una virgola di questo articolo, anche solo uno spazio bianco, anche solo una lettera, l‚Äôhash cambierebbe completamente. Questo dimostra quanto anche una piccola modifica al dato originale cambi radicalmente l‚Äôhash. √à proprio questa propriet√† che rende l‚Äôhashing cos√¨ utile per la verifica dell‚Äôintegrit√† dei dati. E anche per farmi sentire un grande (imbecille) a scrivere un post scriptum del genere. Ciao stelline!"
  },
  {
    "objectID": "blog/sss.html",
    "href": "blog/sss.html",
    "title": "Shamir‚Äôs Secret Sharing",
    "section": "",
    "text": "Shamir‚Äôs Secret Sharing\nIn questo articolo mi voglio concentrare su una spiegazione (speriamo) dettagliata dell‚Äôalgoritmo Shamir‚Äôs Secret Sharing, provando ad illustrare sia la parte pi√π teorica che quella un po‚Äô pi√π pratica con degli esempi concreti. Allacciamo le cinture.\nFirst things first‚Ä¶\nLo Shamir‚Äôs Secret Sharing √® innanzitutto un algoritmo di secret sharing crittografico ideato da Adi Shamir nel 1979. Permette di dividere un segreto \\(S\\) (poi ci arriviamo a che cazpita si intende con un segreto) in \\(n\\) parti, chiamate shares (parti?), in maniera tale che, per ricostruire il segreto (aridajela) originale, sia necessario un numero minimo \\(k\\) (con \\(k \\leq n\\)) di queste shares. Questo schema √® anche conosciuto come threshold scheme \\((k, n)\\).\nQuindi:\n\nUn segreto viene diviso in \\(n\\) shares.\nAlmeno \\(k\\) shares sono necessarie per ricostruire l‚Äôintero segreto (magia).\nCon meno di \\(k\\) shares, il segreto non pu√≤ essere ricostruito (volevi eh!?), e non si ottiene alcuna informazione su di esso (propriet√† di information-theoretically secure).\n\nA che serve:\n\nKey Management: Distribuire una master key crittografica tra pi√π attori, in modo che un numero sufficiente di essi debba cooperare per utilizzarla.\nAccess Control: Dividere una secret key per l‚Äôaccesso a un sistema o a dei dati tra pi√π utenti.\nDistributed Storage: Distribuire i frammenti di un file criptato su pi√π server, in modo che un certo numero di server debba essere accessibile per decriptare il file.\nSecure Multi-Party Computation (MPC): Come building block per protocolli pi√π complessi.\nDividere i vocali della chat coi bro: Cos√¨ andiamo tutti in galera, non solo io.\n\nIn sostanza: un segreto √® qualcosa che vogliamo proteggere. Per esempio pu√≤ essere una chiave privata (una password dai‚Ä¶ senza fare tanto il fenomeno). Per√≤ vogliamo che non risieda in un unico punto bens√¨ che risieda in tanti luoghi fisici diversi. Per esempio un pezzo lo nascondo a casa mia, un pezzo dal salumiere, un pezzo dal gommista eccetera eccetera eccetera.\nA sto punto per√≤ prima o poi la password mi serve e visto che io ho la memoria di un pesce rosso devo andare dal salumiere. Ma lo trovo chiuso. E adesso? sono fottuto. Mi serve la mia password per comprare i filmini con le donne nude, come faccio?\nEh come faccio‚Ä¶ Con lo schema di Shamir‚Äôs non mi serve avere accesso a tutte le share, bens√¨ solo a un gruppo di esse. Ed √® qui appunto la magia.\nCome funziona?\nA livello puramente matematico, l‚Äôalgoritmo si basa sull‚Äôinterpolation polinomiale. L‚Äôidea chiave √® che, dati \\(k\\) punti in un piano, con ascisse distinte, esiste uno e un solo polinomio di grado al pi√π \\(k-1\\) che passa per tutti questi punti. Nel nostro caso, il grado del polinomio sar√† esattamente \\(k-1\\).\nEsempio con polinomio di grado 2 (k=3):\nPer ricostruire un polinomio di grado 2 (una parabola), sono necessari \\(k=3\\) punti. Qui sotto puoi interagire con un grafico che mostra questo concetto. Muovi i punti e osserva come cambia la parabola. Il termine noto del polinomio (il punto in cui la parabola interseca l‚Äôasse y) rappresenta il segreto.\n\n\nCode\nviewof x1 = Inputs.range([-4, 4], {step: 0.5, value: -2, label: \"x1\"})\nviewof y1 = Inputs.range([-5, 5], {step: 0.5, value: 4, label: \"y1\"})\nviewof x2 = Inputs.range([-4, 4], {step: 0.5, value: 2, label: \"x2\"})\nviewof y2 = Inputs.range([-5, 5], {step: 0.5, value: -4, label: \"y2\"})\nviewof x3 = Inputs.range([-4, 4], {step: 0.5, value: 3, label: \"x3\"})\nviewof y3 = Inputs.range([-5, 5], {step: 0.5, value: 4, label: \"y3\"})\n\n// Calculate coefficients function - simplified version using Cramer's rule\n// for the specific case of quadratic regression through 3 points\nfunction calculateCoeffs(x1, y1, x2, y2, x3, y3) {\n  // Matrix A entries for system Ax = b where x are the coefficients\n  const x1_2 = x1 * x1, x2_2 = x2 * x2, x3_2 = x3 * x3;\n  \n  // Determinant of matrix A\n  const detA = x1_2 * (x2 - x3) - x1 * (x2_2 - x3_2) + (x2_2 * x3 - x3_2 * x2);\n  \n  // Using Cramer's rule to solve the system\n  const a = (y1 * (x2 - x3) - x1 * (y2 - y3) + (y2 * x3 - y3 * x2)) / detA;\n  \n  const b = (x1_2 * (y2 - y3) - y1 * (x2_2 - x3_2) + (x2_2 * y3 - x3_2 * y2)) / detA;\n  \n  const c = (x1_2 * (x2 * y3 - x3 * y2) - x1 * (x2_2 * y3 - x3_2 * y2) + y1 * (x2_2 * x3 - x3_2 * x2)) / detA;\n  \n  return [a, b, c];\n}\n\n// Create the plot\n{\n  const width = 800;\n  const height = 600;\n  const margin = {top: 60, right: 40, bottom: 60, left: 60};\n  \n  const svg = d3.create(\"svg\")\n    .attr(\"width\", width)\n    .attr(\"height\", height)\n    .attr(\"viewBox\", [0, 0, width, height])\n    .attr(\"style\", \"max-width: 100%; height: auto;\");\n  \n  const g = svg.append(\"g\")\n    .attr(\"transform\", `translate(${margin.left},${margin.top})`);\n  \n  // Set up scales\n  const xScale = d3.scaleLinear()\n    .domain([-10, 10])\n    .range([0, width - margin.left - margin.right]);\n  \n  const yScale = d3.scaleLinear()\n    .domain([-10, 10])\n    .range([height - margin.top - margin.bottom, 0]);\n  \n  // Add axes\n  g.append(\"g\")\n    .attr(\"transform\", `translate(0,${(height - margin.top - margin.bottom)/2})`)\n    .call(d3.axisBottom(xScale))\n    .attr(\"class\", \"axis\");\n  \n  g.append(\"g\")\n    .attr(\"transform\", `translate(${(width - margin.left - margin.right)/2},0)`)\n    .call(d3.axisLeft(yScale))\n    .attr(\"class\", \"axis\");\n  \n  // Add grid\n  g.append(\"g\")\n    .attr(\"class\", \"grid\")\n    .selectAll(\"line\")\n    .data(d3.range(-10, 10))\n    .join(\"line\")\n    .attr(\"x1\", d =&gt; xScale(d))\n    .attr(\"x2\", d =&gt; xScale(d))\n    .attr(\"y1\", 0)\n    .attr(\"y2\", height - margin.top - margin.bottom)\n    .attr(\"stroke\", \"#ddd\")\n    .attr(\"stroke-width\", 0.5);\n\n  g.append(\"g\")\n    .attr(\"class\", \"grid\")\n    .selectAll(\"line\")\n    .data(d3.range(-10, 10))\n    .join(\"line\")\n    .attr(\"y1\", d =&gt; yScale(d))\n    .attr(\"y2\", d =&gt; yScale(d))\n    .attr(\"x1\", 0)\n    .attr(\"x2\", width - margin.left - margin.right)\n    .attr(\"stroke\", \"#ddd\")\n    .attr(\"stroke-width\", 0.5);\n  \n  // Calculate parabola points\n  const coeffs = calculateCoeffs(x1, y1, x2, y2, x3, y3);\n  const xValues = d3.range(-5, 5.1, 0.1);\n  const parabola = xValues.map(x =&gt; ({\n    x: x,\n    y: coeffs[0] * x * x + coeffs[1] * x + coeffs[2]\n  }));\n  \n  // Draw parabola\n  const line = d3.line()\n    .x(d =&gt; xScale(d.x))\n    .y(d =&gt; yScale(d.y))\n    .curve(d3.curveNatural);\n  \n  g.append(\"path\")\n    .datum(parabola)\n    .attr(\"fill\", \"none\")\n    .attr(\"stroke\", \"blue\")\n    .attr(\"stroke-width\", 2)\n    .attr(\"d\", line);\n  \n  // Add points\n  const points = [\n    {x: x1, y: y1},\n    {x: x2, y: y2},\n    {x: x3, y: y3}\n  ];\n  \n  g.selectAll(\"circle\")\n    .data(points)\n    .join(\"circle\")\n    .attr(\"cx\", d =&gt; xScale(d.x))\n    .attr(\"cy\", d =&gt; yScale(d.y))\n    .attr(\"r\", 8)\n    .attr(\"fill\", \"red\");\n  \n  // Add equation and secret value\n  const equation = `y = ${coeffs[0].toFixed(2)}x¬≤ + ${coeffs[1].toFixed(2)}x + ${coeffs[2].toFixed(2)}`;\n  g.append(\"text\")\n    .attr(\"x\", (width - margin.left - margin.right) / 2)\n    .attr(\"y\", -10)\n    .attr(\"text-anchor\", \"middle\")\n    .text(equation);\n  \n  g.append(\"text\")\n    .attr(\"x\", (width - margin.left - margin.right) / 2)\n    .attr(\"y\", -30)\n    .attr(\"text-anchor\", \"middle\")\n    .text(`Secret: ${coeffs[2].toFixed(2)}`);\n  \n  return svg.node();\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFasi dell‚Äôalgoritmo:\n\nSetup:\n\nSia \\(S\\) il segreto da condividere, rappresentato come un numero intero.\nSia \\(n\\) il numero di shares da generare.\nSia \\(k\\) il threshold, ovvero il numero minimo di shares necessarie per ricostruire il segreto.\nScegli un numero primo \\(p\\) maggiore di \\(S\\) e di \\(n\\). Tutte le operazioni aritmetiche saranno eseguite modulo \\(p\\) (in \\(\\mathbb{Z}_p\\)).\n\nShare Generation:\n\nScegli casualmente \\(k-1\\) coefficienti \\(a_1, a_2, \\dots, a_{k-1}\\), dove ogni \\(a_i\\) √® un numero intero in \\(\\mathbb{Z}_p\\).\nCostruisci il polinomio \\(P(x)\\) di grado \\(k-1\\):\n\n\\[P(x) = S + a_1x + a_2x^2 + \\dots + a_{k-1}x^{k-1}\\]\n\nNotare che il termine noto del polinomio √® il segreto \\(S\\) (\\(P(0) = S\\)).\nGenera \\(n\\) shares calcolando il valore del polinomio in \\(n\\) punti distinti non nulli. Ad esempio, si possono usare i punti \\(x = 1, 2, \\dots, n\\).\nLa share \\(i\\)-esima √® la coppia \\((x_i, y_i)\\), dove \\(x_i = i\\) e \\(y_i = P(i)\\).\n\nSecret Reconstruction:\n\nPer ricostruire il segreto, sono necessarie almeno \\(k\\) shares \\((x_i, y_i)\\).\nUtilizza l‚Äôinterpolation di Lagrange per ricostruire il polinomio \\(P(x)\\) a partire dai \\(k\\) punti. La formula di Lagrange √®:\n\n\\[P(x) = \\sum_{i=1}^{k} y_i \\prod_{j=1, j \\neq i}^{k} \\frac{x - x_j}{x_i - x_j}\\]\n\nUna volta ricostruito il polinomio \\(P(x)\\), il segreto \\(S\\) pu√≤ essere ottenuto valutando il polinomio in \\(x=0\\):\n\n\\[S = P(0)\\]\n\nEsempio (semplificato):\n\nSegreto: \\(S = 1234\\)\nNumero di shares: \\(n = 5\\)\nThreshold: \\(k = 3\\)\nNumero primo: \\(p = 1613\\) (maggiore di \\(S\\) e \\(n\\))\n\nShare Generation:\n\nScegliamo casualmente \\(k-1 = 2\\) coefficienti: \\(a_1 = 166\\), \\(a_2 = 94\\).\nIl polinomio √®: \\(P(x) = 1234 + 166x + 94x^2\\).\nGeneriamo 5 shares:\n\n\\(P(1) = 1234 + 166 \\cdot 1 + 94 \\cdot 1^2 = 1494 \\pmod{1613}\\) -&gt; \\((1, 1494)\\)\n\\(P(2) = 1234 + 166 \\cdot 2 + 94 \\cdot 2^2 = 1942 \\equiv 329 \\pmod{1613}\\) -&gt; \\((2, 329)\\)\n\\(P(3) = 1234 + 166 \\cdot 3 + 94 \\cdot 3^2 = 2578 \\equiv 965 \\pmod{1613}\\) -&gt; \\((3, 965)\\)\n\\(P(4) = 1234 + 166 \\cdot 4 + 94 \\cdot 4^2 = 3402 \\equiv 176 \\pmod{1613}\\) -&gt; \\((4, 176)\\)\n\\(P(5) = 1234 + 166 \\cdot 5 + 94 \\cdot 5^2 = 4414 \\equiv 1188 \\pmod{1613}\\) -&gt; \\((5, 1188)\\)\n\n\nSecret Reconstruction:\nSupponiamo di avere le shares: \\((1, 1494)\\), \\((3, 965)\\), \\((5, 1188)\\).\n\nUsiamo l‚Äôinterpolation di Lagrange per ricostruire il polinomio.\nDopo aver eseguito i calcoli (omessi per brevit√†), otteniamo \\(P(x) = 1234 + 166x + 94x^2\\).\nIl segreto √® \\(S = P(0) = 1234\\).\n\nPropriet√† di sicurezza:\n\nInformation-Theoretically Secure: Con meno di \\(k\\) shares, non si ottiene alcuna informazione sul segreto. Questo perch√© qualsiasi valore del segreto √® ugualmente probabile, dato un numero insufficiente di punti per determinare univocamente il polinomio.\nPerfect Secret Sharing: Ogni share √® grande quanto il segreto originale.\n\nLimitazioni:\n\nShare Size: Ogni share ha la stessa dimensione del segreto. Questo pu√≤ essere problematico se il segreto √® molto grande, per esempio un file.\nDealer Trust: Il dealer (chi genera le shares) conosce il segreto e deve essere fidato. E dunque potenzialmente rende questo inusabile in contesti come blockchain dove si potrebbe volere uno schema meno lasco dove la chiave privata (il segreto) non viene mai veramente materializzata completamente.\nStatic Threshold: Il valore di \\(k\\) (il threshold) √® fissato al momento della generazione delle shares. Ovvero se aumentano o diminuiscono i partecipanti bisogna rifare tutto sto giro da capo.\n\nConclusioni:\nWow. Che figata, loso. Lo Shamir‚Äôs Secret Sharing √® alla fine un algoritmo decisamente potent per la condivisione di segreti o chiavi private. Nonostante alcune limitazioni, rimane uno schema fondamentale nel campo della crittografia e della sicurezza informatica."
  },
  {
    "objectID": "projects/prese-rnn.html#recurrent-neural-networks",
    "href": "projects/prese-rnn.html#recurrent-neural-networks",
    "title": "Of RNNs: Ising, Hopfield, Math and Modernity",
    "section": "Recurrent Neural Networks",
    "text": "Recurrent Neural Networks"
  },
  {
    "objectID": "projects/prese-rnn.html#physics-foundations",
    "href": "projects/prese-rnn.html#physics-foundations",
    "title": "Of RNNs: Ising, Hopfield, Math and Modernity",
    "section": "Physics Foundations",
    "text": "Physics Foundations\n\n1925: The Ising Model laid groundwork for understanding dynamic systems.\n\nConcept: A mathematical model of ferromagnetism in statistical mechanics.\nRelevance: Introduced ideas of interacting components and system states evolving over time, conceptually related to dynamic systems later explored in RNNs."
  },
  {
    "objectID": "projects/prese-rnn.html#hopfield-networks",
    "href": "projects/prese-rnn.html#hopfield-networks",
    "title": "Of RNNs: Ising, Hopfield, Math and Modernity",
    "section": "Hopfield Networks",
    "text": "Hopfield Networks\n\n1982: Hopfield Networks introduced associative memory structures.\n\nConcept: A type of recurrent neural network that serves as a content-addressable ‚Äúassociative memory‚Äù system.\nRelevance: Demonstrated the potential of recurrent connections for memory and pattern completion, a precursor to RNNs for sequential data."
  },
  {
    "objectID": "projects/prese-rnn.html#formalizing-recurrent-networks-and-training",
    "href": "projects/prese-rnn.html#formalizing-recurrent-networks-and-training",
    "title": "Of RNNs: Ising, Hopfield, Math and Modernity",
    "section": "Formalizing Recurrent Networks and Training",
    "text": "Formalizing Recurrent Networks and Training\n\n1986: Recurrent Neural Networks (RNNs) were formalized.\n\nConcept: Neural networks with loops, allowing them to process sequences of inputs by maintaining a hidden state that carries information across time steps.\nRelevance: The birth of the RNN as we know it, designed to handle sequential data."
  },
  {
    "objectID": "projects/prese-rnn.html#backpropation-through-time",
    "href": "projects/prese-rnn.html#backpropation-through-time",
    "title": "Of RNNs: Ising, Hopfield, Math and Modernity",
    "section": "Backpropation Through Time",
    "text": "Backpropation Through Time\n\n1986: Backpropagation Through Time (BPTT) was developed for training RNNs.\n\nConcept: An adaptation of backpropagation algorithm to train RNNs by unfolding the network through time and then applying standard backpropagation.\nRelevance: Provided a practical method to train RNNs, enabling them to learn from sequential data."
  },
  {
    "objectID": "projects/prese-rnn.html#addressing-the-vanishing-gradient",
    "href": "projects/prese-rnn.html#addressing-the-vanishing-gradient",
    "title": "Of RNNs: Ising, Hopfield, Math and Modernity",
    "section": "Addressing the Vanishing Gradient",
    "text": "Addressing the Vanishing Gradient\n\n1997: LSTM networks addressed the vanishing gradient problem.\n\nConcept: Long Short-Term Memory networks, a special type of RNN with memory cells and gates that regulate information flow, designed to mitigate the vanishing gradient problem.\nRelevance: A major breakthrough that allowed RNNs to learn long-range dependencies in sequences, significantly improving performance on tasks like NLP."
  },
  {
    "objectID": "projects/prese-rnn.html#simplifying-and-maintaining-performance",
    "href": "projects/prese-rnn.html#simplifying-and-maintaining-performance",
    "title": "Of RNNs: Ising, Hopfield, Math and Modernity",
    "section": "Simplifying and Maintaining Performance",
    "text": "Simplifying and Maintaining Performance\n\n2014: GRUs simplified RNN structures while maintaining performance.\n\nConcept: Gated Recurrent Units, a simplified version of LSTMs with fewer gates, offering comparable performance with fewer parameters.\nRelevance: Provided a more efficient alternative to LSTMs in many cases, making RNNs more accessible and faster to train."
  },
  {
    "objectID": "projects/prese-rnn.html#modern-advancements",
    "href": "projects/prese-rnn.html#modern-advancements",
    "title": "Of RNNs: Ising, Hopfield, Math and Modernity",
    "section": "Modern Advancements",
    "text": "Modern Advancements\n\n2020: Modern LSTMs evolved for advanced applications like NLP and forecasting.\n\nConcept: Continued research and development on LSTMs and related architectures, incorporating attention mechanisms, transformers, and other advancements for state-of-the-art performance.\nRelevance: Highlights the ongoing evolution of RNNs and their continued relevance in cutting-edge applications, especially in natural language processing and time series forecasting."
  },
  {
    "objectID": "projects/prese-rnn.html#backpropagation-the-math",
    "href": "projects/prese-rnn.html#backpropagation-the-math",
    "title": "Of RNNs: Ising, Hopfield, Math and Modernity",
    "section": "Backpropagation: The Math",
    "text": "Backpropagation: The Math\nTo understand BPTT works, let‚Äôs derive its mathematical formulation."
  },
  {
    "objectID": "projects/prese-rnn.html#unfolding-the-rnn",
    "href": "projects/prese-rnn.html#unfolding-the-rnn",
    "title": "Of RNNs: Ising, Hopfield, Math and Modernity",
    "section": "Unfolding the RNN",
    "text": "Unfolding the RNN\n\nConsider an RNN that processes a sequence of inputs \\[( x_1, x_2, \\ldots, x_T )\\] At each time step \\(t\\), the RNN maintains a hidden state \\(h_t\\) which is updated based on the current input \\(x_t\\) and the previous hidden state \\(h_{t-1}\\):\n\\[h_t = f(W_h h_{t-1} + W_x x_t + b)\\]\nwhere \\(W_h\\) and \\(W_x\\) are weight matrices, \\(b\\) is a bias vector, and \\(f\\) is an activation function (typically tanh or ReLU)."
  },
  {
    "objectID": "projects/prese-rnn.html#loss-function",
    "href": "projects/prese-rnn.html#loss-function",
    "title": "Of RNNs: Ising, Hopfield, Math and Modernity",
    "section": "Loss Function",
    "text": "Loss Function\nAssume we have a loss function \\(L\\) that depends on the outputs of the RNN at each time step. The total loss over the sequence is:\n\\[L = \\sum_{t=1}^T L_t(y_t, \\hat{y}_t)\\]\nwhere \\(y_t\\) is the true output and \\(\\hat{y}_t\\) is the predicted output at time step \\(t\\)."
  },
  {
    "objectID": "projects/prese-rnn.html#backpropagation-through-time",
    "href": "projects/prese-rnn.html#backpropagation-through-time",
    "title": "Of RNNs: Ising, Hopfield, Math and Modernity",
    "section": "Backpropagation Through Time",
    "text": "Backpropagation Through Time\nTo train the RNN, we need to compute the gradients of the loss with respect to the weights \\(W_h\\) and \\(W_x\\). BPTT involves unfolding the RNN through time and applying the chain rule of calculus to compute these gradients.\n\nForward Pass: Compute the hidden states \\(h_t\\) and the outputs \\(\\hat{y}_t\\) for \\(t = 1, 2, \\ldots, T\\).\nBackward Pass: Compute the gradients of the loss with respect to the hidden states and weights by propagating the error backwards through time.\nThe gradient of the loss with respect to the hidden state at time step \\(t\\) is:\n\\[\\frac{\\partial L}{\\partial h_t} = \\sum_{k=t}^T \\frac{\\partial L_k}{\\partial h_t}\\]\nUsing the chain rule, we can express this as:\n\\[\\frac{\\partial L_k}{\\partial h_t} = \\frac{\\partial L_k}{\\partial \\hat{y}_k} \\frac{\\partial \\hat{y}_k}{\\partial h_k} \\frac{\\partial h_k}{\\partial h_t}\\]\nThe gradient of the hidden state \\(h_k\\) with respect to \\(h_t\\) involves the recurrent connection:\n\\[\\frac{\\partial h_k}{\\partial h_t} = \\prod_{j=t+1}^k \\frac{\\partial h_j}{\\partial h_{j-1}}\\]\nFinally, the gradients of the loss with respect to the weights are computed by summing the contributions from each time step:\n\\[\\frac{\\partial L}{\\partial W_h} = \\sum_{t=1}^T \\frac{\\partial L}{\\partial h_t} \\frac{\\partial h_t}{\\partial W_h}\\]\n\\[\\frac{\\partial L}{\\partial W_x} = \\sum_{t=1}^T \\frac{\\partial L}{\\partial h_t} \\frac{\\partial h_t}{\\partial W_x}\\]"
  },
  {
    "objectID": "projects/prese-rnn.html#a-plot",
    "href": "projects/prese-rnn.html#a-plot",
    "title": "Of RNNs: Ising, Hopfield, Math and Modernity",
    "section": "A Plot",
    "text": "A Plot\n\nsunburst = {\n  const root = partition(flareData);\n  root.each(d =&gt; d.current = d);\n  const svg = d3.create(\"svg\")\n      .attr(\"viewBox\", [0, 0, width, width])\n      .style(\"font\", \"15px sans-serif\");\n\n  // ...remainder of implementation\n  \n  return svg.node();\n}"
  },
  {
    "objectID": "projects/connect4.html",
    "href": "projects/connect4.html",
    "title": "Connect 4",
    "section": "",
    "text": "Voglio applicare il metodo DPO a una rete RNN. Creer√≤ una struttura composta da tre elementi che interagiscono tra loro:\n\nuna RNN che ha appreso solo le regole del gioco (banalmente sequenze di partite random)\nuna rete che apprende tramite DPO a discernere tra una mossa giusta e una sbagliata\nuna rete che apprende ad apprendere, che adatta i parametri della rete 1. in base alla situazione/stato corrente."
  },
  {
    "objectID": "projects/connect4.html#february-16th-2025",
    "href": "projects/connect4.html#february-16th-2025",
    "title": "Connect 4",
    "section": "February 16th, 2025",
    "text": "February 16th, 2025\nProgress: Successfully experimented with GRUs on the ‚Äúadding problem‚Äù to understand RNN learning dynamics. Demonstrated that normalizing input/output to [0,1] significantly improves performance. Showed the model (with a hidden size of one) can learn to sum a variable number of masked inputs, even with variable sequence lengths (up to +/- 80% variation). This highlights the surprising capability of even tiny RNNs.\nTechnical Notes: - GRUs were used for all experiments. - Input/output normalization to [0,1] is essential for learning. - The model architecture is extremely small: a single-layer GRU with hidden size 1, followed by a linear layer. - The model successfully learned with a variable number of masked elements and variable sequence length. - The sequence length has little to no effect on the ability to learn.\nNext Steps: - Begin investigating the incorporation of the DPO loss. - Define the structure of the meta-learning network. Consider the size constraint (meta-network should be smaller than the main network?). - Start thinking about how to design signals for ‚Äúflag‚Äù and ‚Äúvalue‚Äù to sum. - Explore whether an Energy-Based Model (EBM) is suitable for the meta-learning component (though stability could a concern)."
  },
  {
    "objectID": "projects/spatial-spike-neural-networks-it.html",
    "href": "projects/spatial-spike-neural-networks-it.html",
    "title": "Spatial Spike Neural Networks",
    "section": "",
    "text": "Voglio provare a studiare un tipo di rete neurale di cui non ho trovato citazioni in ambito di ricerca. Di base Una rete neurale spiking (SNN) √® un tipo di Neural Network che si avvicina maggiormente al funzionamento dei neuroni biologici. A differenza delle reti neurali tradizionali che trasmettono valori continui (ovvero fanno le moltiplicazioni direttamente), le SNN elaborano le informazioni attraverso ‚Äúspike‚Äù, ovvero eventi discreti nel tempo, rendendole computazionalmente pi√π efficienti. Queste reti codificano le informazioni nella temporizzazione e nella frequenza degli spike, consentendo loro di catturare dinamiche temporali complesse e potenzialmente imitare la capacit√† di apprendimento del cervello.\nL‚Äôelemento che voglio introdurre √® quello di strutture (basate su grafi) che includano all‚Äôinterno della rete anche il concetto di vicinanza dei neuroni della rete. Voglio inoltre provare a comprendere se i neuroni possono spostarsi all‚Äôinterno della struttura a grafo.\nIn altre parole: do una struttura alla rete, come ad esempio delle strutture simili a quelle cristalline, e posiziono i neuroni. Man mano che apprendono pattern all‚Äôinterno dei dati, li faccio muovere seguendo la struttura data, seguendo l‚Äôapprendimento Hebbiano\n\n\n\n\nüáÆüáπ Mentre mi preparavo la camomilla con la melatonina ieri sera ho visualizzato per un istante la rete che ho in mente come una specie di struttura che si adatta all‚Äôinput che riceve. Pu√≤ essere realizzata una roba del genere? Pu√≤ l‚Äôinput guidare il movimento dei neuroni? Devo ancora capire bene cosa ho visto, ma quello che forse voglio realizzare √® un grafo con i neuroni posizionati dove devono stare. Quando ricevono un input (che potrebbe essere un vettore latente) si riorganizzano seguendo i percorsi obbligati dalla struttura per far s√¨ che neuroni che devono stare vicini restino vicini. Nulla vieta che i ‚Äúneuroni‚Äù possano in realt√† essere intere reti deep.\nüá¨üáß While I was preparing my chamomile with melatonin last night, I briefly visualized the network I have in mind as a kind of structure that adapts to the input it receives. Can such a thing be realized? Can the input guide the movement of the neurons? I still need to fully understand what I saw, but what I might want to create is a graph with neurons positioned where they need to be. When they receive an input (which could be a latent vector), they rearrange following the paths dictated by the structure to ensure that neurons that need to stay close remain close. Nothing prevents the ‚Äúneurons‚Äù from actually being entire deep networks.\n\n\n\nüáÆüáπ a colazione ho mangiato latte e cereali. Ho scaldato il latte nella pentolino, versato sui cereali caldo. Mangiato. Finto, metto nel lavandino la pentola e la ciotola dentro la pentola e butto un po‚Äô d‚Äôacqua cos√¨, come mi ha insegnato la mamma, √® pi√π facile lavare. Se metto la ciotola dentro la pentola, all‚Äôinterno della pentola l‚Äôacqua raggiunge prima il bordo superiore perch√© c‚Äô√® la ciotola a fare volume, cos√¨ posso usare meno acqua. Ovvero: avere qualcosa che riempe il volume permette di usare meno risorse. Pu√≤ essere utile? boh.\nüá¨üáß for breakfast I had milk and cereal. I heated the milk in the saucepan, poured it hot over the cereal. Ate. Finished, I put the saucepan in the sink and the bowl inside the saucepan and I put a bit of water in so that, like my mum taught me, it‚Äôs easier to wash up. If I put the bowl inside the saucepan, inside the saucepan the water reaches the top edge sooner because the bowl is taking up volume, so I can use less water. Basically: having something that fills the volume means you can use fewer resources. Could that be useful? Dunno."
  },
  {
    "objectID": "projects/spatial-spike-neural-networks-it.html#log",
    "href": "projects/spatial-spike-neural-networks-it.html#log",
    "title": "Spatial Spike Neural Networks",
    "section": "",
    "text": "üáÆüáπ Mentre mi preparavo la camomilla con la melatonina ieri sera ho visualizzato per un istante la rete che ho in mente come una specie di struttura che si adatta all‚Äôinput che riceve. Pu√≤ essere realizzata una roba del genere? Pu√≤ l‚Äôinput guidare il movimento dei neuroni? Devo ancora capire bene cosa ho visto, ma quello che forse voglio realizzare √® un grafo con i neuroni posizionati dove devono stare. Quando ricevono un input (che potrebbe essere un vettore latente) si riorganizzano seguendo i percorsi obbligati dalla struttura per far s√¨ che neuroni che devono stare vicini restino vicini. Nulla vieta che i ‚Äúneuroni‚Äù possano in realt√† essere intere reti deep.\nüá¨üáß While I was preparing my chamomile with melatonin last night, I briefly visualized the network I have in mind as a kind of structure that adapts to the input it receives. Can such a thing be realized? Can the input guide the movement of the neurons? I still need to fully understand what I saw, but what I might want to create is a graph with neurons positioned where they need to be. When they receive an input (which could be a latent vector), they rearrange following the paths dictated by the structure to ensure that neurons that need to stay close remain close. Nothing prevents the ‚Äúneurons‚Äù from actually being entire deep networks.\n\n\n\nüáÆüáπ a colazione ho mangiato latte e cereali. Ho scaldato il latte nella pentolino, versato sui cereali caldo. Mangiato. Finto, metto nel lavandino la pentola e la ciotola dentro la pentola e butto un po‚Äô d‚Äôacqua cos√¨, come mi ha insegnato la mamma, √® pi√π facile lavare. Se metto la ciotola dentro la pentola, all‚Äôinterno della pentola l‚Äôacqua raggiunge prima il bordo superiore perch√© c‚Äô√® la ciotola a fare volume, cos√¨ posso usare meno acqua. Ovvero: avere qualcosa che riempe il volume permette di usare meno risorse. Pu√≤ essere utile? boh.\nüá¨üáß for breakfast I had milk and cereal. I heated the milk in the saucepan, poured it hot over the cereal. Ate. Finished, I put the saucepan in the sink and the bowl inside the saucepan and I put a bit of water in so that, like my mum taught me, it‚Äôs easier to wash up. If I put the bowl inside the saucepan, inside the saucepan the water reaches the top edge sooner because the bowl is taking up volume, so I can use less water. Basically: having something that fills the volume means you can use fewer resources. Could that be useful? Dunno."
  },
  {
    "objectID": "projects/adding-problem-part2.html",
    "href": "projects/adding-problem-part2.html",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 2",
    "section": "",
    "text": "In this blog post, I‚Äôll complete the journey started in the previous post in which I introduced the problem and showed some interesting plots depicting weights and the learning dynamics that was behind it. I tried to explain the way that GRU works under the hood, step by step with a full example of a sequence going through the whole GRU. We saw step by step each and every transformation that the input is subjected to. In this blog post we will do something in my opinion more interesting that is analysing the weights. Now, this can actually mean everything so I want to split the discussion in two parts:\n\nWhat the weights actually mean? We saw a complete forward pass that showed more or less how the initial input is transformed into the hidden layer. But why those specific weights rather than other values?\nHow did we get there? It‚Äôs well known that weights are usually randomly initialized in networks, so how did we get from those random values to our values? Is the dynamics of the learning responsible of the specific values? Could have this been done differently? Could have the weights have followed different trajectories? In either case can it be proved?\n\nLet‚Äôs begin, this is gonna be a lot of fun! (And a lot of work!!)"
  },
  {
    "objectID": "projects/adding-problem-part2.html#introduction",
    "href": "projects/adding-problem-part2.html#introduction",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 2",
    "section": "",
    "text": "In this blog post, I‚Äôll complete the journey started in the previous post in which I introduced the problem and showed some interesting plots depicting weights and the learning dynamics that was behind it. I tried to explain the way that GRU works under the hood, step by step with a full example of a sequence going through the whole GRU. We saw step by step each and every transformation that the input is subjected to. In this blog post we will do something in my opinion more interesting that is analysing the weights. Now, this can actually mean everything so I want to split the discussion in two parts:\n\nWhat the weights actually mean? We saw a complete forward pass that showed more or less how the initial input is transformed into the hidden layer. But why those specific weights rather than other values?\nHow did we get there? It‚Äôs well known that weights are usually randomly initialized in networks, so how did we get from those random values to our values? Is the dynamics of the learning responsible of the specific values? Could have this been done differently? Could have the weights have followed different trajectories? In either case can it be proved?\n\nLet‚Äôs begin, this is gonna be a lot of fun! (And a lot of work!!)"
  },
  {
    "objectID": "projects/adding-problem-part2.html#what-those-parameters-mean",
    "href": "projects/adding-problem-part2.html#what-those-parameters-mean",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 2",
    "section": "2 What those parameters mean?",
    "text": "2 What those parameters mean?\n\n\n\n\n\n\nNote\n\n\n\nI will use weights and parameters interchangeably. One could argue that they have different meanings (and it might even be true) but in our case maybe we can loosen a bit this detail in terminology and live happily anyway.\n\n\nSo: what those parameter mean?\nI tried to figure this out because I noticed one really weird thing looking at the last hidden layer of our walkthrough.\nLet‚Äôs refresh quickly what was the example, remember that the input sequence was:\n\\[\n\\text{x} =\n\\left[\n\\begin{bmatrix}\n12 \\\\ 0\n\\end{bmatrix}\n\\begin{bmatrix}\n37 \\\\ 1\n\\end{bmatrix}\n\\begin{bmatrix}\n12 \\\\ 0\n\\end{bmatrix}\n\\begin{bmatrix}\n21 \\\\ 1\n\\end{bmatrix}\n\\right]\n\\]\nwe normalized the numbers so that they were represented in hundreths, so basically what we had after the normalization was:\nThe input sequence is represented as a tensor:\n\\[\n\\text{x} =\n\\begin{bmatrix}\n0.12 & 0 \\\\\n0.37 & 1 \\\\\n0.12 & 0 \\\\\n0.21 & 1\n\\end{bmatrix}\n\\]\nMy architecture was given the smallest hidden layer as possible on purpose. What I was trying to do was pushin the GRU cell to make the best out of what it had, and my expectation was that at some point it would have learned to basically add to it‚Äôs hidden layer the value in position \\(0\\) iff the flaf in position \\(1\\) was equal to \\(true\\) (or \\(1\\) ok‚Ä¶) This didn‚Äôt happen and looking back at my original thought I feel a bit stupid about it. Let‚Äôs recall what a GRU cell is. Initially, for \\(t = 0\\), the hidden layer is \\(h_0 = 0\\).\nthen the cell uses this series of transformations to get tha hidden layer out:\n\\[\\begin{aligned}\nz_t &= \\sigma(W_z x_t + U_z h_{t-1} + b_z) \\\\\nr_t &= \\sigma(W_r x_t + U_r h_{t-1} + b_r) \\\\\n\\hat{h}_t &= \\phi(W_h x_t + U_h (r_t \\odot h_{t-1}) + b_h) \\\\\nh_t &= z_t \\odot h_{t-1} + (1 - z_t) \\odot \\hat{h}_t\n\\end{aligned}\\]\nif you look at the activation function this should already ring a bell, but let‚Äôs walk through step by step. Let‚Äôs assume we always have four elements and 2 of them are summed to the total. Let‚Äôs also assume that we normalize to the maximum each single value can reach (in our case \\(100\\)). Let‚Äôs have an extreme example now:\n\\[\n\\text{x} =\n\\begin{bmatrix}\n100 & 1 \\\\\n100 & 1 \\\\\n100 & 0 \\\\\n100 & 0\n\\end{bmatrix}\n\\]\nwhich after our normalization becomes:\n\\[\n\\text{x} =\n\\begin{bmatrix}\n1.0 & 1 \\\\\n1.0 & 1 \\\\\n1.0 & 0 \\\\\n1.0 & 0\n\\end{bmatrix}\n\\]\nNow: if my first hypothesis was right the hidden layer should have contained something like \\(1.0 + 1.0 = 2.0\\). But this could have never happened. Why? Look at the activation functions and also at how the new hidden cell is calculated\n\\[\nh_t = z_t \\odot h_{t-1} + (1 - z_t) \\odot \\hat{h}_t\n\\]\nBasically it‚Äôs a weighted sum of the old value of the cell and the new value of the cell. If the weight was either \\(0\\) or \\(1\\) in extreme cases this would have meant that either the old value was being kept as it was (ignoring totally the new value) or the converse: the old value was forgotten and the new was taking its place. So we might expect that the truth is in the middle, meaning that \\(z_t\\) was around \\(.5\\) so that it took half the information from the \\(t-1\\) step and half the information from the new value. But does this make sense? Why \\(\\frac{1}{2}\\)?\nIf we see at what the network is doing instead, we observe a cool and symmetrical (I LOVE SYMMETRIES!) behaviour. What is that? If you see the old post you‚Äôll see that \\(z_t\\) takes on some nice values that are\n\\[\\begin{aligned}\n\nz_1 = 0.8275274634361267 \\\\\nz_2 = 0.15969596803188324 \\\\\nz_3 = 0.8254608511924744 \\\\\nz_4 = 0.1668987274169922 \\\\\n\\end{aligned}\\]\nBasically when the flag is \\(0\\), then \\(z_t \\ge .825\\) otherwise \\(z_t \\le .166\\). So we already observe a pattern. I love this so much! Let‚Äôs recall then how \\(z_t\\) is computed:\n\\[\nz_t = \\sigma(W_z x_t + U_z h_{t-1} + b_z)\n\\]\nwhere \\(\\sigma\\) is the sigmoid function. Recall that the sigmoid function squeezes its input in the range \\([0,1]\\) In the plot below we map \\(100\\) points to the sigmoid function (did you know that function, map and application are quite the same thing in math?)\n\n\nCode\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-10,10,100)\ny = 1/(1+np.exp(-x))\nsns.set_theme(style=\"darkgrid\")  # This gives the typical Seaborn look\n\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=x, y=y, linewidth=2)\nplt.title('Sigmoid Function')\nplt.xlabel('x')\nplt.ylabel('$\\sigma(x)$')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nSo now, let‚Äôs do something nice and get back our weights, that are gonna be useful for our next section.\n\n\nCode\nimport json\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm.notebook import trange, tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport os\n\nclass AddingProblemGRU(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(AddingProblemGRU, self).__init__()\n        self.gru = nn.GRU(\n            input_size, hidden_size, num_layers=1, batch_first=True\n        )\n        self.linear = nn.Linear(hidden_size, output_size)\n        self.init_weights()\n\n    def init_weights(self):\n        for name, param in self.gru.named_parameters():\n            if \"weight\" in name:\n                nn.init.orthogonal_(param)\n            elif \"bias\" in name:\n                nn.init.constant_(param, 0)\n        nn.init.xavier_uniform_(self.linear.weight)\n        nn.init.zeros_(self.linear.bias)\n\n    def forward(self, x):\n        out, hn = self.gru(x)\n        output = self.linear(out[:, -1, :])\n        return output, out\n\n\n# Reproducibility\nRANDOM_SEED = 37\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\ntorch.cuda.manual_seed_all(RANDOM_SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Hyperparameters\nDELTA = 0\nSEQ_LEN = 4\nHIGH = 100\nN_SAMPLES = 10000\nTRAIN_SPLIT = 0.8\nBATCH_SIZE = 256\nLEARNING_RATE = 1e-4\nWEIGHT_DECAY = 1e-5\nCLIP_VALUE = 2.0\nNUM_EPOCHS = 3000\nHIDDEN_SIZE = 1\nOUTPUT_SIZE = 1\nINPUT_SIZE = 2\n\ndef adding_problem_generator(N, seq_len=6, high=1, delta=0.6):\n    actual_seq_len = np.random.randint(\n        int(seq_len * (1 - delta)), int(seq_len * (1 + delta))\n    ) if delta &gt; 0 else seq_len\n    num_ones = np.random.randint(2, min(actual_seq_len - 1, 4))\n    X_num = np.random.randint(low=0, high=high, size=(N, actual_seq_len, 1))\n    X_mask = np.zeros((N, actual_seq_len, 1))\n    Y = np.ones((N, 1))\n    for i in range(N):\n        positions = np.random.choice(actual_seq_len, size=num_ones, replace=False)\n        X_mask[i, positions] = 1\n        Y[i, 0] = np.sum(X_num[i, positions])\n    X = np.append(X_num, X_mask, axis=2)\n    return X, Y\n\nX, Y = adding_problem_generator(N_SAMPLES, seq_len=SEQ_LEN, high=HIGH, delta=DELTA)\n\ntraining_len = int(TRAIN_SPLIT * N_SAMPLES)\ntrain_X = X[:training_len]\ntest_X = X[training_len:]\ntrain_Y = Y[:training_len]\ntest_Y = Y[training_len:]\n\ntrain_dataset = TensorDataset(\n    torch.tensor(train_X).float(), torch.tensor(train_Y).float()\n)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\ntest_dataset = TensorDataset(\n    torch.tensor(test_X).float(), torch.tensor(test_Y).float()\n)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# File paths for saved data\ntrain_losses_path = \"train_losses.json\"\ntest_losses_path = \"test_losses.json\"\nall_weights_path = \"all_weights.json\"\nmodel_save_path = (\n    f\"gru_adding_problem_model_epochs_{NUM_EPOCHS}_hidden_{HIDDEN_SIZE}.pth\"\n)\n\nFORCE_TRAIN = False\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ncriterion = nn.MSELoss()\ndef evaluate(model, data_loader, criterion, high):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for inputs, labels in data_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            inputs[:, :, 0] /= high\n            outputs, _ = model(inputs)\n            outputs = outputs * high\n            loss = criterion(outputs, labels)\n            total_loss += loss.item() * inputs.size(0)\n    return total_loss / len(data_loader.dataset)\n\n\n# Try to load data from files\nif FORCE_TRAIN == False and os.path.exists(train_losses_path) and os.path.exists(test_losses_path) and os.path.exists(all_weights_path) and os.path.exists(model_save_path):\n    with open(train_losses_path, \"r\") as f:\n        train_losses = json.load(f)\n    with open(test_losses_path, \"r\") as f:\n        test_losses = json.load(f)\n    with open(all_weights_path, \"r\") as f:\n        all_weights_loaded = json.load(f)\n\n    # Convert loaded weights (which are lists) back to numpy arrays\n    all_weights = []\n    for epoch_weights_list in all_weights_loaded:\n        epoch_weights_dict = {}\n        for name, weights_list in epoch_weights_list.items():\n            epoch_weights_dict[name] = np.array(weights_list)\n        all_weights.append(epoch_weights_dict)\n    #load model\n    model = AddingProblemGRU(\n    input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, output_size=OUTPUT_SIZE)\n    model.load_state_dict(torch.load(model_save_path))\n    model.to(device)\n\n\n\nelse:\n    model = AddingProblemGRU(\n        input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, output_size=OUTPUT_SIZE\n    )\n    model.to(device)\n\n    optimizer = torch.optim.Adam(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode=\"min\", factor=0.5, patience=5, min_lr=1e-6, verbose=False\n    )\n\n\n    \n\n    train_losses = []\n    test_losses = []\n    all_weights = []\n\n    for epoch in trange(NUM_EPOCHS, desc=\"Epoch\"):\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            inputs[:, :, 0] /= HIGH\n            labels_scaled = labels / HIGH\n            optimizer.zero_grad()\n            outputs, _ = model(inputs)\n            loss = criterion(outputs, labels_scaled)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_VALUE)\n            optimizer.step()\n            running_loss += loss.item() * inputs.size(0)\n\n        epoch_loss = running_loss / len(train_loader.dataset)\n        train_losses.append(epoch_loss)\n\n        if epoch % 49 == 0:\n            test_loss = evaluate(model, test_loader, criterion, HIGH)\n            test_losses.append(test_loss)\n            scheduler.step(test_loss)\n\n            weights_dict = {}\n            for name, param in model.named_parameters():\n                weights_dict[name] = param.data.cpu().numpy().copy()\n            all_weights.append(weights_dict)\n        else:\n            test_losses.append(None)\n\n    # Save data to files\n    with open(train_losses_path, \"w\") as f:\n        json.dump(train_losses, f)\n    with open(test_losses_path, \"w\") as f:\n        json.dump(test_losses, f)\n    # Convert weights to lists for JSON serialization\n    all_weights_serializable = [\n        {k: v.tolist() for k, v in epoch_weights.items()}\n        for epoch_weights in all_weights\n    ]\n    with open(all_weights_path, \"w\") as f:\n        json.dump(all_weights_serializable, f)\n\n    # Save Model\n    model_save_path = (\n        f\"gru_adding_problem_model_epochs_{NUM_EPOCHS}_hidden_{HIDDEN_SIZE}.pth\"\n    )\n    torch.save(model.state_dict(), model_save_path)\n\ninput_sequence = torch.tensor([\n    [12,0],\n    [37,1],\n    [12,0],\n    [21,1],\n]).float().unsqueeze(0)\n\n\ninput_sequence[:, :, 0] /= HIGH\n\n# Get weights from the last training epoch.  'all_weights' is populated by the loading/training section.\nlast_epoch_weights = model.state_dict() #all_weights[-1]\n\n# Extract the relevant weight matrices\nW_ih = torch.tensor(last_epoch_weights['gru.weight_ih_l0']).float()  # Input-to-hidden\nW_hh = torch.tensor(last_epoch_weights['gru.weight_hh_l0']).float()  # Hidden-to-hidden\nb_ih = torch.tensor(last_epoch_weights['gru.bias_ih_l0']).float()  # Input-to-hidden bias\nb_hh = torch.tensor(last_epoch_weights['gru.bias_hh_l0']).float()  # Hidden-to-hidden bias\nW_linear = torch.tensor(last_epoch_weights['linear.weight']).float() # Linear layer weights\nb_linear = torch.tensor(last_epoch_weights['linear.bias']).float()   # Linear layer bias"
  },
  {
    "objectID": "projects/adding-problem-part2.html#the-update-gate-at-t0",
    "href": "projects/adding-problem-part2.html#the-update-gate-at-t0",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 2",
    "section": "3 The Update Gate at \\(t=0\\)",
    "text": "3 The Update Gate at \\(t=0\\)\nLet‚Äôs start our analysis from the bottom up in the calculation of the updated value of the hidden state. For how PyTorch implements the GRU cell under the hood the \\(z_t\\) value is used in this way:\n\\[\n\\texttt{hy} = (1 - \\texttt{updategate}) * \\texttt{newgate} + \\texttt{updategate} * \\texttt{h}\n\\]\nThis means basically that the higher the value of \\(\\texttt{updategate}\\), the more we should keep in memory our previous value (sum?) and ignore the newgate (the update in memory, the term to be added to the sum?). This sums up with our previous observation: with flag \\(1\\), update gate was pretty low and when flag was \\(0\\) then \\(z_t\\) was pretty high. But how much each component is involved in this behaviour?\n\n\nCode\nx_1 = input_sequence[:, 0, :]\nh = torch.zeros(1, 1, 1)\n\n# Stable version (using torch.matmul)\ngi_stable = torch.matmul(x_1, W_ih.t())\ngh_stable = torch.matmul(h, W_hh.t())\n\n# Get gates\ni_r, i_z, i_n = gi_stable.chunk(3, dim=1)\nh_r, h_z, h_n = gh_stable.chunk(3, dim=2)\n\n# Apply gate operations with controlled precision\nresetgate = torch.sigmoid(i_r + h_r  + b_ih[0] + b_hh[0])\nupdategate = torch.sigmoid(i_z + h_z + b_ih[1] + b_hh[1])\nnewgate = torch.tanh(i_n + b_ih[2] + (resetgate * (h_n + b_hh[2])))\n\nhy = (1 - updategate) * newgate + updategate * h\nh_t = hy\n\n\nThese are the three components that go through the sigmoid for the update gate:\n\\[\\eqalign{\nb_{ih_z} +  b_{hh_z} &= 1.6185756921768188 \\cr\ni_z &= -0.05037130042910576; \\cr\nh_z &= -0.0; \\cr\ni_z + h_z + b_{ih_z} + b_{hh_z} &= 1.568204402923584 \\cr\nz_t &= 0.8275274634361267 \\cr\n}\n\\]\nThis is interesting. So let‚Äôs break this down a little bit.\nProbably it‚Äôs a good idea to verify how each term here is calculated and what happens. Let‚Äôs begin with seeing how \\(i_z\\) is calculated, using the original weight matrix.\n\\[\\eqalign{\ni_z &= x_t \\cdot W_{ih_z}^T\\cr\nW_{ih_z}^T &=\n\\begin{bmatrix}\n-0.4197608530521393 \\\\ -3.02486252784729\n\\end{bmatrix} \\cr\n}\n\\]\nWhat we do to obtain \\(i_z\\) is multiply our input row vector \\(x_0^T = \\left[\\begin{smallmatrix}0.12 \\\\ 0.0\\end{smallmatrix}\\right]\\) by the column vector \\(W_{ih_z}^T\\)\n\\[\n\\begin{bmatrix}0.12 & 0.0\\end{bmatrix}\\cdot \\begin{bmatrix}\n-0.4197608530521393 \\\\ -3.02486252784729\n\\end{bmatrix}\n\\]\nwhich becomes:\n\\[\\eqalign{\ni_z &= 0.12  \\cdot -0.4197608530521393 + 0.0 \\cdot  -3.02486252784729 \\cr\ni_z &= 0.12  \\cdot -0.4197608530521393 + \\cancel{0.0 \\cdot  -3.02486252784729} (\\texttt{flag}=0) \\cr\ni_z &= 0.12  \\cdot -0.4197608530521393 \\cr\ni_z &= -0.05037130042910576\n}\n\\]\nNow this conveys a super important information: when the \\(\\texttt{flag} = 0\\) then only the number has some importance in the final calculation because the flag cancels the second term of the sum as saw before. Now remember, this is only one of the three terms that go through the sigmoid at the end to obtain the final \\(z_t\\) term.\nSo let‚Äôs carry on with the second part of it, \\(h_z\\): basically it‚Äôs \\(0\\) since when our GRU cell sees the input for the first time its input state is \\(h=0\\), and whatever we multiply here stays zero.\n\\[\nh_z = -0.0\n\\]\nNow the last portion of our sum, the bias. Now, you should know that many ML scientists avoid using the bias term when the data is already centered or when the model inherently accounts for offsets, as it can be redundant and complicate interpretation. But in my case the bias was left there. And in the first calculation you can observe that without the bias term accounts super heavily on the final sum:\n\\[\\eqalign{\nb_{ih_z} &= 0.8092878460884094 \\cr\nb_{hh_z} &= 0.8092878460884094 \\cr\n}\n\\]\nSo the final sum is:\n\\[\n\\texttt{temp} = -0.05037130042910576 + 0 + 0.8092878460884094 + 0.8092878460884094;\n\\]\nI‚Äôve always believed that a plot tells more then hundred numbers, so let‚Äôs plot a cumulative sum of the four terms to check how much each of them accounts for the gran total:\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.rcParams[\"text.usetex\"] = True\n\nitems = [r\"$i_z$\", r\"$h_z$\", r\"$b_{ih_z}$\", r\"$b_{hh_z}$\"]\nvalues = [(input_sequence[0][0][0] * W_ih[1][0]).item(), 0, (b_ih[1]).item(), (b_hh[1]).item()]\n\ncumsum_values = np.cumsum(values)\n\ndf = pd.DataFrame({\"Term\": items, \"Cumulative Sum\": cumsum_values})\n\nplt.figure(figsize=(8, 5))\nsns.barplot(x=\"Term\", y=\"Cumulative Sum\", data=df, color=\"skyblue\", alpha=0.7)\n\nfor i, (item, value) in enumerate(zip(items, values)):\n    plt.text(i, cumsum_values[i] - value / 2, f\"$+{value}$\", ha=\"center\", fontsize=12, color=\"black\")\n\nplt.ylabel(r\"\\textbf{Cumulative Sum}\", fontsize=12)\nplt.xlabel(r\"\\textbf{Terms}\", fontsize=12)\nplt.title(r\"\\textbf{Cumulative Sum Contribution}\", fontsize=14)\nplt.ylim(0, max(cumsum_values) + .5)  # Adjust y-limit\nplt.show()\n\n\n\n\n\n\n\n\n\nWow, so basically during the first iteration, we‚Äôre only dealing with bias terms. An important distinction to remember is that bias terms don‚Äôt depend on or connect to any individual data point being analyzed. Instead, they capture and convey overall patterns or information from the entire dataset as a whole.\nTo complete this first step, let‚Äôs do a sanity check. I plot here what would have happened after the first iteration to a number in the range \\([0, 100]\\) (remember normalized by a factor of \\(100\\), so in the final range of \\([0,1]\\)) There are 4 plots:\n\n\\(y_0\\) with bias: In our trained GRU cell (green line), when \\(\\sigma\\) is applied to an input flagged with \\(0\\), the values oscillate between \\([0.77, 0.83]\\).\n\\(y_1\\) with bias: In our trained GRU cell (red line), when \\(\\sigma\\) is applied to an input flagged with \\(1\\), the values oscillate between \\([0.14, 0.20]\\).\n\\(y_0\\) no bias: If we removed the bias term \\(b_z\\) from the GRU cell (blue line) and applied \\(\\sigma\\) to an input flagged with \\(0\\), the value stays around \\(0.5\\). The reason for the cell choosing such a strong bias remains unclear - it seems significant since it pushes \\(z_t\\) considerably higher.\n\\(y_1\\) no bias: If we removed the bias term \\(b_z\\) from the GRU cell (orange line), and applied \\(\\sigma\\) to an input flagged with \\(1\\), the values oscillate between \\(0.05\\). Again, the strong bias choice is super cool. One (crazy!) hypothesis: it pushes up \\(z_t\\) to act as a factor \\(\\frac{1}{4}\\), possibly because the network expects 4 items where 2 flags are in unknown positions and maintains this factor to account for missing items by adding \\(\\frac{1}{4}\\) of the current candidate.\n\nThe takeaway here is how the bias term strongly affects the outputs, particularly in pushing up \\(z_t\\). While we can hypothesize about the network‚Äôs strategy (especially regarding the \\(\\frac{1}{4}\\) factor), the exact reason for such a strong bias will be hopefully uncovered in the next sections.\n\n\nCode\nbz =  b_ih[1].item() + b_hh[1].item()\nx = np.linspace(0,1, 1000)\ny0 = x * W_ih[1][0].item()\ny1 = x * W_ih[1][0].item() + W_ih[1][1].item() \ny0_nb = 1 / (1 + np.exp(-(y0)))\ny1_nb = 1 / (1 + np.exp(-(y1)))\ny0_b = 1 / (1 + np.exp(-(y0 + bz)))\ny1_b = 1 / (1 + np.exp(-(y1 + bz)))\n\nsns.set_theme(style=\"darkgrid\")  # This gives the typical Seaborn look\n\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=x, y=y0_nb, linewidth=2, label='$y_0$ no bias')\nsns.lineplot(x=x, y=y1_nb, linewidth=2, label='$y_1$ no bias')\nsns.lineplot(x=x, y=y0_b, linewidth=2, label='$y_0$ with bias')\nsns.lineplot(x=x, y=y1_b, linewidth=2, label='$y_1$ with bias')\nplt.title('Sigmoid Function')\nplt.xlabel('x')\n\nplt.ylabel('$i_z$')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†1"
  },
  {
    "objectID": "projects/adding-problem-part2.html#the-reset-gate-at-t0",
    "href": "projects/adding-problem-part2.html#the-reset-gate-at-t0",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 2",
    "section": "4 The Reset gate at \\(t=0\\)",
    "text": "4 The Reset gate at \\(t=0\\)\nI lied earlier. I said we were going bottom up, but we‚Äôre actually approaching this top down because next in our discussion is the reset gate. The reasons is easy to tell: the update gate value \\(z_t\\) acts on the previous \\(h_{t-1}\\) as well as on \\(\\hat{h}_t\\) (the candidate memory update). But to make a discussion about \\(\\hat{h}_t\\) we need to discuss the reset gate \\(r_t\\) first.\nLet‚Äôs recall again how both the reset gate and newgate are computed\n\\[\\eqalign{\nr_t &= \\sigma(W_r x_t + U_r h_{t-1} + b_r) \\\\\n\\hat{h}_t &= \\phi(W_h x_t + U_h (r_t \\odot h_{t-1}) + b_h) \\\\\n}\n\\]\nFirst of all, you might have noticed already in the previous post that there was a different activation function in one step, which is \\(\\phi\\). But what is \\(\\phi\\)? It is the \\(\\tanh\\), which yields a plot like this:\n\n\nCode\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-10,10,100)\ny = np.tanh(x)\nsns.set_theme(style=\"darkgrid\")  # This gives the typical Seaborn look\n\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=x, y=y, linewidth=2)\nplt.title('Sigmoid Function')\nplt.axis(True)\nplt.xlabel('x')\nplt.ylabel('$\\sigma(x)$')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nPreviously \\(\\sigma\\) was squeezing its input in the range \\([0, 1]\\), whereas \\(\\phi\\) maps its input to a value in the range \\([-1, 1]\\). The reason why the update gate employs this activation function is that the range ([-1, 1]) allows the gate to dynamically amplify, suppress, or invert features from the previous hidden state, enabling more nuanced control over the update mechanism compared to the purely additive or multiplicative behavior of \\([0, 1]\\). Moreover: this symmetry and bidirectional scaling can improve gradient flow during training and help the model learn richer representations by incorporating both positive and negative adjustments to the hidden state. In other words \\(\\sigma\\) was telling: ‚ÄúHow much of this should I add to this other thing?‚Äù, whereas \\(\\phi\\) is telling ‚ÄúHow much of this should I add or remove from this other thing?‚Äù\nNow, why is this important? Remember that the GRU cell during its computation tries to build an internal hidden state that conveys some information. Think of the hidden state as some sort of scratchpad, where you take notes as you read more data. Sometimes some of the new information should be added to the previous state, sometimes new data should be ignored. Sometimes new data instead needs to kinda remove information from the state, in order to have fresher information.\nImagine for the sake of example to be a detective, which is trying to solve a mystery. You might get new information as time flows. At some point you might even have a track on a suspect and build your knowledge on that. But then at some point, you find out that your suspect was just cheating on his wife and as such you need to forget about him, otherwise you‚Äôll focus on something that‚Äôs not needed in your ivnestigation.\nHow does our network know what to forget, what to keep, what to update and so on is the task of machine learning. But we can surely observe what happened here!\nFirst we need to study the reset gate \\(r_t\\), similarly to how we did before!\nAgain, there are three components that go through the sigmoid for the update gate:\n\\[\\eqalign{\nb_{ih_r} +  b_{hh_r} &= 1.4853122234344482 \\cr\ni_r &= 0.027704410254955292;  \\cr\nh_r &= -0.0;  \\cr\ni_r + h_r + b_{ih_r} + b_{hh_r} &= 1.513016700744629 \\cr\nr_t &= 0.819507896900177 \\cr\n}\n\\]\nAs we observed before the bias is king again, because it accounts for the largest part of the activation. But we can observe something else here, let‚Äôs compare how the flag affects both the update (\\(z_t\\)) and reset (\\(r_t\\)) gate:\nUpdate gate \\(z_t\\) with \\(\\texttt{flag} = 0\\)\n\\[\\eqalign{\ni_z + h_z + b_{ih_z} + b_{hh_z} &= 1.568204402923584\\cr\nz_t &= 0.8275274634361267 \\cr\n}\\] and with \\(\\texttt{flag} = 1\\) \\[\\eqalign{\ni_z + h_z + b_{ih_z} + b_{hh_z} &= -1.456658124923706\\cr\nz   _t &= 0.18897898495197296 \\cr\n}\\]\nIn Figure¬†1 we saw how \\(z_t\\) was affected pretty heavily by the flag at position \\(1\\) of the input item, whereas in this case we observe that the reset gate is not affected too much about the flag. Basically it looks like if we have \\(\\texttt{flag} = 0\\)\n\\[\\eqalign{\ni_r + h_r + b_{ih_r} + b_{hh_r} &= 1.513016700744629\\cr\nr_t &= 0.819507896900177 \\cr\n}\\]\nor \\(\\texttt{flag} = 1\\)\n\\[\\eqalign{\ni_r + h_r + b_{ih_r} + b_{hh_r} &= 2.146040916442871\\cr\nr_t &= 0.8952982425689697 \\cr\n}\\]\nthe reset gate \\(r_t\\) keeps taking a pretty large value (remember that sigmoid is in the range \\([0,1]\\), so you can consider it as a percentage, meaning that the reset gate is always above \\(80\\%\\))\nWhy is this? Let‚Äôs take a look at the weights involved. Again being this the first step in the sequence the hidden state is not yer involved and we can safely (FOR NOW!) ignore it.\n\\[\\eqalign{\ni_r &= x_t \\cdot W_{ih_r}^T\\cr\nW_{ih_r}^T &=\n\\begin{bmatrix}\n0.2308700829744339 \\\\ 0.6330242156982422\n\\end{bmatrix} \\cr\n}\\]\nLet‚Äôs compare it with the update gate weights: \\[\\eqalign{\ni_z &= x_t \\cdot W_{ih_z}^T\\cr\nW_{ih_z}^T &=\n\\begin{bmatrix}\n-0.4197608530521393 \\\\ -3.02486252784729\n\\end{bmatrix} \\cr\n}\\]\nWe can take two key observations:\n\nThe update weights are both negative and reset are both positive\nThe difference in magnitude is noteworthy: in the update gate is heavily affected by the flag input, moving the activation along the \\(\\hat{y}\\), whereas the reset gate is not so strongly affected by it.\n\nLet‚Äôs plot them both here where \\(y_{0z}\\) and \\(y_{1z}\\) are the update gates when \\(\\texttt{flag} = 0\\) and \\(\\texttt{flag} = 1\\), and \\(y_{0r}\\) and \\(y_{1r}\\) are the reset gates in the same cases, respectively.\n\n\nCode\nbr =  b_ih[0].item() + b_hh[0].item()\nx = np.linspace(0,1, 1000)\ny0 = x * W_ih[1][0].item()\ny1 = x * W_ih[1][0].item() + W_ih[1][1].item() \ny0_r = x * W_ih[0][0].item()\ny1_r = x * W_ih[0][0].item() + W_ih[0][1].item() \n\ny0_b = 1 / (1 + np.exp(-(y0 + br)))\ny1_b = 1 / (1 + np.exp(-(y1 + br)))\ny0_r_b = 1 / (1 + np.exp(-(y0_r + br)))\ny1_r_b = 1 / (1 + np.exp(-(y1_r + br)))\n\nsns.set_theme(style=\"darkgrid\")\n\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=x, y=y0_b, linewidth=2, linestyle='--', color='blue', label='$y_{0z}$')\nsns.lineplot(x=x, y=y1_b, linewidth=2, linestyle='--', color='orange', label='$y_{1z}$')\nsns.lineplot(x=x, y=y0_r_b, linewidth=2, linestyle='-', color='green', label='$y_{0r}$')\nsns.lineplot(x=x, y=y1_r_b, linewidth=2, linestyle='-', color='red', label='$y_{1r}$')\n\nplt.title('Comparison between $z_t$ and $r_t$')\nplt.xlabel('x')\nplt.ylabel('$i_r$')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†2\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThis is all cool and stuff‚Ä¶ But I forgot to mention that is also useless. Just kidding. But thre‚Äôs an actual catch here: at \\(t=0\\) this is acting only on the bias term because \\(r_t\\) is applied via a Hadamard product (element-wise multiplication) to the previous hidden state plus the bias. Its purpose is to control how much of it we want to remember. But at \\(t=0\\), \\(h_t = h_0 = 0\\), meaning \\(r_0\\) only acts on the bias term!"
  },
  {
    "objectID": "projects/adding-problem-part2.html#the-new-gate-at-t0",
    "href": "projects/adding-problem-part2.html#the-new-gate-at-t0",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 2",
    "section": "5 The New gate at \\(t=0\\)",
    "text": "5 The New gate at \\(t=0\\)\nYou will find in literature multiple ways to refer to it, but I think the most useful is: candidate \\(\\hat{h}_t\\). Basically, it weights some of the input \\(\\mathbf{x}_t\\) and some of the \\(h_{t-1}\\) value using the reset gate \\(r_t\\) to decide how much it should be brought to the activation. In other words, the candidate \\(\\hat{h}_t\\) represents a proposed new hidden state that combines the current input \\(\\mathbf{x}_t\\) and the previous hidden state \\(h_{t-1}\\), modulated by the reset gate \\(r_t\\).\nLet‚Äôs recall how the candidate hidden state \\(\\hat{h}_t\\) is computed:\n\\[\n\\hat{h}_t = \\phi(W_h \\mathbf{x}_t + b_{{ih}_n} + r_t \\odot  [(U_h \\cdot h_{t-1}) + b_{{hh}_n}])\n\\] where \\(\\phi = \\tanh(\\cdot)\\)\n\n\n\n\n\n\nNote\n\n\n\nUp until now we always treated the bias terms as a single value, summing them up. But now we need to make extra care! Previously we had 2 terms added up and each of them had its bias, so we could safely add them either during the $$ product or later when inside the \\(\\sigma\\) sigmoid. But now, the bias term is multipled to the reset gate as much as the projected hidden state.\n\n\nOne term cancels out, because at \\(t=0\\) \\(h_0 = 0\\):\n\\[\\eqalign{\n\\hat{h}_t &= \\phi(W_h \\mathbf{x}_t + b_{{ih}_n} + r_t \\odot  [\\cancel{(U_h \\cdot h_{t-1})} + b_{{hh}_n}])\\cr\n\\hat{h}_t &= \\phi(W_h \\mathbf{x}_t + b_{{ih}_n} + r_t \\odot  b_{{hh}_n})\n}\\]\nfrom the previous section we learnt that \\(r_t\\) has always values above \\(.8\\), which at \\(t=0\\) means that we carry on with us \\(80\\%\\) of the bias in the activatoin \\(\\phi\\).\nNow this leaves us with these terms: \\[\\eqalign{\nb_{ih_n}  &= 0.6560405492782593 \\cr\nb_{hh_n} &= 0.16526484489440918 \\cr\ni_n &= -0.03407169133424759;  \\cr\nh_t &= 0.0;  \\cr\nh_t + b_{hh_n} &= 0.16526484489440918;  \\cr\n\\hat{h}_t &= \\phi(i_n + b_{ih_n} + r_t \\odot b_{hh_n}) \\cr\n\\hat{h}_t &= \\phi(-0.03407169133424759 + 0.6560405492782593 + 0.819507896900177 \\odot 0.16526484489440918) \\cr\n\\hat{h}_t &= \\phi(0.7574047034149629) \\cr\n\\hat{h}_t &= 0.639545738697052 \\cr\n}\n\\]\nAgain, bias is king! It accounts for basically the whole activation. And again a reminder about it:\n\n\n\n\n\n\nImportant\n\n\n\nBias is not about a single data point. Instead, it reflects a systematic shift in the entire dataset or even during the learning process. It tells us something about the data as a whole, rather than just the point we‚Äôre observing. This is so important: no matter what data we input, the bias would remain the same!"
  },
  {
    "objectID": "projects/adding-problem-part2.html#final-step",
    "href": "projects/adding-problem-part2.html#final-step",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 2",
    "section": "6 Final step",
    "text": "6 Final step\nSo we‚Äôre finally ready for our final hidden state:\n\\[\\eqalign{\nh_t &= (1 - z_t) \\cdot  \\hat{h} + z_t \\cdot h_0 \\cr\nh_t &= 0.1724725365638733 \\cdot 0.639545738697052 + \\cancel{0.8275274634361267 \\cdot 0.0}\\cr\nh_t &= 0.11030407580169665 + \\cancel{0.0}\\cr\nh_t &= 0.11030407249927521\\cr\n}\\]\nNow: this is a Recurrent Neural Network, so theory tells us that it should be able to handle sequences of arbitrary length. But is it?\nLet‚Äôs try to just project into the final output through the linear layer:\n\\[\n\\text{x} =\n\\begin{bmatrix}\n12 & 0 \\\\\n\\end{bmatrix}\n\\]\n\\[\n\\hat{y} = 41.076526045799255 \\neq 0\n\\]\nWoooh! That‚Äôs waaay off. What if we let the flag be one instead?\n\\[\n\\text{x} =\n\\begin{bmatrix}\n12 & 1 \\\\\n\\end{bmatrix}\n\\]\n\\[\n\\hat{y} = 101.57540893554688 \\gg 12\n\\]\nWay off again!\nWait‚Ä¶ let‚Äôs see hat happend if we put 4 elements, 2 of which are set to true?\n\\[\n\\text{x} =\n\\left[\n\\begin{bmatrix}\n75 \\\\ 1\n\\end{bmatrix}\n\\begin{bmatrix}\n38 \\\\ 1\n\\end{bmatrix}\n\\begin{bmatrix}\n12 \\\\ 0\n\\end{bmatrix}\n\\begin{bmatrix}\n12 \\\\ 0\n\\end{bmatrix}\n\\right]\n\\]\nThis shows your 2-dimensional temporal sequence in the format you wanted. Let me know if you‚Äôd like to adjust the spacing or format further.\n\\[\n\\hat{y} = 112.553955078125 \\approxeq 113 = y\n\\]\nMmmmh! That works much better. But why? It seems the network has learned three key things:\n\nIt‚Äôs a sum: The network has learned to sum only the numbers flagged with \\(1\\). Well, that was actually the main task.\n\nFixed length \\(n = 4\\): It expects an input sequence of length \\(n = 4\\).\n\nExactly two flagged items: It assumes that exactly two elements in the input are flagged with \\(1\\).\n\n\n\n\n\n\n\nFirst Step Insights: What Have We Learned?\n\n\n\nAlright, let‚Äôs pause and take stock. After dissecting the GRU‚Äôs very first move, some things are becoming clearer. We‚Äôre seeing how much those bias terms are driving the initial behavior ‚Äì they‚Äôre the unsung heroes at \\(t=0\\)! And it‚Äôs fascinating how the flag in the input is already so specifically wired to control the update gate (but less so the reset gate, interesting!). Plus, we‚Äôre starting to suspect the network is already ‚Äòassuming‚Äô a certain kind of input ‚Äì fixed length, maybe even expecting those two flagged numbers.\nBut remember our starting questions? We‚Äôre just scratching the surface of ‚Äúwhat these weights mean.‚Äù And the big ‚ÄúWHY?‚Äù ‚Äì ‚Äúhow did we even get these weights?‚Äù ‚Äì is still a complete mystery! This first step analysis is cool, but it‚Äôs just the beginning. To really understand this GRU, we gotta dig deeper into how these weights learned to be this way. And that‚Äôs where the real fun begins‚Ä¶"
  },
  {
    "objectID": "projects/adding-problem-part2.html#coming-next",
    "href": "projects/adding-problem-part2.html#coming-next",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 2",
    "section": "7 Coming next‚Ä¶",
    "text": "7 Coming next‚Ä¶\nIn this part, we took a thorough investagion into the inner workings of our GRU cell, dissecting its very first iteration piece by piece. We carefully traced how the input and weights influenced the ( z_t ), ( r_t ), and ( ) gates, step by step, uncovering how activations evolved and what they actually meant. Along the way, we stumbled upon three key insights about what the network had learned.\nBut that is still just the what‚Äînow it‚Äôs time to ask why.\nWhy did the model learn to do this seemingly strange thing? Was it always heading in this direction, or did it explore different strategies earlier in training? Were the weights trying to do something entirely different at first?\nTo answer these questions, we‚Äôll rewind the clock and analyze how the model‚Äôs weights evolved over time. Did they start off chaotic before settling into a structured pattern? Were different strategies competing before the final approach emerged?\nAnd then, we‚Äôll take things a step further. Instead of just observing the learned weights, we‚Äôll craft our own by hand‚Äîdesigning a set that does precisely what we expect. Then, we‚Äôll compare our manually created weights with the network‚Äôs chosen ones. Did the network find a more efficient solution? Did it take shortcuts we wouldn‚Äôt have thought of? Or did it stumble upon an elegant trick that we can learn from?\nNext up: reverse-engineering learning itself. Let‚Äôs crack this thing open!\nI LOVE THIS SO MUCH!"
  },
  {
    "objectID": "projects/adding-problem-part1.html",
    "href": "projects/adding-problem-part1.html",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 1",
    "section": "",
    "text": "In this blog post, I want to report on some insights I discovered about the training dynamics of a GRU network applied to a toy problem called the ‚ÄúAdding Problem.‚Äù This is a benchmark task for recurrent neural networks. The Adding Problem, introduced by Le, Jaitly, and Hinton, tests a model‚Äôs ability to selectively remember and sum relevant information over a sequence. I trained a small GRU model, visualized its training progress, and examined how its weights evolve during training. This low-dimensional weight space allowed me to observe unique insights about the learning process. This work is part of a larger research project where I aim to leverage training dynamics to create a lightweight network. This network will use a combination of meta-learning and Energy-Based Models (EBM) to adjust weights based on input sequences, allowing it to leverage generic pre-training and adapt to different situations."
  },
  {
    "objectID": "projects/adding-problem-part1.html#introduction",
    "href": "projects/adding-problem-part1.html#introduction",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 1",
    "section": "",
    "text": "In this blog post, I want to report on some insights I discovered about the training dynamics of a GRU network applied to a toy problem called the ‚ÄúAdding Problem.‚Äù This is a benchmark task for recurrent neural networks. The Adding Problem, introduced by Le, Jaitly, and Hinton, tests a model‚Äôs ability to selectively remember and sum relevant information over a sequence. I trained a small GRU model, visualized its training progress, and examined how its weights evolve during training. This low-dimensional weight space allowed me to observe unique insights about the learning process. This work is part of a larger research project where I aim to leverage training dynamics to create a lightweight network. This network will use a combination of meta-learning and Energy-Based Models (EBM) to adjust weights based on input sequences, allowing it to leverage generic pre-training and adapt to different situations."
  },
  {
    "objectID": "projects/adding-problem-part1.html#setup-and-data-generation",
    "href": "projects/adding-problem-part1.html#setup-and-data-generation",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 1",
    "section": "2 Setup and Data Generation",
    "text": "2 Setup and Data Generation\nFirst, I import the necessary libraries and define the function to generate the Adding Problem dataset. The dataset consists of sequences of synthetic numbers and a binary mask. The model‚Äôs task is to sum the numbers indicated by the ‚Äô1‚Äôs in the mask. The sequence length, mask disposition, and even the number of masked elements are variable. I tried to introduce this variability as an additional source of randomness that will be fundamental in the final network trained for playing a table game.\n\n\nCode\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm.notebook import trange, tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport os\n\n# Reproducibility\nRANDOM_SEED = 37\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\ntorch.cuda.manual_seed_all(RANDOM_SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Hyperparameters\nDELTA = 0\nSEQ_LEN = 4\nHIGH = 100\nN_SAMPLES = 10000\nTRAIN_SPLIT = 0.8\nBATCH_SIZE = 256\nLEARNING_RATE = 1e-4\nWEIGHT_DECAY = 1e-5\nCLIP_VALUE = 2.0\nNUM_EPOCHS = 3000\nHIDDEN_SIZE = 1\nOUTPUT_SIZE = 1\nINPUT_SIZE = 2\n\ndef adding_problem_generator(N, seq_len=6, high=1, delta=0.6):\n    actual_seq_len = np.random.randint(\n        int(seq_len * (1 - delta)), int(seq_len * (1 + delta))\n    ) if delta &gt; 0 else seq_len\n    num_ones = np.random.randint(2, min(actual_seq_len - 1, 4))\n    X_num = np.random.randint(low=0, high=high, size=(N, actual_seq_len, 1))\n    X_mask = np.zeros((N, actual_seq_len, 1))\n    Y = np.ones((N, 1))\n    for i in range(N):\n        positions = np.random.choice(actual_seq_len, size=num_ones, replace=False)\n        X_mask[i, positions] = 1\n        Y[i, 0] = np.sum(X_num[i, positions])\n    X = np.append(X_num, X_mask, axis=2)\n    return X, Y\n\nX, Y = adding_problem_generator(N_SAMPLES, seq_len=SEQ_LEN, high=HIGH, delta=DELTA)\n\ntraining_len = int(TRAIN_SPLIT * N_SAMPLES)\ntrain_X = X[:training_len]\ntest_X = X[training_len:]\ntrain_Y = Y[:training_len]\ntest_Y = Y[training_len:]\n\ntrain_dataset = TensorDataset(\n    torch.tensor(train_X).float(), torch.tensor(train_Y).float()\n)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\ntest_dataset = TensorDataset(\n    torch.tensor(test_X).float(), torch.tensor(test_Y).float()\n)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
  },
  {
    "objectID": "projects/adding-problem-part1.html#model-definition-and-training",
    "href": "projects/adding-problem-part1.html#model-definition-and-training",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 1",
    "section": "3 Model Definition and Training",
    "text": "3 Model Definition and Training\nI defined my GRU model, which consists of a single GRU layer followed by a linear layer. I used orthogonal weight initialization for the GRU and Xavier initialization for the linear layer, following best practices for RNN training. I also defined the evaluation function and the main training loop. Crucially, I added code to store the model‚Äôs weights at different epochs. This allows me to analyze their evolution later.\n\n\nCode\nclass AddingProblemGRU(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(AddingProblemGRU, self).__init__()\n        self.gru = nn.GRU(\n            input_size, hidden_size, num_layers=1, batch_first=True\n        )\n        self.linear = nn.Linear(hidden_size, output_size)\n        self.init_weights()\n\n    def init_weights(self):\n        for name, param in self.gru.named_parameters():\n            if \"weight\" in name:\n                nn.init.orthogonal_(param)\n            elif \"bias\" in name:\n                nn.init.constant_(param, 0)\n        nn.init.xavier_uniform_(self.linear.weight)\n        nn.init.zeros_(self.linear.bias)\n\n    def forward(self, x):\n        out, hn = self.gru(x)\n        output = self.linear(out[:, -1, :])\n        return output, out\n\n\n\n\nCode\nimport json\n\n# File paths for saved data\ntrain_losses_path = \"train_losses.json\"\ntest_losses_path = \"test_losses.json\"\nall_weights_path = \"all_weights.json\"\nmodel_save_path = (\n    f\"gru_adding_problem_model_epochs_{NUM_EPOCHS}_hidden_{HIDDEN_SIZE}.pth\"\n)\n\nFORCE_TRAIN = False\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ncriterion = nn.MSELoss()\ndef evaluate(model, data_loader, criterion, high):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for inputs, labels in data_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            inputs[:, :, 0] /= high\n            outputs, _ = model(inputs)\n            outputs = outputs * high\n            loss = criterion(outputs, labels)\n            total_loss += loss.item() * inputs.size(0)\n    return total_loss / len(data_loader.dataset)\n\n\n# Try to load data from files\nif FORCE_TRAIN == False and os.path.exists(train_losses_path) and os.path.exists(test_losses_path) and os.path.exists(all_weights_path) and os.path.exists(model_save_path):\n    with open(train_losses_path, \"r\") as f:\n        train_losses = json.load(f)\n    with open(test_losses_path, \"r\") as f:\n        test_losses = json.load(f)\n    with open(all_weights_path, \"r\") as f:\n        all_weights_loaded = json.load(f)\n\n    # Convert loaded weights (which are lists) back to numpy arrays\n    all_weights = []\n    for epoch_weights_list in all_weights_loaded:\n        epoch_weights_dict = {}\n        for name, weights_list in epoch_weights_list.items():\n            epoch_weights_dict[name] = np.array(weights_list)\n        all_weights.append(epoch_weights_dict)\n    #load model\n    model = AddingProblemGRU(\n    input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, output_size=OUTPUT_SIZE)\n    model.load_state_dict(torch.load(model_save_path))\n    model.to(device)\n\n\n\nelse:\n    model = AddingProblemGRU(\n        input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, output_size=OUTPUT_SIZE\n    )\n    model.to(device)\n\n    optimizer = torch.optim.Adam(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode=\"min\", factor=0.5, patience=5, min_lr=1e-6, verbose=False\n    )\n\n\n    \n\n    train_losses = []\n    test_losses = []\n    all_weights = []\n\n    for epoch in trange(NUM_EPOCHS, desc=\"Epoch\"):\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            inputs[:, :, 0] /= HIGH\n            labels_scaled = labels / HIGH\n            optimizer.zero_grad()\n            outputs, _ = model(inputs)\n            loss = criterion(outputs, labels_scaled)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_VALUE)\n            optimizer.step()\n            running_loss += loss.item() * inputs.size(0)\n\n        epoch_loss = running_loss / len(train_loader.dataset)\n        train_losses.append(epoch_loss)\n\n        if epoch % 49 == 0:\n            test_loss = evaluate(model, test_loader, criterion, HIGH)\n            test_losses.append(test_loss)\n            scheduler.step(test_loss)\n\n            weights_dict = {}\n            for name, param in model.named_parameters():\n                weights_dict[name] = param.data.cpu().numpy().copy()\n            all_weights.append(weights_dict)\n        else:\n            test_losses.append(None)\n\n    # Save data to files\n    with open(train_losses_path, \"w\") as f:\n        json.dump(train_losses, f)\n    with open(test_losses_path, \"w\") as f:\n        json.dump(test_losses, f)\n    # Convert weights to lists for JSON serialization\n    all_weights_serializable = [\n        {k: v.tolist() for k, v in epoch_weights.items()}\n        for epoch_weights in all_weights\n    ]\n    with open(all_weights_path, \"w\") as f:\n        json.dump(all_weights_serializable, f)\n\n    # Save Model\n    model_save_path = (\n        f\"gru_adding_problem_model_epochs_{NUM_EPOCHS}_hidden_{HIDDEN_SIZE}.pth\"\n    )\n    torch.save(model.state_dict(), model_save_path)"
  },
  {
    "objectID": "projects/adding-problem-part1.html#training-metrics-visualization",
    "href": "projects/adding-problem-part1.html#training-metrics-visualization",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 1",
    "section": "4 Training Metrics Visualization",
    "text": "4 Training Metrics Visualization\nThe following two plots show the training and validation loss curves. These provide a visual indication of how well the model is learning and whether it‚Äôs overfitting (which would be indicated by the training loss continuing to decrease while the validation loss starts to increase). In my case, the training loss closely follows the validation loss, suggesting that I likely reached a global minimum.\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.plot(train_losses, label=\"Training Loss\")\n# plt.plot(range(0,NUM_EPOCHS,50), [x for x in test_losses if x is not None], label=\"Validation Loss\") #plot only not none values\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"MSE Loss\")\nplt.ylim(0, max(train_losses)*1.05)\nplt.title(\"Training and Validation Loss over Epochs\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\nplt.figure(figsize=(10, 6))\n# plt.plot(train_losses, label=\"Training Loss\")\nplt.plot(range(0,NUM_EPOCHS+1,50), [x for x in test_losses if x is not None][0:len(range(0,NUM_EPOCHS+1,50))], label=\"Validation Loss\") #plot only not none values\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"MSE Loss\")\nplt.title(\"Training and Validation Loss over Epochs\")\nplt.ylim(0, max([x for x in test_losses if x is not None])*1.05)\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\nTraining and Validation Loss\n\n\n\n\n\n\n\n\n\n\n\nIt‚Äôs important to note that if the model had overfit the data, then the weight analysis would have been nearly useless."
  },
  {
    "objectID": "projects/adding-problem-part1.html#weight-evolution-analysis",
    "href": "projects/adding-problem-part1.html#weight-evolution-analysis",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 1",
    "section": "5 Weight Evolution Analysis",
    "text": "5 Weight Evolution Analysis\nThis is the core of my analysis. I examine how the model‚Äôs weights change over the course of training. Because my GRU has a small number of parameters, I can visualize this evolution effectively. The PyTorch documentation helps us understand the names of the involved parameters:\n\nweight_ih_l[k] - the learnable input-hidden weights of the \\(k^{th}\\) layer (\\(W_{ir}\\)|\\(W_{iz}\\)|\\(W_{in}\\)), of shape \\((3 * \\text{hidden\\_size}, \\text{input\\_size})\\) for \\(k=0\\). In my example, with \\(\\text{hidden\\_size} = 1\\) and \\(\\text{input\\_size} = 2\\), this is \\((3*1, 2)\\), or \\((3, 2)\\).\nweight_hh_l[k] - the learnable hidden-hidden weights of the \\(k^{th}\\) layer (\\(W_{hr}\\)|\\(W_{hz}\\)|\\(W_{hn}\\)), of shape \\((3 * \\text{hidden\\_size}, \\text{hidden\\_size})\\). In my example, with \\(\\text{hidden\\_size} = 1\\), this is \\((3*1, 1)\\), or \\((3, 1)\\).\nbias_ih_l[k] - the learnable input-hidden bias of the \\(k^{th}\\) layer (\\(b_{ir}\\)|\\(b_{iz}\\)|\\(b_{in}\\)), of shape \\((3 * \\text{hidden\\_size})\\). In my example, with \\(\\text{hidden\\_size} = 1\\), this is \\((3*1)\\), or \\((3)\\).\nbias_hh_l[k] - the learnable hidden-hidden bias of the \\(k^{th}\\) layer (\\(b_{hr}\\)|\\(b_{hz}\\)|\\(b_{hn}\\)), of shape \\((3 * \\text{hidden\\_size})\\). In my example, with \\(\\text{hidden\\_size} = 1\\), this is \\((3*1)\\), or \\((3)\\).\n\nI focused on the key weight matrices:\n\ngru.weight_ih_l0: Input-to-hidden weights. In my example, this has size \\((3 \\times 2)\\).\ngru.weight_hh_l0: Hidden-to-hidden weights. In my example, this has size \\((3 \\times 1)\\).\nlinear.weight: Weights of the final linear layer. In my example, with \\(\\text{hidden\\_size} = 1\\) and \\(\\text{output\\_size} = 1\\), this has size \\((1 \\times 1)\\). I included this to allow for potentially adding an additional dimension to the hidden layer later.\n\nNow, here‚Äôs how GRUs work (it might look a little intimidating at first, but it‚Äôs actually quite straightforward):\nInitially, for \\(t = 0\\), the output vector is \\(h_0 = 0\\).\n\\[\\begin{aligned}\nz_t &= \\sigma(W_z x_t + U_z h_{t-1} + b_z) \\\\\nr_t &= \\sigma(W_r x_t + U_r h_{t-1} + b_r) \\\\\n\\hat{h}_t &= \\phi(W_h x_t + U_h (r_t \\odot h_{t-1}) + b_h) \\\\\nh_t &= (1 - z_t) \\odot h_{t-1} + z_t \\odot \\hat{h}_t\n\\end{aligned}\\]\nThe \\(W_{*}\\) weights are the first set of weights (i.e., gru.weight_ih_l0), while the \\(U_{*}\\) weights are the second set of weights (i.e., gru.weight_hh_l0). The bias terms are straightforward, so I won‚Äôt go into detail.\nI created plots showing the values of these weights at different stages of training. This visualization helps understand how the GRU learns to solve the adding problem. I used a heatmap to represent the weight matrices, making it easy to spot patterns and changes.\n\n\nCode\n# Select a subset of epochs for clearer visualization\nepochs_to_plot = [0, len(all_weights) // 4, len(all_weights) // 2, len(all_weights) - 1]\nselected_weights = [all_weights[i] for i in epochs_to_plot]\n\n# Plotting function for a single weight matrix\ndef plot_weight_matrix(weight_matrix, title, ax):\n    sns.heatmap(weight_matrix, annot=True, fmt=\".2f\", cmap=\"viridis\", cbar=True, ax=ax)\n    ax.set_title(title)\n\nfig, axes = plt.subplots(len(selected_weights), 3, figsize=(15, 5 * len(selected_weights)))\n\nfor i, epoch_weights in enumerate(selected_weights):\n  plot_weight_matrix(epoch_weights['gru.weight_ih_l0'], f'Epoch {epochs_to_plot[i]*50}: Input-to-Hidden', axes[i, 0])\n  plot_weight_matrix(epoch_weights['gru.weight_hh_l0'], f'Epoch {epochs_to_plot[i]*50}: Hidden-to-Hidden', axes[i, 1])\n  plot_weight_matrix(epoch_weights['linear.weight'], f'Epoch {epochs_to_plot[i]*50}: Linear Layer', axes[i, 2])\n\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nEvolution of GRU + Linear Weights"
  },
  {
    "objectID": "projects/adding-problem-part1.html#weight-dynamics-and-learned-strategy",
    "href": "projects/adding-problem-part1.html#weight-dynamics-and-learned-strategy",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 1",
    "section": "6 Weight Dynamics and Learned Strategy",
    "text": "6 Weight Dynamics and Learned Strategy\nThis section combines the detailed weight evolution visualizations (heatmaps and line plots) with the analysis of the GRU‚Äôs learned strategy. This provides a single, cohesive narrative.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Assuming all_weights is already loaded\n\nweight_types = ['gru.weight_ih_l0', 'gru.weight_hh_l0', 'linear.weight']\nweight_type_names = ['Input-to-Hidden', 'Hidden-to-Hidden', 'Linear Layer']\n\nfor i, weight_type in enumerate(weight_types):\n    evolution_data = []\n    for epoch_weights in all_weights:\n        weight_matrix = epoch_weights[weight_type]\n        if weight_type == 'gru.weight_ih_l0':\n            evolution_data.append(weight_matrix.flatten(order='F'))\n        else:\n            evolution_data.append(weight_matrix.flatten())\n\n    evolution_heatmap_data = np.array(evolution_data)\n\n    fig, ax = plt.subplots(figsize=(10, 8))\n    sns.heatmap(evolution_heatmap_data, cmap=\"viridis\", cbar=True, ax=ax, xticklabels=False)\n\n    ax.set_title(f'Heatmap of {weight_type_names[i]} Weights Over Epochs')\n    ax.set_ylabel('Epoch (x49)')\n\n    ax.axhline(2, color='#EE4B2B', linestyle='--', linewidth=2)\n    ax.axhline(10, color='#EE4B2B', linestyle='--', linewidth=2)\n    ax.axhline(15, color='#EE4B2B', linestyle='--', linewidth=2)\n\n\n    if weight_type == 'gru.weight_ih_l0':\n        ax.axvline(3, color='#EE4B2B')\n        ax.text(x=0.1, y=60, s=\"Input[0]\", color='#EE4B2B', fontsize=8,\n            ha='left', va='bottom', bbox=dict(facecolor='black', alpha=0.3))\n        ax.text(x=3.1, y=60, s=\"Input[1]\", color='#EE4B2B', fontsize=8,\n                ha='left', va='bottom', bbox=dict(facecolor='black', alpha=0.3))\n\n\n\n    ax.text(x=0.1, y=2-.2, s=\"Epoch 100\", color='#EE4B2B', fontsize=8,\n        ha='left', va='bottom')\n    ax.text(x=0.1, y=10-.2, s=\"Epoch 500\", color='#EE4B2B', fontsize=8,\n            ha='left', va='bottom')\n    ax.text(x=0.1, y=15-.2, s=\"Epoch 750\", color='#EE4B2B', fontsize=8,\n        ha='left', va='bottom')\n\n\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\nCombined Heatmap of Weight Evolution Over Epochs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo visualize weight changes directly, I used line plots to show the evolution of individual weights across sampled epochs. These plots complement the heatmaps by providing a more detailed view of how each weight changes in magnitude and direction. Plotting signed values reveals symmetries and patterns that emerge during training, even in a simple network like mine.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np  # Make sure numpy is imported\n\ndef plot_gru_weight_evolution(all_weights, title_prefix):\n    \"\"\"\n    Plots the evolution of GRU weight matrices (input-hidden, hidden-hidden)\n    separated by gate (Reset, Update, New).\n    \"\"\"\n\n    def plot_single_gate_weights(weight_values, gate_name, title):\n        \"\"\"Helper function to plot weights for a single gate.\"\"\"\n        df = pd.DataFrame(weight_values).T\n        df.columns = [f\"Epoch {i*49}\" for i in range(len(all_weights))]\n\n        plt.figure(figsize=(12, 6))\n        for i in range(df.shape[0]):\n            plt.plot(df.columns, df.iloc[i], label=f\"{gate_name} Weight {i}\")  # Add weight index to label\n\n        plt.title(title)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Weight Value\")\n        plt.xticks(rotation=90)\n        plt.legend()\n        plt.grid(True)\n        plt.axvline(x=\"Epoch 98\")\n        plt.axvline(x=\"Epoch 490\")\n        plt.axvline(x=\"Epoch 735\")\n        plt.tight_layout()\n        plt.show()\n\n    # --- Input-to-Hidden Weights ---\n    ih_weights = [all_weights[i][\"gru.weight_ih_l0\"] for i in range(len(all_weights))]\n    # Split into Reset, Update, and New gate weights (assuming standard GRU structure)\n    ih_reset_weights = [w[0:w.shape[0]//3, :].flatten() for w in ih_weights]\n    ih_update_weights = [w[w.shape[0]//3:2*w.shape[0]//3, :].flatten() for w in ih_weights]\n    ih_new_weights = [w[2*w.shape[0]//3:, :].flatten() for w in ih_weights]\n\n    plot_single_gate_weights(ih_reset_weights, \"Reset\", f\"{title_prefix} - Input-Hidden (Reset Gate)\")\n    plot_single_gate_weights(ih_update_weights, \"Update\", f\"{title_prefix} - Input-Hidden (Update Gate)\")\n    plot_single_gate_weights(ih_new_weights, \"New\", f\"{title_prefix} - Input-Hidden (New Gate)\")\n\n    # --- Hidden-to-Hidden Weights ---\n    hh_weights = [all_weights[i][\"gru.weight_hh_l0\"] for i in range(len(all_weights))]\n    # Split into Reset, Update, and New gate weights\n    hh_reset_weights = [w[0:w.shape[0]//3, :].flatten() for w in hh_weights]\n    hh_update_weights = [w[w.shape[0]//3:2*w.shape[0]//3, :].flatten() for w in hh_weights]\n    hh_new_weights = [w[2*w.shape[0]//3:, :].flatten() for w in hh_weights]\n\n    plot_single_gate_weights(hh_reset_weights, \"Reset\", f\"{title_prefix} - Hidden-Hidden (Reset Gate)\")\n    plot_single_gate_weights(hh_update_weights, \"Update\", f\"{title_prefix} - Hidden-Hidden (Update Gate)\")\n    plot_single_gate_weights(hh_new_weights, \"New\", f\"{title_prefix} - Hidden-Hidden (New Gate)\")\n\n\n\ndef plot_linear_weight_evolution(all_weights, title):\n    \"\"\"Plots the evolution of the linear layer weights.\"\"\"\n    weight_values = [all_weights[i][\"linear.weight\"].flatten() for i in range(len(all_weights))]\n\n    df = pd.DataFrame(weight_values).T\n    df.columns = [f\"Epoch {i*49}\" for i in range(len(all_weights))]\n\n    plt.figure(figsize=(12, 6))\n    for i in range(df.shape[0]):\n        plt.plot(df.columns, df.iloc[i], label=f\"Linear Weight {i}\")\n\n    plt.title(title)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Weight Value\")\n    plt.xticks(rotation=90)\n    plt.legend()\n    plt.grid(True)\n    plt.axvline(x=\"Epoch 98\")\n    plt.axvline(x=\"Epoch 490\")\n    plt.axvline(x=\"Epoch 735\")\n    plt.tight_layout()\n    plt.show()\n\n\n# Create the plots\nplot_gru_weight_evolution(all_weights, \"GRU Weight Evolution\")\nplot_linear_weight_evolution(all_weights, \"Linear Layer Weight Evolution\")\n\n\n\n\n\nWeight Dynamics Over Training"
  },
  {
    "objectID": "projects/adding-problem-part1.html#illustrative-example-forward-pass",
    "href": "projects/adding-problem-part1.html#illustrative-example-forward-pass",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 1",
    "section": "7 Illustrative Example: Forward Pass",
    "text": "7 Illustrative Example: Forward Pass\nTo further clarify how the trained GRU operates, I‚Äôll walk through a single forward pass with a concrete example. This will demonstrate the roles of the input-to-hidden weights, hidden-to-hidden weights, and the final linear layer, as well as how the binary mask selects the relevant input values.\nExample Input:\nThe input sequence is represented as a tensor:\n\\[\n\\text{input\\_sequence} =\n\\begin{bmatrix}\n12 & 0 \\\\\n37 & 1 \\\\\n12 & 0 \\\\\n21 & 1\n\\end{bmatrix}\n\\]\nThe problem requires summing the values in the first dimension (column) only when the corresponding value in the second dimension (column) is 1 (the flag). In this example:\n\nThe second element, \\([37, 1]\\), has a flag of 1, so 37 is included in the sum.\nThe fourth element, \\([21, 1]\\), has a flag of 1, so 21 is included in the sum.\nThe first and third elements have flags of 0, so their first-dimension values (12 and 12) are excluded.\n\nTherefore, the expected output is \\(37 + 21 = 58\\).\n\n\nCode\ninput_sequence = torch.tensor([\n    [12,0],\n    [37,1],\n    [12,0],\n    [21,1],\n]).float().unsqueeze(0)\n\n\ninput_sequence[:, :, 0] /= HIGH\n\n\nAnd a quick sanity check applying my model. The result is: \\(57.80980587005615\\approx58\\)\nwhich is really close to the actual sum (\\(37 + 21 = 58\\))!!!!\nLoaded Weights (from the last training epoch):\nI used the weights from the last epoch of training, which I previously saved. This ensures that I‚Äôm using the fully trained model. Let‚Äôs get the weights:\n\n\nCode\n# Get weights from the last training epoch.  'all_weights' is populated by the loading/training section.\nlast_epoch_weights = model.state_dict() #all_weights[-1]\n\n# Extract the relevant weight matrices\nW_ih = torch.tensor(last_epoch_weights['gru.weight_ih_l0']).float()  # Input-to-hidden\nW_hh = torch.tensor(last_epoch_weights['gru.weight_hh_l0']).float()  # Hidden-to-hidden\nb_ih = torch.tensor(last_epoch_weights['gru.bias_ih_l0']).float()  # Input-to-hidden bias\nb_hh = torch.tensor(last_epoch_weights['gru.bias_hh_l0']).float()  # Hidden-to-hidden bias\nW_linear = torch.tensor(last_epoch_weights['linear.weight']).float() # Linear layer weights\nb_linear = torch.tensor(last_epoch_weights['linear.bias']).float()   # Linear layer bias\n\n\nNow, let‚Äôs walk through the GRU step-by-step for each element in our input sequence.\nFor each element, I calculate the Update Gate (\\(z_t\\)), Reset Gate (\\(r_t\\)), New Gate (\\(\\tilde{h}_t\\)), and Hidden State (\\(h_t\\)).\nI start by initializing the hidden state to zero:\n\n\nCode\nh_t = torch.zeros(1, 1, 1) # Initialize hidden state\n\n\n\\[\nh_t = 0.0\n\\]\n\n7.1 Step 1: Input \\([12, 0]\\)\nInput for Step 1: \\[\nx_1 = \\begin{bmatrix} 0.12 & 0 \\end{bmatrix}\n\\]\n\nUpdate Gate \\(z_1\\): \\[\nz_1 = \\sigma(x_1 W_{iz}^T + h_0 W_{hz}^T + b_{iz})\n\\]\n\n\nCode\nx_1 = input_sequence[:, 0, :]\nh = torch.zeros(1, 1, 1)\n\n# Stable version (using torch.matmul)\ngi_stable = torch.matmul(x_1, W_ih.t()) + b_ih\ngh_stable = torch.matmul(h, W_hh.t()) + b_hh\n\n# Get gates\ni_r, i_z, i_n = gi_stable.chunk(3, dim=1)\nh_r, h_z, h_n = gh_stable.chunk(3, dim=2)\n\n# Apply gate operations with controlled precision\nresetgate = torch.sigmoid(i_r + h_r)\nupdategate = torch.sigmoid(i_z + h_z)\nnewgate = torch.tanh(i_n + (resetgate * h_n))\n\nhy = (1 - updategate) * newgate + updategate * h\nh_t = hy\n\n\nUpdate Gate: \\[\nz_1 = 0.8275274634361267\n\\]\nReset Gate \\(r_1\\): \\[\nr_1 = \\sigma(x_1 W_{ir}^T + h_0 W_{hr}^T + b_{ir})\n\\]\nReset Gate: \\[\nr_1 = 0.819507896900177\n\\]\nNew Gate \\(\\tilde{h}_1\\): \\[\n\\tilde{h}_1 = \\tanh(x_1 W_{in}^T + (r_1 \\odot h_0) W_{hn}^T + b_{in})\n\\]\nNew Gate: \\[\n\\tilde{h}_1 = 0.639545738697052\n\\]\nHidden State \\(h_1\\): \\[\nh_1 = (1 - z_1) \\odot h_0 + z_1 \\odot \\tilde{h}_1\n\\]\nHidden State: \\[\nh_1 = 0.11030407249927521\n\\]\n\n\n\n7.2 Step 2: Input \\([37, 1]\\)\nInput for Step 2: \\[\nx_2 = \\begin{bmatrix} 0.37 & 1 \\end{bmatrix}\n\\]\n\nUpdate Gate \\(z_2\\): \\[\nz_2 = \\sigma(x_2 W_{iz}^T + h_1 W_{hz}^T + b_{iz})\n\\]\n\n\nCode\nx_2 = input_sequence[:, 1, :]\n\n# Stable version (using torch.matmul)\ngi_stable = torch.matmul(x_2, W_ih.t()) + b_ih\ngh_stable = torch.matmul(h_t, W_hh.t()) + b_hh\n\n# Get gates\ni_r, i_z, i_n = gi_stable.chunk(3, dim=1)\nh_r, h_z, h_n = gh_stable.chunk(3, dim=2)\n\n# Apply gate operations\nresetgate2 = torch.sigmoid(i_r + h_r)\nupdategate2 = torch.sigmoid(i_z + h_z)\nnewgate2 = torch.tanh(i_n + (resetgate2 * h_n))\n\nhy = (1 - updategate2) * newgate2 + updategate2 * h_t\nh_t = hy\n\n\nUpdate Gate: \\[\nz_2 = 0.15969596803188324\n\\]\nReset Gate \\(r_2\\): \\[\nr_2 = \\sigma(x_2 W_{ir}^T + h_1 W_{hr}^T + b_{ir})\n\\]\nReset Gate: \\[\nr_2 = 0.89918053150177\n\\]\nNew Gate \\(\\tilde{h}_2\\): \\[\n\\tilde{h}_2 = \\tanh(x_2 W_{in}^T + (r_2 \\odot h_1) W_{hn}^T + b_{in})\n\\]\nNew Gate: \\[\n\\tilde{h}_2 = -0.001833615591749549\n\\]\nHidden State \\(h_2\\): \\[\nh_2 = (1 - z_2) \\odot h_1 + z_2 \\odot \\tilde{h}_2\n\\]\nHidden State: \\[\nh_2 = 0.016074320301413536\n\\]\n\n\n\n7.3 Step 3: Input \\([12, 0]\\)\nInput for Step 3: \\[\nx_3 = \\begin{bmatrix} 0.12 & 0 \\end{bmatrix}\n\\]\n\nUpdate Gate \\(z_3\\): \\[\nz_3 = \\sigma(x_3 W_{iz}^T + h_2 W_{hz}^T + b_{iz})\n\\]\n\n\nCode\nx_3 = input_sequence[:, 2, :]\n\n# Stable version (using torch.matmul)\ngi_stable = torch.matmul(x_3, W_ih.t()) + b_ih\ngh_stable = torch.matmul(h_t, W_hh.t()) + b_hh\n\n# Get gates\ni_r, i_z, i_n = gi_stable.chunk(3, dim=1)\nh_r, h_z, h_n = gh_stable.chunk(3, dim=2)\n\n# Apply gate operations\nresetgate3 = torch.sigmoid(i_r + h_r)\nupdategate3 = torch.sigmoid(i_z + h_z)\nnewgate3 = torch.tanh(i_n + (resetgate3 * h_n))\n\nhy = (1 - updategate3) * newgate3 + updategate3 * h_t\nh_t = hy\n\n\nUpdate Gate: \\[\nz_3 = 0.8254608511924744\n\\]\nReset Gate \\(r_3\\): \\[\nr_3 = \\sigma(x_3 W_{ir}^T + h_2 W_{hr}^T + b_{ir})\n\\]\nReset Gate: \\[\nr_3 = 0.8191711902618408\n\\]\nNew Gate \\(\\tilde{h}_3\\): \\[\n\\tilde{h}_3 = \\tanh(x_3 W_{in}^T + (r_3 \\odot h_2) W_{hn}^T + b_{in})\n\\]\nNew Gate: \\[\n\\tilde{h}_3 = 0.648215115070343\n\\]\nHidden State \\(h_3\\): \\[\nh_3 = (1 - z_3) \\odot h_2 + z_3 \\odot \\tilde{h}_3\n\\]\nHidden State: \\[\nh_3 = 0.12640763819217682\n\\]\n\n\n\n7.4 Step 4: Input \\([21, 1]\\)\nInput for Step 4: \\[\nx_4 = \\begin{bmatrix} 0.21 & 1 \\end{bmatrix}\n\\]\n\nUpdate Gate \\(z_4\\): \\[\nz_4 = \\sigma(x_4 W_{iz}^T + h_3 W_{hz}^T + b_{iz})\n\\]\n\n\nCode\nx_4 = input_sequence[:, 3, :]\n\n\n# Stable version (using torch.matmul)\ngi_stable = torch.matmul(x_4, W_ih.t()) + b_ih\ngh_stable = torch.matmul(h_t, W_hh.t()) + b_hh\n\n# Get gates\ni_r, i_z, i_n = gi_stable.chunk(3, dim=1)\nh_r, h_z, h_n = gh_stable.chunk(3, dim=2)\n\n# Apply gate operations\nresetgate4 = torch.sigmoid(i_r + h_r)\nupdategate4 = torch.sigmoid(i_z + h_z)\nnewgate4 = torch.tanh(i_n + (resetgate4 * h_n))\n\nhy = (1 - updategate4) * newgate4 + updategate4 * h_t\nh_t = hy\n\n\nUpdate Gate: \\[\nz_4 = 0.1668987274169922\n\\]\nReset Gate \\(r_4\\): \\[\nr_4 = \\sigma(x_4 W_{ir}^T + h_3 W_{hr}^T + b_{ir})\n\\]\nReset Gate: \\[\nr_4 = 0.8955692052841187\n\\]\nNew Gate \\(\\tilde{h}_4\\): \\[\n\\tilde{h}_4 = \\tanh(x_4 W_{in}^T + (r_4 \\odot h_3) W_{hn}^T + b_{in})\n\\]\nNew Gate: \\[\n\\tilde{h}_4 = 0.05876209959387779\n\\]\nHidden State \\(h_4\\): \\[\nh_4 = (1 - z_4) \\odot h_3 + z_4 \\odot \\tilde{h}_4\n\\]\nHidden State: \\[\nh_4 = 0.07005205750465393\n\\]\n\n\n\n7.5 Step 5: Linear Layer\nFinally, the hidden state \\(h_4\\) is passed through the linear layer to produce the output.\n\\[\n\\text{output} = h_4 W_{\\text{linear}}^T + b_{\\text{linear}}\n\\]\n\n\nCode\noutput = h_t @ W_linear.T + b_linear\n\n\nLinear Layer output: \\[\n\\text{output} = 57.8097939491272\n\\]\nThe final result, as we can see, when approximated using the ceiling function, matches the expected result of 58."
  },
  {
    "objectID": "projects/adding-problem-part1.html#coming-next",
    "href": "projects/adding-problem-part1.html#coming-next",
    "title": "Analyzing GRU Training Dynamics on the Adding Problem - Part 1",
    "section": "8 Coming next‚Ä¶",
    "text": "8 Coming next‚Ä¶\nThis post set some ground work for a more thorough analysis of the model‚Äôs weights. I‚Äôve trained a simple model, and now it‚Äôs time to crack it open and see what makes it tick. Think of it like this: usually these models are black boxes, and the weights that are inside and make them move or somwhat foggy to us. I‚Äôm going to be taking a close look at what those gears and levers look like after all the training is done ‚Äì are they big or small? Are they organized in any particular way? What do they do?\nBut it‚Äôs not enough just to look at them. I want to figure out what they actually do. Each weight contributes, in some way, to the final output of the model. I‚Äôll be exploring how different weights, or groups of weights, affect the model‚Äôs predictions. I might even try tweaking them a bit to see what happens! This is like figuring out which lever controls which part of the machine.\nAnd finally, the really fun part: I‚Äôll try to guess how they got that way. The training process is like a long and winding road, and the weights are constantly changing along the way. I‚Äôll try to reconstruct that journey, piecing together clues from the final weights to understand the path the model took to learn. It‚Äôs a bit like detective work, trying to figure out the story behind the final result and I just love that! Get ready for some serious model investigation!"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Research Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nFeb 18, 2025\n\n\nAnalyzing GRU Training Dynamics on the Adding Problem - Part 1\n\n\nYour Name\n\n\n\n\nFeb 20, 2025\n\n\nAnalyzing GRU Training Dynamics on the Adding Problem - Part 2\n\n\nLuca Simonetti\n\n\n\n\nJan 30, 2025\n\n\nConnect 4\n\n\nLuca Simonetti\n\n\n\n\n¬†\n\n\nHopfield Network Demo\n\n\n¬†\n\n\n\n\nFeb 3, 2025\n\n\nOf RNNs: Ising, Hopfield, Math and Modernity\n\n\n¬†\n\n\n\n\nJan 30, 2025\n\n\nSpatial Spike Neural Networks\n\n\nLuca Simonetti\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/hopfield.html#hopfield-network",
    "href": "projects/hopfield.html#hopfield-network",
    "title": "Hopfield Network Demo",
    "section": "Hopfield Network",
    "text": "Hopfield Network"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Luca Simonetti, yet another idiot",
    "section": "",
    "text": "Luca Simonetti\n\n\nFeb 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLuca Simonetti\n\n\nFeb 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLuca Simonetti\n\n\nFeb 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLuca Simonetti\n\n\nJan 30, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#blog",
    "href": "index.html#blog",
    "title": "Luca Simonetti, yet another idiot",
    "section": "",
    "text": "Luca Simonetti\n\n\nFeb 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLuca Simonetti\n\n\nFeb 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLuca Simonetti\n\n\nFeb 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLuca Simonetti\n\n\nJan 30, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Luca Simonetti, yet another idiot",
    "section": "Projects",
    "text": "Projects\n\n\n\n\n\n\n\n\n\n\nAnalyzing GRU Training Dynamics on the Adding Problem - Part 1\n\n\n\n\n\n\nYour Name\n\n\nFeb 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing GRU Training Dynamics on the Adding Problem - Part 2\n\n\n\n\n\n\nLuca Simonetti\n\n\nFeb 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConnect 4\n\n\n\n\n\n\nLuca Simonetti\n\n\nJan 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHopfield Network Demo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOf RNNs: Ising, Hopfield, Math and Modernity\n\n\n\n\n\n\n\n\n\nFeb 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial Spike Neural Networks\n\n\n\n\n\n\nLuca Simonetti\n\n\nJan 30, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/group-prime-order.html",
    "href": "blog/group-prime-order.html",
    "title": "Gruppi di Ordine Primo",
    "section": "",
    "text": "Una volta parlavo con un mio amico di Algebra astratta e ricordo che mi chiese se davvero esistesse qualcosa come l‚Äôalgebra astratta. Secondo lui l‚Äôaggettivo astratta era ridondante, visto che considerava l‚Äôalgebra di per s√© gi√† astratta. No.¬†Non √® cos√¨. Dovete sapere che ai matematici piace astrarre qualsiasi cosa. Ma c‚Äô√® un motivo se √® cos√¨ ed √® per dare delle definizioni formali, non assiomatiche di operazioni comuni per permettere (in algebre diverse da quelle che usiamo comunemente) di poter definire operazioni diverse ma con propriet√† simili.\n\n‚ÄúCazzo vuol dire? Parla potabile Luca.‚Äù\n\nC‚Äôhai ragione, calmati.\nA volte fa comodo sapere che gli oggeti matematici con cui lavoriamo hanno delle propriet√† definite, dimostrate da teoremi.\nPerch√©? Perch√© cos√¨ si possono creare approcci nuovi, algoritmi, schemi e compagnia cantante senza dover dimostrare la loro fondatezza dalle basi, ma assumendo per esempio di partire con oggetti matematici noti.\nUn esempio di questo genere di oggetti matematici sono i gruppi. Esiste tutto un filone di matematica chiamato Teoria dei gruppi che si occupa di dimostrare, scoprire e studiare tutto ci√≤ che riguarda i Gruppi.\nOk ma che caz sono ‚Äôsti benedetti gruppi? E perch√© ci servono?\nCalmati, di nuovo.\nPrima di arrivare al cuore dell‚Äôargomento di questo post, i gruppi di ordine primo, conviene costruire prima una base, partendo dalla definizione stessa di gruppo.\nUn gruppo √® un insieme non vuoto, che possiamo chiamare come ci pare, ma chiamaremo \\(G\\) (che sta per \\(Giancarlo\\)), dotato di un‚Äôoperazione binaria, che spesso indicheremo con \\(\\cdot\\) (ma potrebbe essere +, *, o altro a seconda del contesto o di come vi svegliate la mattina. E i matematici lo fanno.). Per essere un gruppo, questa coppia \\((G, \\cdot)\\) deve soddisfare quattro propriet√† fondamentali, i cosiddetti assiomi di gruppo:\n\nChiusura: Per ogni coppia di elementi \\(a\\) e \\(b\\) appartenenti a \\(G\\), il risultato dell‚Äôoperazione \\(a \\cdot b\\) deve essere ancora un elemento di \\(G\\). In simboli: se \\(a, b \\in G\\), allora \\(a \\cdot b \\in G\\). Questo significa che l‚Äôoperazione non ci ‚Äúporta fuori‚Äù dall‚Äôinsieme \\(G\\).\nAssociativit√†: L‚Äôoperazione deve essere associativa. Questo significa che per ogni terna di elementi \\(a, b, c\\) in \\(G\\), l‚Äôordine in cui eseguiamo le operazioni non cambia il risultato: \\((a \\cdot b) \\cdot c = a \\cdot (b \\cdot c)\\).\nElemento Neutro (Identit√†): Esiste un elemento speciale in \\(G\\), che chiamiamo elemento neutro (o identit√†), solitamente indicato con \\(e\\) (o \\(1\\) in notazione moltiplicativa, \\(0\\) in notazione additiva). Questo elemento ha la propriet√† che, per ogni elemento \\(a\\) in \\(G\\), \\(a \\cdot e = e \\cdot a = a\\). L‚Äôelemento neutro si comporta come un ‚Äúnon-operazione‚Äù.\nElemento Inverso: Per ogni elemento \\(a\\) in \\(G\\), esiste un altro elemento in \\(G\\), chiamato inverso di \\(a\\), che indicheremo con \\(a^{-1}\\) (o \\(-a\\) in notazione additiva). L‚Äôelemento inverso ha la propriet√† che quando operato con \\(a\\) d√† come risultato l‚Äôelemento neutro: \\(a \\cdot a^{-1} = a^{-1} \\cdot a = e\\).\n\nSe un gruppo soddisfa anche la propriet√† commutativa, cio√® se per ogni coppia di elementi \\(a, b \\in G\\), \\(a \\cdot b = b \\cdot a\\), allora il gruppo √® detto gruppo abeliano (o commutativo). Se la propriet√† commutativa non vale per tutti gli elementi, il gruppo √® detto non-abeliano.\n\n\nIo ho gi√† perso per strada met√† dei lettori. Dico met√† perch√© sono un inguaribile ottimista. In realt√† se stai ancora leggendo forse sei l‚Äôunico che ha avuto il fegato di farlo. Quindi bravo.\nCome ricompnensa, vediamo alcuni esempi un po‚Äô pi√π concreti:\n\nI numeri interi con l‚Äôaddizione (\\((\\mathbb{Z}, +)\\)): L‚Äôinsieme dei numeri interi \\(\\mathbb{Z} = \\{..., -2, -1, 0, 1, 2, ...\\}\\) con l‚Äôoperazione di addizione usuale (+) forma un gruppo abeliano.\n\nChiusura: La somma di due numeri interi √® sempre un numero intero. 3 + 19 = 21 dove 3, 19 e 21 \\(\\in \\mathbb{Z}\\)\nAssociativit√†: L‚Äôaddizione √® associativa: \\((a + b) + c = a + (b + c)\\). Roba da seconda elementare, forza, su.\nElemento Neutro: L‚Äôelemento neutro √® lo zero (0), poich√© \\(a + 0 = 0 + a = a\\) per ogni intero \\(a\\).\nElemento Inverso: L‚Äôinverso di un intero \\(a\\) √® \\(-a\\), poich√© \\(a + (-a) = (-a) + a = 0\\).\nAbeliano: L‚Äôaddizione √® commutativa: \\(a + b = b + a\\).\n\nI numeri razionali non nulli con la moltiplicazione (\\((\\mathbb{Q} \\setminus \\{0\\}, \\cdot)\\)): L‚Äôinsieme dei numeri razionali escluso lo zero, con l‚Äôoperazione di moltiplicazione usuale (\\(\\cdot\\)), forma un gruppo abeliano.\n\nChiusura: Il prodotto di due numeri razionali non nulli √® ancora un numero razionale non nullo.\nAssociativit√†: La moltiplicazione √® associativa: \\((a \\cdot b) \\cdot c = a \\cdot (b \\cdot c)\\).\nElemento Neutro: L‚Äôelemento neutro √® uno (1), poich√© \\(a \\cdot 1 = 1 \\cdot a = a\\) per ogni razionale non nullo \\(a\\).\nElemento Inverso: L‚Äôinverso di un razionale non nullo \\(a = \\frac{p}{q}\\) (con \\(p, q \\neq 0\\)) √® \\(a^{-1} = \\frac{q}{p}\\).\nAbeliano: La moltiplicazione √® commutativa: \\(a \\cdot b = b \\cdot a\\).\n\nIl gruppo simmetrico \\(S_n\\) (per \\(n \\ge 3\\)): Questo √® un esempio di gruppo non-abeliano. \\(S_n\\) √® il gruppo delle permutazioni di \\(n\\) oggetti, con l‚Äôoperazione di composizione di funzioni. Per \\(n \\ge 3\\), la composizione di permutazioni non √® commutativa in generale.\n\nQuindi, riassumendo un pochino: un gruppo √® semplicemente l‚Äôastrazione di quello che noi usiamo ogni giorno per contare quanti soldi ci sono rimasti sul conto e scoprire di essere molto povery. Per essere definito un gruppo ha bisogno di due elementi: un insieme (non vuoto altrimenti grazie al cazzo) e un‚Äôoperazione binaria (operazione binaria=tra due operandi). Facile no? Ecco, adesso complichiamo le cose.\n\n\n\nInnanzitutto introduciamo un nuovo termine: l‚Äôordine. L‚Äôordine pu√≤ essere definito sia sul gruppo nella sua interezza, sia solo sull‚Äôelemento.\n\nL‚Äôordine di un gruppo \\(G\\), indicato con \\(|G|\\), non √® altro che il numero di elementi contenuti nell‚Äôinsieme \\(G\\). Se il numero di elementi √® finito, si dice che il gruppo √® finito, altrimenti √® infinito (minchia, la fantasia dei matematici eh!?). Gli esempi \\((\\mathbb{Z}, +)\\) e \\((\\mathbb{Q} \\setminus \\{0\\}, \\cdot)\\) sono gruppi infiniti, mentre il gruppo simmetrico \\(S_n\\) √® un gruppo finito di ordine \\(n!\\).\nL‚Äôordine di un elemento \\(g\\) in un gruppo \\(G\\) √® il pi√π piccolo intero positivo \\(k\\) tale che \\(g^k = e\\), dove \\(e\\) √® l‚Äôelemento neutro del gruppo e \\(g^k\\) indica l‚Äôoperazione di \\(g\\) con se stesso \\(k\\) volte (ad esempio, in notazione moltiplicativa, \\(g^k = g \\cdot g \\cdot ... \\cdot g\\) (\\(k\\) volte); in notazione additiva, \\(kg = g + g + ... + g\\) (\\(k\\) volte)). Se tale intero positivo non esiste, si dice che l‚Äôelemento \\(g\\) ha ordine infinito.\n\nNah nah nah nah‚Ä¶ ferma tutto. Il primo √® chiaro e semplice. Il secondo non si capisce un tubo. Fammi un esempio.\nConsideriamo il gruppo \\((\\mathbb{Z}_4, +_4)\\), che √® il gruppo degli interi modulo 4 sotto l‚Äôoperazione di addizione modulo 4.\nEh!?\nL‚Äôelemento neutro in \\((\\mathbb{Z}_4, +_4)\\) √® lo 0, perch√© per ogni elemento \\(a \\in \\mathbb{Z}_4\\), si ha \\(a +_4 0 = 0 +_4 a = a\\). Eh vabb√® fin qua.\n\nCos‚Äô√® \\(\\mathbb{Z}_4\\)? √à l‚Äôinsieme \\(\\{0, 1, 2, 3\\}\\). Facile, ok.\nCos‚Äô√® \\(+_4\\)? √à l‚Äôaddizione modulo 4. Ad esempio, \\(2 +_4 3 = 5 \\pmod{4} = 1\\). Eh!?\n\nL‚Äôadidzione modulo 4 √® semplicemente una addizione che quando ‚Äúsfora‚Äù il 4 ricomincia da zero. Che uno pu√≤ immaginare sia una roba incasinatissima e invece no, perch√© volendo √® concettualmente simila a quello che fai dalla terza elementare quando fai le somme ma alle elementari lo chiamavi ‚Äúriporto‚Äù1. \\(8+4\\) quanto fa? \\(12\\), emb√©?\nS√¨, ma come ci siamo arrivati a far comparire una seconda cifra l√¨? Arriviamoci passo passo: \\(8 + 1 = 9\\).\nOk e fin qua‚Ä¶\n\\(9 + 1\\)? Non ho pi√π numeri a una cifra, allora cosa faccio? Ricomincio dallo zero, aggiungo uno davanti e continuo. Quindi: \\(9 + 1 = 10\\).\n\\(10 + 1 = 11\\).\n\\(11 + 1 = 12\\)\nL‚Äôaddizione modulo \\(n\\) la facciamo dalla seconda elementare senza manco saperlo\nNell‚Äôesempio di sopra il \\(2\\) di \\(12\\) si ottiene in termini di addizione modulo 10 con: \\[8 +_{10} 4 = 12 \\pmod{10} = 2\\]\nOk, andiamo avanti.\nOra, prendiamo un elemento a caso da \\(\\mathbb{Z}_4\\), ad esempio l‚Äôelemento 2. Vogliamo trovare l‚Äôordine dell‚Äôelemento 2. Dobbiamo trovare il pi√π piccolo intero positivo \\(k\\) tale che \\(k \\cdot 2 = 0\\) (ricorda che in notazione additiva, \\(g^k\\) diventa \\(k \\cdot g\\)). Qui, l‚Äôoperazione √® l‚Äôaddizione modulo 4, quindi stiamo cercando il pi√π piccolo \\(k\\) tale che:\n\\(2 +_4 2 +_4 \\ldots +_4 2\\) (\\(k\\) volte) \\(= 0 \\pmod{4}\\)\nIn altre parole, stiamo cercando il pi√π piccolo intero positivo \\(k\\) tale che \\(k \\times 2\\) sia un multiplo di 4. Vediamo un po‚Äô:\n\nPer \\(k = 1\\): \\(1 \\cdot 2 = 2 \\pmod{4} = 2 \\neq 0\\).\nPer \\(k = 2\\): \\(2 \\cdot 2 = 2 +_4 2 = 4 \\pmod{4} = 0\\).\n\nVAMOS! L‚Äôabbiamo trovato! Il pi√π piccolo intero positivo \\(k\\) per cui \\(k \\cdot 2 = 0 \\pmod{4}\\) √® \\(k = 2\\).\nQuindi, l‚Äôordine dell‚Äôelemento 2 nel gruppo \\((\\mathbb{Z}_4, +_4)\\) √® 2.\nProviamo con un altro elemento, ad esempio l‚Äôelemento 1. Vogliamo trovare il pi√π piccolo intero positivo \\(k\\) tale che \\(k \\cdot 1 = 0 \\pmod{4}\\).\n\nPer \\(k = 1\\): \\(1 \\cdot 1 = 1 \\pmod{4} = 1 \\neq 0\\).\nPer \\(k = 2\\): \\(2 \\cdot 1 = 1 +_4 1 = 2 \\pmod{4} = 2 \\neq 0\\).\nPer \\(k = 3\\): \\(3 \\cdot 1 = 1 +_4 1 +_4 1 = 3 \\pmod{4} = 3 \\neq 0\\).\nPer \\(k = 4\\): \\(4 \\cdot 1 = 1 +_4 1 +_4 1 +_4 1 = 4 \\pmod{4} = 0\\).\n\nIl pi√π piccolo intero positivo \\(k\\) per cui \\(k \\cdot 1 = 0 \\pmod{4}\\) √® \\(k = 4\\).\nQuindi, l‚Äôordine dell‚Äôelemento 1 nel gruppo \\((\\mathbb{Z}_4, +_4)\\) √® 4.\nFantastico. Finora era tutto bello e semplice. Ora scendiamo un po‚Äô nell‚Äôabisso."
  },
  {
    "objectID": "blog/group-prime-order.html#introduzione-ai-gruppi-le-fondamenta-dellalgebra-astratta",
    "href": "blog/group-prime-order.html#introduzione-ai-gruppi-le-fondamenta-dellalgebra-astratta",
    "title": "Gruppi di Ordine Primo",
    "section": "",
    "text": "Una volta parlavo con un mio amico di Algebra astratta e ricordo che mi chiese se davvero esistesse qualcosa come l‚Äôalgebra astratta. Secondo lui l‚Äôaggettivo astratta era ridondante, visto che considerava l‚Äôalgebra di per s√© gi√† astratta. No.¬†Non √® cos√¨. Dovete sapere che ai matematici piace astrarre qualsiasi cosa. Ma c‚Äô√® un motivo se √® cos√¨ ed √® per dare delle definizioni formali, non assiomatiche di operazioni comuni per permettere (in algebre diverse da quelle che usiamo comunemente) di poter definire operazioni diverse ma con propriet√† simili.\n\n‚ÄúCazzo vuol dire? Parla potabile Luca.‚Äù\n\nC‚Äôhai ragione, calmati.\nA volte fa comodo sapere che gli oggeti matematici con cui lavoriamo hanno delle propriet√† definite, dimostrate da teoremi.\nPerch√©? Perch√© cos√¨ si possono creare approcci nuovi, algoritmi, schemi e compagnia cantante senza dover dimostrare la loro fondatezza dalle basi, ma assumendo per esempio di partire con oggetti matematici noti.\nUn esempio di questo genere di oggetti matematici sono i gruppi. Esiste tutto un filone di matematica chiamato Teoria dei gruppi che si occupa di dimostrare, scoprire e studiare tutto ci√≤ che riguarda i Gruppi.\nOk ma che caz sono ‚Äôsti benedetti gruppi? E perch√© ci servono?\nCalmati, di nuovo.\nPrima di arrivare al cuore dell‚Äôargomento di questo post, i gruppi di ordine primo, conviene costruire prima una base, partendo dalla definizione stessa di gruppo.\nUn gruppo √® un insieme non vuoto, che possiamo chiamare come ci pare, ma chiamaremo \\(G\\) (che sta per \\(Giancarlo\\)), dotato di un‚Äôoperazione binaria, che spesso indicheremo con \\(\\cdot\\) (ma potrebbe essere +, *, o altro a seconda del contesto o di come vi svegliate la mattina. E i matematici lo fanno.). Per essere un gruppo, questa coppia \\((G, \\cdot)\\) deve soddisfare quattro propriet√† fondamentali, i cosiddetti assiomi di gruppo:\n\nChiusura: Per ogni coppia di elementi \\(a\\) e \\(b\\) appartenenti a \\(G\\), il risultato dell‚Äôoperazione \\(a \\cdot b\\) deve essere ancora un elemento di \\(G\\). In simboli: se \\(a, b \\in G\\), allora \\(a \\cdot b \\in G\\). Questo significa che l‚Äôoperazione non ci ‚Äúporta fuori‚Äù dall‚Äôinsieme \\(G\\).\nAssociativit√†: L‚Äôoperazione deve essere associativa. Questo significa che per ogni terna di elementi \\(a, b, c\\) in \\(G\\), l‚Äôordine in cui eseguiamo le operazioni non cambia il risultato: \\((a \\cdot b) \\cdot c = a \\cdot (b \\cdot c)\\).\nElemento Neutro (Identit√†): Esiste un elemento speciale in \\(G\\), che chiamiamo elemento neutro (o identit√†), solitamente indicato con \\(e\\) (o \\(1\\) in notazione moltiplicativa, \\(0\\) in notazione additiva). Questo elemento ha la propriet√† che, per ogni elemento \\(a\\) in \\(G\\), \\(a \\cdot e = e \\cdot a = a\\). L‚Äôelemento neutro si comporta come un ‚Äúnon-operazione‚Äù.\nElemento Inverso: Per ogni elemento \\(a\\) in \\(G\\), esiste un altro elemento in \\(G\\), chiamato inverso di \\(a\\), che indicheremo con \\(a^{-1}\\) (o \\(-a\\) in notazione additiva). L‚Äôelemento inverso ha la propriet√† che quando operato con \\(a\\) d√† come risultato l‚Äôelemento neutro: \\(a \\cdot a^{-1} = a^{-1} \\cdot a = e\\).\n\nSe un gruppo soddisfa anche la propriet√† commutativa, cio√® se per ogni coppia di elementi \\(a, b \\in G\\), \\(a \\cdot b = b \\cdot a\\), allora il gruppo √® detto gruppo abeliano (o commutativo). Se la propriet√† commutativa non vale per tutti gli elementi, il gruppo √® detto non-abeliano.\n\n\nIo ho gi√† perso per strada met√† dei lettori. Dico met√† perch√© sono un inguaribile ottimista. In realt√† se stai ancora leggendo forse sei l‚Äôunico che ha avuto il fegato di farlo. Quindi bravo.\nCome ricompnensa, vediamo alcuni esempi un po‚Äô pi√π concreti:\n\nI numeri interi con l‚Äôaddizione (\\((\\mathbb{Z}, +)\\)): L‚Äôinsieme dei numeri interi \\(\\mathbb{Z} = \\{..., -2, -1, 0, 1, 2, ...\\}\\) con l‚Äôoperazione di addizione usuale (+) forma un gruppo abeliano.\n\nChiusura: La somma di due numeri interi √® sempre un numero intero. 3 + 19 = 21 dove 3, 19 e 21 \\(\\in \\mathbb{Z}\\)\nAssociativit√†: L‚Äôaddizione √® associativa: \\((a + b) + c = a + (b + c)\\). Roba da seconda elementare, forza, su.\nElemento Neutro: L‚Äôelemento neutro √® lo zero (0), poich√© \\(a + 0 = 0 + a = a\\) per ogni intero \\(a\\).\nElemento Inverso: L‚Äôinverso di un intero \\(a\\) √® \\(-a\\), poich√© \\(a + (-a) = (-a) + a = 0\\).\nAbeliano: L‚Äôaddizione √® commutativa: \\(a + b = b + a\\).\n\nI numeri razionali non nulli con la moltiplicazione (\\((\\mathbb{Q} \\setminus \\{0\\}, \\cdot)\\)): L‚Äôinsieme dei numeri razionali escluso lo zero, con l‚Äôoperazione di moltiplicazione usuale (\\(\\cdot\\)), forma un gruppo abeliano.\n\nChiusura: Il prodotto di due numeri razionali non nulli √® ancora un numero razionale non nullo.\nAssociativit√†: La moltiplicazione √® associativa: \\((a \\cdot b) \\cdot c = a \\cdot (b \\cdot c)\\).\nElemento Neutro: L‚Äôelemento neutro √® uno (1), poich√© \\(a \\cdot 1 = 1 \\cdot a = a\\) per ogni razionale non nullo \\(a\\).\nElemento Inverso: L‚Äôinverso di un razionale non nullo \\(a = \\frac{p}{q}\\) (con \\(p, q \\neq 0\\)) √® \\(a^{-1} = \\frac{q}{p}\\).\nAbeliano: La moltiplicazione √® commutativa: \\(a \\cdot b = b \\cdot a\\).\n\nIl gruppo simmetrico \\(S_n\\) (per \\(n \\ge 3\\)): Questo √® un esempio di gruppo non-abeliano. \\(S_n\\) √® il gruppo delle permutazioni di \\(n\\) oggetti, con l‚Äôoperazione di composizione di funzioni. Per \\(n \\ge 3\\), la composizione di permutazioni non √® commutativa in generale.\n\nQuindi, riassumendo un pochino: un gruppo √® semplicemente l‚Äôastrazione di quello che noi usiamo ogni giorno per contare quanti soldi ci sono rimasti sul conto e scoprire di essere molto povery. Per essere definito un gruppo ha bisogno di due elementi: un insieme (non vuoto altrimenti grazie al cazzo) e un‚Äôoperazione binaria (operazione binaria=tra due operandi). Facile no? Ecco, adesso complichiamo le cose.\n\n\n\nInnanzitutto introduciamo un nuovo termine: l‚Äôordine. L‚Äôordine pu√≤ essere definito sia sul gruppo nella sua interezza, sia solo sull‚Äôelemento.\n\nL‚Äôordine di un gruppo \\(G\\), indicato con \\(|G|\\), non √® altro che il numero di elementi contenuti nell‚Äôinsieme \\(G\\). Se il numero di elementi √® finito, si dice che il gruppo √® finito, altrimenti √® infinito (minchia, la fantasia dei matematici eh!?). Gli esempi \\((\\mathbb{Z}, +)\\) e \\((\\mathbb{Q} \\setminus \\{0\\}, \\cdot)\\) sono gruppi infiniti, mentre il gruppo simmetrico \\(S_n\\) √® un gruppo finito di ordine \\(n!\\).\nL‚Äôordine di un elemento \\(g\\) in un gruppo \\(G\\) √® il pi√π piccolo intero positivo \\(k\\) tale che \\(g^k = e\\), dove \\(e\\) √® l‚Äôelemento neutro del gruppo e \\(g^k\\) indica l‚Äôoperazione di \\(g\\) con se stesso \\(k\\) volte (ad esempio, in notazione moltiplicativa, \\(g^k = g \\cdot g \\cdot ... \\cdot g\\) (\\(k\\) volte); in notazione additiva, \\(kg = g + g + ... + g\\) (\\(k\\) volte)). Se tale intero positivo non esiste, si dice che l‚Äôelemento \\(g\\) ha ordine infinito.\n\nNah nah nah nah‚Ä¶ ferma tutto. Il primo √® chiaro e semplice. Il secondo non si capisce un tubo. Fammi un esempio.\nConsideriamo il gruppo \\((\\mathbb{Z}_4, +_4)\\), che √® il gruppo degli interi modulo 4 sotto l‚Äôoperazione di addizione modulo 4.\nEh!?\nL‚Äôelemento neutro in \\((\\mathbb{Z}_4, +_4)\\) √® lo 0, perch√© per ogni elemento \\(a \\in \\mathbb{Z}_4\\), si ha \\(a +_4 0 = 0 +_4 a = a\\). Eh vabb√® fin qua.\n\nCos‚Äô√® \\(\\mathbb{Z}_4\\)? √à l‚Äôinsieme \\(\\{0, 1, 2, 3\\}\\). Facile, ok.\nCos‚Äô√® \\(+_4\\)? √à l‚Äôaddizione modulo 4. Ad esempio, \\(2 +_4 3 = 5 \\pmod{4} = 1\\). Eh!?\n\nL‚Äôadidzione modulo 4 √® semplicemente una addizione che quando ‚Äúsfora‚Äù il 4 ricomincia da zero. Che uno pu√≤ immaginare sia una roba incasinatissima e invece no, perch√© volendo √® concettualmente simila a quello che fai dalla terza elementare quando fai le somme ma alle elementari lo chiamavi ‚Äúriporto‚Äù1. \\(8+4\\) quanto fa? \\(12\\), emb√©?\nS√¨, ma come ci siamo arrivati a far comparire una seconda cifra l√¨? Arriviamoci passo passo: \\(8 + 1 = 9\\).\nOk e fin qua‚Ä¶\n\\(9 + 1\\)? Non ho pi√π numeri a una cifra, allora cosa faccio? Ricomincio dallo zero, aggiungo uno davanti e continuo. Quindi: \\(9 + 1 = 10\\).\n\\(10 + 1 = 11\\).\n\\(11 + 1 = 12\\)\nL‚Äôaddizione modulo \\(n\\) la facciamo dalla seconda elementare senza manco saperlo\nNell‚Äôesempio di sopra il \\(2\\) di \\(12\\) si ottiene in termini di addizione modulo 10 con: \\[8 +_{10} 4 = 12 \\pmod{10} = 2\\]\nOk, andiamo avanti.\nOra, prendiamo un elemento a caso da \\(\\mathbb{Z}_4\\), ad esempio l‚Äôelemento 2. Vogliamo trovare l‚Äôordine dell‚Äôelemento 2. Dobbiamo trovare il pi√π piccolo intero positivo \\(k\\) tale che \\(k \\cdot 2 = 0\\) (ricorda che in notazione additiva, \\(g^k\\) diventa \\(k \\cdot g\\)). Qui, l‚Äôoperazione √® l‚Äôaddizione modulo 4, quindi stiamo cercando il pi√π piccolo \\(k\\) tale che:\n\\(2 +_4 2 +_4 \\ldots +_4 2\\) (\\(k\\) volte) \\(= 0 \\pmod{4}\\)\nIn altre parole, stiamo cercando il pi√π piccolo intero positivo \\(k\\) tale che \\(k \\times 2\\) sia un multiplo di 4. Vediamo un po‚Äô:\n\nPer \\(k = 1\\): \\(1 \\cdot 2 = 2 \\pmod{4} = 2 \\neq 0\\).\nPer \\(k = 2\\): \\(2 \\cdot 2 = 2 +_4 2 = 4 \\pmod{4} = 0\\).\n\nVAMOS! L‚Äôabbiamo trovato! Il pi√π piccolo intero positivo \\(k\\) per cui \\(k \\cdot 2 = 0 \\pmod{4}\\) √® \\(k = 2\\).\nQuindi, l‚Äôordine dell‚Äôelemento 2 nel gruppo \\((\\mathbb{Z}_4, +_4)\\) √® 2.\nProviamo con un altro elemento, ad esempio l‚Äôelemento 1. Vogliamo trovare il pi√π piccolo intero positivo \\(k\\) tale che \\(k \\cdot 1 = 0 \\pmod{4}\\).\n\nPer \\(k = 1\\): \\(1 \\cdot 1 = 1 \\pmod{4} = 1 \\neq 0\\).\nPer \\(k = 2\\): \\(2 \\cdot 1 = 1 +_4 1 = 2 \\pmod{4} = 2 \\neq 0\\).\nPer \\(k = 3\\): \\(3 \\cdot 1 = 1 +_4 1 +_4 1 = 3 \\pmod{4} = 3 \\neq 0\\).\nPer \\(k = 4\\): \\(4 \\cdot 1 = 1 +_4 1 +_4 1 +_4 1 = 4 \\pmod{4} = 0\\).\n\nIl pi√π piccolo intero positivo \\(k\\) per cui \\(k \\cdot 1 = 0 \\pmod{4}\\) √® \\(k = 4\\).\nQuindi, l‚Äôordine dell‚Äôelemento 1 nel gruppo \\((\\mathbb{Z}_4, +_4)\\) √® 4.\nFantastico. Finora era tutto bello e semplice. Ora scendiamo un po‚Äô nell‚Äôabisso."
  },
  {
    "objectID": "blog/group-prime-order.html#gruppi-ciclici-e-generatori-la-chiave-per-i-gruppi-di-ordine-primo",
    "href": "blog/group-prime-order.html#gruppi-ciclici-e-generatori-la-chiave-per-i-gruppi-di-ordine-primo",
    "title": "Gruppi di Ordine Primo",
    "section": "Gruppi Ciclici e Generatori: La Chiave per i Gruppi di Ordine Primo",
    "text": "Gruppi Ciclici e Generatori: La Chiave per i Gruppi di Ordine Primo\nUn tipo speciale di gruppo, che poi √® l‚Äôargomento per cui siamo qui tutti riuniti quest‚Äôoggi, √® il gruppo ciclico.\nUn gruppo ciclico √® (formalmente) un gruppo che pu√≤ essere generato da un singolo elemento. Ovvero, un gruppo \\(G\\) √® ciclico se esiste un elemento \\(g \\in G\\) tale che ogni elemento di \\(G\\) pu√≤ essere espresso come una ‚Äúpotenza‚Äù di \\(g\\) (dove ‚Äúpotenza‚Äù significa ripetere l‚Äôoperazione di gruppo). Questo elemento \\(g\\) √® chiamato generatore del gruppo \\(G\\).\nOra: avete notato una cosa importante? Abbiamo usato il termini ‚Äúpotenza‚Äù ma specificando che vuol dire ‚Äúripere l‚Äôoperazione di gruppo‚Äù. Intendevo questo quando parlavo di Algebra astratta. Non √® altro che prendere l‚Äôalgebra che usiamo tutti i giorni e semplicemente astrarre dei concetti e dimostrare le propriet√† di quell‚Äôoggetto a prescindere che l‚Äôoperazione sia l‚Äôaddizione o la moltiplicazione.\nFormalmente, un gruppo \\(G\\) √® ciclico se esiste un elemento \\(g \\in G\\) tale che:\n\\(G = \\{g^k \\mid k \\in \\mathbb{Z} \\}\\) (in notazione moltiplicativa)\noppure\n\\(G = \\{kg \\mid k \\in \\mathbb{Z} \\}\\) (in notazione additiva)\ndove \\(\\mathbb{Z}\\) rappresenta l‚Äôinsieme dei numeri interi.\nConviene fare un esempio, che sento gi√† la puzza di bruciato. Consideriamo il gruppo degli interi modulo 5 (perch√© se facciamo sempre con quel cazzo di 4 pensate che funziona solo col 4) con l‚Äôaddizione, \\((\\mathbb{Z}_5, +) = \\{[0], [1], [2], [3], [4]\\}\\). Questo √® un gruppo ciclico di ordine 5. L‚Äôelemento [1] √® un generatore di \\(\\mathbb{Z}_5\\), poich√©:\n\n\\([1]^1 = [1]\\)\n\\([1]^2 = [1] + [1] = [2]\\)\n\\([1]^3 = [1] + [1] + [1] = [3]\\)\n\\([1]^4 = [1] + [1] + [1] + [1] = [4]\\)\n\\([1]^5 = [1] + [1] + [1] + [1] + [1] = [5] \\equiv [0] \\pmod{5}\\) (elemento neutro)\n\\([1]^6 = [1] + [1] + [1] + [1] + [1] + [1] = [6] \\equiv [1] \\pmod{5}\\), e cos√¨ via.\n\nQuindi, tornando a \\(\\mathbb{Z}_5 = \\{[0], [1], [2], [3], [4]\\}\\), √® chiaro che partendo da [1] e sommandolo a se stesso un po‚Äô di volte, possiamo ottenere tutti gli altri elementi. Infatti: [1], [1]+[1]=[2], [1]+[1]+[1]=[3], [1]+[1]+[1]+[1]=[4], e [1] sommato 5 volte ci rid√† [0] (cio√® [5] che modulo 5 fa [0]). Figo no? Questo significa che [1] √® un generatore di \\(\\mathbb{Z}_5\\).\nMa non solo [1]! Anche [2] √® un generatore di \\(\\mathbb{Z}_5\\). Proviamo: [2], [2]+[2]=[4], [2]+[2]+[2]=[6]=[1] (modulo 5), [2]+[2]+[2]+[2]=[8]=[3] (modulo 5), e [2] sommato 5 volte fa [10]=[0] (modulo 5). Anche con [2] abbiamo generato tutti gli elementi di \\(\\mathbb{Z}_5\\)!\nOra, uno potrebbe chiedersi: ma vale sempre? √à vero che [2] √® un generatore di \\(\\mathbb{Z}_n\\) per ogni \\(n\\) dispari? Si lascia la dimostrazione come esercizio al lettore.\nNaaah! Scherzo. In realt√† lo schema della dimostrazione potrebbe essere abbastanza intuitivo.\nL‚Äôidea √® che se \\(n\\) √® dispari, allora 2 e \\(n\\) sono coprimi, cio√® non hanno fattori comuni (a parte 1, ovviamente). E questo √® un dettaglio FONDAMENTALE.\nCerchiamo di capirlo meglio. Quando sommiamo [2] a se stesso un po‚Äô di volte in \\(\\mathbb{Z}_n\\), otteniamo i multipli di [2]: [2], [4], [6], [8], [10], e cos√¨ via, sempre modulo \\(n\\). Se 2 e \\(n\\) fossero ‚Äúamici‚Äù e avessero un fattore comune, diciamo \\(d &gt; 1\\), allora i multipli di 2 ‚Äúmodulo \\(n\\)‚Äù si ripeterebbero prima di coprire tutti gli elementi di \\(\\mathbb{Z}_n\\). Ma se 2 e \\(n\\) sono coprimi, allora i multipli di 2 ‚Äúmodulo \\(n\\)‚Äù continuano a sforare e a dare resti diversi fino a quando non abbiamo generato tutti gli elementi da [0] a \\([n-1]\\).\nPer esempio, prendiamo \\(n=9\\) (dispari). Partiamo da [2]: [2], [4], [6], [8], [10]=[1] (modulo 9), [12]=[3] (modulo 9), [14]=[5] (modulo 9), [16]=[7] (modulo 9), [18]=[0] (modulo 9). E voil√†! Abbiamo ottenuto [0], [1], [2], [3], [4], [5], [6], [7], [8], cio√® tutti gli elementi di \\(\\mathbb{Z}_9\\).\nSe invece prendessimo un \\(n\\) pari, tipo \\(n=6\\), e provassimo con [2] in \\(\\mathbb{Z}_6\\): [2], [4], [6]=[0] (modulo 6), [8]=[2] (modulo 6)‚Ä¶ Vediamo che otteniamo solo [0], [2], [4]. Non generiamo tutto \\(\\mathbb{Z}_6\\). Questo succede perch√© 2 e 6 non sono coprimi (hanno il fattore comune 2).\n\\[\\blacksquare\\]"
  },
  {
    "objectID": "blog/group-prime-order.html#gruppi-di-ordine-primo-un-risultato-fondamentale",
    "href": "blog/group-prime-order.html#gruppi-di-ordine-primo-un-risultato-fondamentale",
    "title": "Gruppi di Ordine Primo",
    "section": "Gruppi di Ordine Primo: Un Risultato Fondamentale",
    "text": "Gruppi di Ordine Primo: Un Risultato Fondamentale\nOra arriviamo al cuore del nostro articolo: i gruppi di ordine primo. Tutti sapete cos‚Äô√® un numero primo, altrimenti vi vengo a cercare e a strappare la licenza media di persona.\nComunque.\nFormalmente (perch√© un po‚Äô di formalit√† ci vuole, senn√≤ sembra il bar dello sport): Un numero primo √® un intero positivo maggiore di 1 che ha solo due divisori positivi: 1 e se stesso. Esempi di numeri primi sono 2, 3, 5, 7, 11, 13, ecc.\nUn risultato fondamentale e notevole in teoria dei gruppi afferma (e rieccoci alla questione algebra astratta!):\nTeorema: Ogni gruppo di ordine primo √® ciclico.\nCazzo vuol dire? (Cit.)\nQuesto teorema ci dice che se prendiamo un gruppo \\(G\\) il cui ordine \\(|G|\\) √® un numero primo \\(q\\), allora \\(G\\) √® necessariamente un gruppo ciclico. Questo significa che esiste almeno un elemento \\(g \\in G\\) che genera tutto il gruppo \\(G\\).\nIl che √® notevole. Vuol dire che possiamo prendere un gruppo \\(G\\) di pordine \\(q\\) (ricordate? prima per esempio avevamo ordine \\(4\\) oppure \\(5\\)) e possiamo avere un elemento all‚Äôinterno del gruppo che genera tutti gli altri, dato un numero \\(k\\).\n\nNon riesco a capire perch√© sei cos√¨ eccitato Luca. Spiegamelo, prima che ti faccio male\n\nCalma e sangue freddo.\nRiprendiamo l‚Äôesempio di \\(\\mathbb{Z}_5\\) e del generatore [1]. Se io vi dico che un certo elemento nel gruppo √® [3], e vi dico che il generatore √® [1], voi sapete subito che per ottenere [3] da [1] devo fare:\n\\[\n[1] + [1] + [1] = [3] \\qquad \\text{oppure} \\qquad 3 \\cdot [1] = [3]\n\\]\nMa potrei averlo ottenuto anche facendo molte pi√π operazioni, tipo:\n\\[\n8 \\cdot [1] = [1] + [1] + [1] + [1] + [1] + [1] + [1] + [1] = [8] = [3] \\pmod{5}\n\\]\no anche\n\\[\n90901291213 \\cdot [1] = [90901291213] = [3] \\pmod{5}\n\\]\nCi sono infiniti modi per descrivere lo stesso elemento [3] usando il generatore [1]! Se io vi dico solo che il risultato √® [3], che l‚Äôordine del gruppo ciclico √® 5 e il generatore √® [1], voi non potete sapere quante volte ho applicato l‚Äôoperazione, cio√® quale esponente \\(k\\) ho usato tra gli infiniti possibili. Ed √® proprio su questa ‚Äúignoranza‚Äù che si basa una valanga di algoritmi di crittografia moderni!\n‚ÄúCose semplici da verificare, ma difficili da calcolare‚Äù: Verificare che \\(90901291213 \\cdot [1] = [3] \\pmod{5}\\) √® facilissimo. Basta fare la divisione e vedere il resto. Ma se io vi dessi solo [3], [1] e 5, e vi chiedessi di trovare quel numero enorme \\(k=90901291213\\) (o uno simile), sarebbe praticamente impossibile farlo in tempi ragionevoli, soprattutto se l‚Äôordine del gruppo fosse un numero primo enorme di centinaia di cifre!\nEcco perch√© questo teorema sui gruppi di ordine primo, che all‚Äôinizio sembrava solo un giochino matematico, √® in realt√† fondamentale per la sicurezza delle nostre comunicazioni online, delle nostre transazioni bancarie, e di un sacco di altre cose che usiamo tutti i giorni. La matematica astratta che salva il mondo, ragazzi! Chi l‚Äôavrebbe mai detto? Come? Banalmente, l‚Äôidea √® che se io uso un numero segreto \\(k\\) per ‚Äúcriptare‚Äù in un certo modo con un certo algoritmo, anche se vi mostro il risultato ‚Äúcriptato‚Äù, indovinare \\(k\\) sarebbe un disastro.\nDetto questo: ora dimostrazione formale del teorema e tutti a nanna.\nDimostrazione (Schema):\nSia \\(G\\) un gruppo di ordine primo \\(q\\). Consideriamo un elemento \\(g \\in G\\) diverso dall‚Äôelemento neutro \\(e\\). Consideriamo il sottogruppo ciclico generato da \\(g\\), che indichiamo con \\(\\langle g \\rangle = \\{g^k \\mid k \\in \\mathbb{Z} \\}\\). Per il teorema di Lagrange, l‚Äôordine di un sottogruppo deve dividere l‚Äôordine del gruppo. Quindi, l‚Äôordine di \\(\\langle g \\rangle\\), cio√® \\(|\\langle g \\rangle|\\), deve dividere l‚Äôordine di \\(G\\), che √® \\(q\\). Poich√© \\(q\\) √® primo, i divisori positivi di \\(q\\) sono solo 1 e \\(q\\).\nL‚Äôordine di \\(\\langle g \\rangle\\) non pu√≤ essere 1, perch√© \\(g \\neq e\\), quindi \\(\\langle g \\rangle\\) contiene almeno due elementi (\\(e\\) e \\(g\\)). Pertanto, l‚Äôunica possibilit√† √® che \\(|\\langle g \\rangle| = q\\). Ma se l‚Äôordine del sottogruppo generato da \\(g\\) √® uguale all‚Äôordine del gruppo \\(G\\), e \\(\\langle g \\rangle\\) √® un sottogruppo di \\(G\\), allora deve essere che \\(\\langle g \\rangle = G\\). Questo significa che \\(G\\) √® generato dall‚Äôelemento \\(g\\), e quindi \\(G\\) √® ciclico.\nAltre Conseguenze e Implicazioni:\nQuesto teorema ha importanti conseguenze. Se sappiamo che un gruppo ha ordine primo \\(q\\), sappiamo immediatamente che:\n\n√à ciclico: Esiste un generatore \\(g \\in G\\) tale che ogni elemento di \\(G\\) √® una potenza di \\(g\\).\n√à abeliano: Tutti i gruppi ciclici sono abeliani. Quindi, ogni gruppo di ordine primo √® abeliano.\nStruttura semplice: I gruppi ciclici sono tra i gruppi pi√π semplici da comprendere e studiare. Il teorema ci dice che i gruppi di ordine primo, nonostante la loro apparente ‚Äúrarit√†‚Äù (ci sono infiniti numeri primi), hanno una struttura algebrica molto ben definita e semplice.\n\nGeneratore \\(g\\):\nAbbiamo visto che un gruppo \\(G\\) di ordine primo \\(q\\) √® ciclico e ha un generatore \\(g\\). Questo generatore \\(g\\) √® un elemento di \\(G\\) tale che ‚Äúripetendo‚Äù l‚Äôoperazione di gruppo con \\(g\\) (con se stesso), possiamo ottenere tutti gli elementi di \\(G\\).\nAd esempio, se \\(G\\) √® un gruppo di ordine 7 (7 √® primo), allora esiste un elemento \\(g \\in G\\) tale che:\n\\(G = \\{e, g, g^2, g^3, g^4, g^5, g^6\\}\\)\ndove \\(e\\) √® l‚Äôelemento neutro e \\(g^7 = e\\). Gli esponenti sono presi modulo 7.\nUnicit√† (a meno di isomorfismo):\nUn altro risultato importante (che non dimostreremo qui perch√© oggettivamente mi sono scassato le balle pure io) √® che esiste, a meno di isomorfismo, un solo gruppo ciclico di ordine \\(n\\) per ogni intero positivo \\(n\\). In particolare, per ogni numero primo \\(q\\), esiste un solo gruppo di ordine \\(q\\), a meno di isomorfismo. Questo gruppo √® isomorfo al gruppo ciclico \\(\\mathbb{Z}_q = (\\mathbb{Z}/q\\mathbb{Z}, +)\\), cio√® gli interi modulo \\(q\\) con l‚Äôaddizione.\nEsempio:\nSe consideriamo un gruppo \\(G\\) di ordine 3. Poich√© 3 √® un numero primo, sappiamo che \\(G\\) √® ciclico e abeliano. Esiste un elemento \\(g \\in G\\) tale che \\(G = \\{e, g, g^2\\}\\) e \\(g^3 = e\\). La struttura di \\(G\\) √® completamente determinata da questa propriet√†. Ogni gruppo di ordine 3 √® ‚Äúessenzialmente lo stesso‚Äù (isomorfo) a \\(\\mathbb{Z}_3 = \\{[0], [1], [2]\\}\\) con l‚Äôaddizione modulo 3."
  },
  {
    "objectID": "blog/group-prime-order.html#conclusione-la-bellezza-e-la-potenza-dei-gruppi-di-ordine-primo",
    "href": "blog/group-prime-order.html#conclusione-la-bellezza-e-la-potenza-dei-gruppi-di-ordine-primo",
    "title": "Gruppi di Ordine Primo",
    "section": "Conclusione: La Bellezza e la Potenza dei Gruppi di Ordine Primo**",
    "text": "Conclusione: La Bellezza e la Potenza dei Gruppi di Ordine Primo**\nE con questo, siamo arrivati alla fine di questo viaggione nell‚Äôabisso dell‚Äôalgebra astratta. Spero che abbiate capito perch√© questo teorema, all‚Äôapparenza cos√¨ semplice, √® in realt√† un risultato fondamentale e potente. Ci dice che i gruppi con un numero primo di elementi non sono solo un‚Äôastrazione matematica, ma hanno una struttura incredibilmente ordinata e prevedibile: sono ciclici, sono abeliani, e sono, in un certo senso, i gruppi pi√π ‚Äúsemplici‚Äù che possiamo immaginare.\nMa non √® tanto la loro semplicit√† che deve entusiasmare. Come abbiamo visto, proprio questa struttura ‚Äúsemplice‚Äù e ben definita dei gruppi ciclici di ordine primo √® alla base di tecnologie complesse come la crittografia moderna. La prossima volta che fate un pagamento online su onlyfans o che inviate un messaggio criptato che tanto la vostra fidanzata scoprir√† comunque, pensateci: dietro le quinte, c‚Äô√® l‚Äôalgebra astratta, ci sono i gruppi di ordine primo, e c‚Äô√® un sacco di matematica fighissima al servizio della vostra sicureza.\nLa teoria dei gruppi √® piena di altri risultati sorprendenti e connessioni inaspettate. Questo teorema sui gruppi di ordine primo √® solo un piccolo assaggio della bellezza e della potenza di questa branca della matematica. Se siete arrivati fin qui, complimenti, sinceramente!\n\nFIN"
  },
  {
    "objectID": "blog/group-prime-order.html#footnotes",
    "href": "blog/group-prime-order.html#footnotes",
    "title": "Gruppi di Ordine Primo",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nin realt√† l‚Äôanalogia √® un po‚Äô azzardata e pericolante, visto che nelle operazioni modulo \\(n\\) si resta sempre all‚Äôinterno dell‚Äôinsieme iniziale. Il ‚Äúriporto‚Äù nelle somme in base 10 √® legato alle potenze di 10 (unit√†, decine, centinaia, ecc.), mentre l‚Äôaddizione modulo 4 √® legata ai resti della divisione per 4. Non sono esattamente la stessa cosa, anche se condividono un‚Äôidea di ‚Äúsuperamento di una soglia‚Äù e di ‚Äúripartenza‚Äù.‚Ü©Ô∏é"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Sono Luca, e come dico spesso: solo un povero cog**one. Scherzi a parte, sono un programmatore con 16 anni di esperienza. Ho iniziato a sviluppare piccoli siti per privati nel novembre 2009, poi ho lavorato per cinque anni come full stack developer presso Digitalfun S.r.l.\nNel novembre 2014, mentre lavoravo ancora in DF, ho co-fondato una startup con mio cugino e un terzo socio. Due mesi dopo mi sono trasferito a Londra in cerca di opportunit√†, dove ho lavorato per un anno come backend developer in SaveWaterSaveMoney Ltd, utilizzando Symfony.\nNel luglio 2016 sono tornato in Italia e ho iniziato a lavorare in Nohup S.r.l. come full stack developer, ricoprendo il ruolo di R&D Team Leader nell‚Äôultimo anno.\nMi sono laureato presso l‚ÄôUniversit√† degli Studi Guglielmo Marconi, un‚Äôuniversit√† telematica. Attualmente, sono uno studente part-time del corso magistrale in Artificial Intelligence all‚ÄôUniversit√† degli Studi di Udine. Dopo la mia esperienza alla Marconi, ho deciso di affrontare il percorso magistrale con un ritmo pi√π sostenibile.\nSono appassionato di tecnologia, scienza e insegnamento. Il mio pi√π grande sogno nel cassetto √® diventare insegnante di materie scientifiche.\n\n\n\n\nI‚Äôm Luca, and as I often say: just a poor dum**ss. Jokes aside, I‚Äôve been a programmer for 16 years. I started developing small websites for private clients in November 2009, then spent five years as a full stack developer at Digitalfun S.r.l.\nIn November 2014, while still working at DF, I co-founded a startup with my cousin and a third partner. Two months later, I moved to London to seek new opportunities and worked for a year as a backend developer at SaveWaterSaveMoney Ltd, using Symfony.\nIn July 2016, I returned to Italy and joined Nohup S.r.l. as a full stack developer, eventually becoming the R&D Team Leader in my final year.\nI earned my degree from Universit√† degli Studi Guglielmo Marconi, an online university. Currently, I‚Äôm a part-time Master‚Äôs student in Artificial Intelligence at the University of Udine. After my experience at Marconi, I decided to take the Master‚Äôs program at a more sustainable pace.\nI‚Äôm passionate about technology, science, and teaching. My biggest dream is to become a teacher in scientific subjects."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About",
    "section": "",
    "text": "Sono Luca, e come dico spesso: solo un povero cog**one. Scherzi a parte, sono un programmatore con 16 anni di esperienza. Ho iniziato a sviluppare piccoli siti per privati nel novembre 2009, poi ho lavorato per cinque anni come full stack developer presso Digitalfun S.r.l.\nNel novembre 2014, mentre lavoravo ancora in DF, ho co-fondato una startup con mio cugino e un terzo socio. Due mesi dopo mi sono trasferito a Londra in cerca di opportunit√†, dove ho lavorato per un anno come backend developer in SaveWaterSaveMoney Ltd, utilizzando Symfony.\nNel luglio 2016 sono tornato in Italia e ho iniziato a lavorare in Nohup S.r.l. come full stack developer, ricoprendo il ruolo di R&D Team Leader nell‚Äôultimo anno.\nMi sono laureato presso l‚ÄôUniversit√† degli Studi Guglielmo Marconi, un‚Äôuniversit√† telematica. Attualmente, sono uno studente part-time del corso magistrale in Artificial Intelligence all‚ÄôUniversit√† degli Studi di Udine. Dopo la mia esperienza alla Marconi, ho deciso di affrontare il percorso magistrale con un ritmo pi√π sostenibile.\nSono appassionato di tecnologia, scienza e insegnamento. Il mio pi√π grande sogno nel cassetto √® diventare insegnante di materie scientifiche.\n\n\n\n\nI‚Äôm Luca, and as I often say: just a poor dum**ss. Jokes aside, I‚Äôve been a programmer for 16 years. I started developing small websites for private clients in November 2009, then spent five years as a full stack developer at Digitalfun S.r.l.\nIn November 2014, while still working at DF, I co-founded a startup with my cousin and a third partner. Two months later, I moved to London to seek new opportunities and worked for a year as a backend developer at SaveWaterSaveMoney Ltd, using Symfony.\nIn July 2016, I returned to Italy and joined Nohup S.r.l. as a full stack developer, eventually becoming the R&D Team Leader in my final year.\nI earned my degree from Universit√† degli Studi Guglielmo Marconi, an online university. Currently, I‚Äôm a part-time Master‚Äôs student in Artificial Intelligence at the University of Udine. After my experience at Marconi, I decided to take the Master‚Äôs program at a more sustainable pace.\nI‚Äôm passionate about technology, science, and teaching. My biggest dream is to become a teacher in scientific subjects."
  }
]