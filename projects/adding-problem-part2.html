<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Luca Simonetti">
<meta name="dcterms.date" content="2025-02-20">

<title>Analyzing GRU Training Dynamics on the Adding Problem - Part 2 – L.</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-86daaaaad7353f9cc0c554efc1dd6d94.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-9a528ce568208207557d4826b9a112ab.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
<meta property="og:title" content="Analyzing GRU Training Dynamics on the Adding Problem - Part 2">
<meta property="og:description" content="Personal website of Luca Simonetti - Research Projects, Blog Posts, and sh*t like that.">
<meta property="og:image" content="https://theghoul21.github.io//images/gru-sana-4.jpeg">
<meta property="og:site_name" content="L.">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">L.</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../projects/index.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-lecture-notes" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Lecture notes</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-lecture-notes">    
        <li>
    <a class="dropdown-item" href="https://theghoul21.github.io/CIT-2024-2025/" target="_blank">
 <span class="dropdown-text">Complexity and Information Theory 2024/2025</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://theghoul21.github.io/DL-2024-2025/" target="_blank">
 <span class="dropdown-text">Deep Learning 2024/2025</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://theghoul21.github.io/RO-2024-2025/" target="_blank">
 <span class="dropdown-text">Ricerca Operativa 2024/2025</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://theghoul21.github.io/FNN-2024-2025/" target="_blank">
 <span class="dropdown-text">Foundations of Neural Networks 2024/2025</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://theghoul21.github.io/QC-2023-2024/" target="_blank">
 <span class="dropdown-text">Quantum Computing 2023/2024</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://theghoul21.github.io/MB-2024-2025/" target="_blank">
 <span class="dropdown-text">Biologia Molecolare 2024/2025</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://theghoul21.github.io/VVT-2024-2025/" target="_blank">
 <span class="dropdown-text">Verification and Validation Techniques 2024/2025</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://theghoul21.github.io/AR-2024-2025/" target="_blank">
 <span class="dropdown-text">Automated Reasoning 2024/2025</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://theghoul21.github.io/AA-2023-2024/" target="_blank">
 <span class="dropdown-text">Advanced Algorithms 2023/2024</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://theghoul21.github.io/AI-2023-2024/" target="_blank">
 <span class="dropdown-text">Artificial Intelligence 2023/2024</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://theghoul21.github.io/SA-2024-2025/" target="_blank">
 <span class="dropdown-text">Applied Statistics 2024/2025</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-page-left">
      <h1 class="title">Analyzing GRU Training Dynamics on the Adding Problem - Part 2</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">programming</div>
                <div class="quarto-category">web development</div>
                <div class="quarto-category">research</div>
                <div class="quarto-category">diary</div>
                <div class="quarto-category">R&amp;D</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta column-page-left">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Luca Simonetti </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 20, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#what-those-parameters-mean" id="toc-what-those-parameters-mean" class="nav-link" data-scroll-target="#what-those-parameters-mean"><span class="header-section-number">2</span> What those parameters mean?</a></li>
  <li><a href="#the-update-gate-at-t0" id="toc-the-update-gate-at-t0" class="nav-link" data-scroll-target="#the-update-gate-at-t0"><span class="header-section-number">3</span> The Update Gate at <span class="math inline">\(t=0\)</span></a></li>
  <li><a href="#the-reset-gate-at-t0" id="toc-the-reset-gate-at-t0" class="nav-link" data-scroll-target="#the-reset-gate-at-t0"><span class="header-section-number">4</span> The Reset gate at <span class="math inline">\(t=0\)</span></a></li>
  <li><a href="#the-new-gate-at-t0" id="toc-the-new-gate-at-t0" class="nav-link" data-scroll-target="#the-new-gate-at-t0"><span class="header-section-number">5</span> The New gate at <span class="math inline">\(t=0\)</span></a></li>
  <li><a href="#final-step" id="toc-final-step" class="nav-link" data-scroll-target="#final-step"><span class="header-section-number">6</span> Final step</a></li>
  <li><a href="#coming-next" id="toc-coming-next" class="nav-link" data-scroll-target="#coming-next"><span class="header-section-number">7</span> Coming next…</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block column-page-left" id="quarto-document-content">





<section id="introduction" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1</span> Introduction</h2>
<p>In this blog post, I’ll complete the journey started in the <a href="../projects/adding-problem-part1.html">previous post</a> in which I introduced the problem and showed some interesting plots depicting weights and the learning dynamics that was behind it. I tried to explain the way that GRU works under the hood, step by step with a full example of a sequence going through the whole GRU. We saw step by step each and every transformation that the input is subjected to. In this blog post we will do something in my opinion more interesting that is analysing the weights. Now, this can actually mean everything so I want to split the discussion in two parts:</p>
<ol type="1">
<li>What the weights actually mean? We saw a complete forward pass that showed more or less how the initial input is transformed into the hidden layer. But why those specific weights rather than other values?</li>
<li>How did we get there? It’s well known that weights are usually randomly initialized in networks, so how did we get from those random values to our values? Is the dynamics of the learning responsible of the specific values? Could have this been done differently? Could have the weights have followed different trajectories? In either case can it be proved?</li>
</ol>
<p>Let’s begin, this is gonna be a lot of fun! (And a lot of work!!)</p>
</section>
<section id="what-those-parameters-mean" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="what-those-parameters-mean"><span class="header-section-number">2</span> What those parameters mean?</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>I will use weights and parameters interchangeably. One could argue that they have different meanings (and it might even be true) but in our case maybe we can loosen a bit this detail in terminology and live happily anyway.</p>
</div>
</div>
<p>So: what those parameter mean?</p>
<p>I tried to figure this out because I noticed one really weird thing looking at the last hidden layer of our walkthrough.</p>
<p>Let’s refresh quickly what was the example, remember that the input sequence was:</p>
<p><span class="math display">\[
\text{x} =
\left[
\begin{bmatrix}
12 \\ 0
\end{bmatrix}
\begin{bmatrix}
37 \\ 1
\end{bmatrix}
\begin{bmatrix}
12 \\ 0
\end{bmatrix}
\begin{bmatrix}
21 \\ 1
\end{bmatrix}
\right]
\]</span></p>
<p>we normalized the numbers so that they were represented in hundreths, so basically what we had after the normalization was:</p>
<p>The input sequence is represented as a tensor:</p>
<p><span class="math display">\[
\text{x} =
\begin{bmatrix}
0.12 &amp; 0 \\
0.37 &amp; 1 \\
0.12 &amp; 0 \\
0.21 &amp; 1
\end{bmatrix}
\]</span></p>
<p>My architecture was given the smallest hidden layer as possible on purpose. What I was trying to do was pushin the GRU cell to make the best out of what it had, and my expectation was that at some point it would have learned to basically add to it’s hidden layer the value in position <span class="math inline">\(0\)</span> iff the flaf in position <span class="math inline">\(1\)</span> was equal to <span class="math inline">\(true\)</span> (or <span class="math inline">\(1\)</span> ok…) This didn’t happen and looking back at my original thought I feel a bit stupid about it. Let’s recall what a GRU cell is. Initially, for <span class="math inline">\(t = 0\)</span>, the hidden layer is <span class="math inline">\(h_0 = 0\)</span>.</p>
<p>then the cell uses this series of transformations to get tha hidden layer out:</p>
<span class="math display">\[\begin{aligned}
z_t &amp;= \sigma(W_z x_t + U_z h_{t-1} + b_z) \\
r_t &amp;= \sigma(W_r x_t + U_r h_{t-1} + b_r) \\
\hat{h}_t &amp;= \phi(W_h x_t + U_h (r_t \odot h_{t-1}) + b_h) \\
h_t &amp;= z_t \odot h_{t-1} + (1 - z_t) \odot \hat{h}_t
\end{aligned}\]</span>
<p>if you look at the activation function this should already ring a bell, but let’s walk through step by step. Let’s assume we always have four elements and 2 of them are summed to the total. Let’s also assume that we normalize to the maximum each single value can reach (in our case <span class="math inline">\(100\)</span>). Let’s have an extreme example now:</p>
<p><span class="math display">\[
\text{x} =
\begin{bmatrix}
100 &amp; 1 \\
100 &amp; 1 \\
100 &amp; 0 \\
100 &amp; 0
\end{bmatrix}
\]</span></p>
<p>which after our normalization becomes:</p>
<p><span class="math display">\[
\text{x} =
\begin{bmatrix}
1.0 &amp; 1 \\
1.0 &amp; 1 \\
1.0 &amp; 0 \\
1.0 &amp; 0
\end{bmatrix}
\]</span></p>
<p>Now: if my first hypothesis was right the hidden layer should have contained something like <span class="math inline">\(1.0 + 1.0 = 2.0\)</span>. But this could have <strong>never</strong> happened. Why? Look at the activation functions and also at how the new hidden cell is calculated<br>
<span class="math display">\[
h_t = z_t \odot h_{t-1} + (1 - z_t) \odot \hat{h}_t
\]</span></p>
<p>Basically it’s a weighted sum of the <em>old</em> value of the cell and the <em>new</em> value of the cell. If the weight was either <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> in extreme cases this would have meant that either the old value was being kept as it was (ignoring totally the new value) or the converse: the old value was forgotten and the new was taking its place. So we might expect that the truth is in the middle, meaning that <span class="math inline">\(z_t\)</span> was around <span class="math inline">\(.5\)</span> so that it took half the information from the <span class="math inline">\(t-1\)</span> step and half the information from the <em>new</em> value. But does this make sense? Why <span class="math inline">\(\frac{1}{2}\)</span>?</p>
<p>If we see at what the network is doing instead, we observe a cool and symmetrical (I LOVE SYMMETRIES!) behaviour. What is that? If you see the old post you’ll see that <span class="math inline">\(z_t\)</span> takes on some nice values that are</p>
<span class="math display">\[\begin{aligned}

z_1 = 0.8275274634361267 \\
z_2 = 0.15969596803188324 \\
z_3 = 0.8254608511924744 \\
z_4 = 0.1668987274169922 \\
\end{aligned}\]</span>
<p>Basically when the flag is <span class="math inline">\(0\)</span>, then <span class="math inline">\(z_t \ge .825\)</span> otherwise <span class="math inline">\(z_t \le .166\)</span>. So we already observe a pattern. I love this so much! Let’s recall then how <span class="math inline">\(z_t\)</span> is computed:</p>
<p><span class="math display">\[
z_t = \sigma(W_z x_t + U_z h_{t-1} + b_z)
\]</span></p>
<p>where <span class="math inline">\(\sigma\)</span> is the sigmoid function. Recall that the sigmoid function squeezes its input in the range <span class="math inline">\([0,1]\)</span> In the plot below we map <span class="math inline">\(100\)</span> points to the sigmoid function (did you know that <em>function</em>, <em>map</em> and <em>application</em> are quite the same thing in math?)</p>
<div id="9903ab61" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>,<span class="dv">10</span>,<span class="dv">100</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>np.exp(<span class="op">-</span>x))</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>sns.set_theme(style<span class="op">=</span><span class="st">"darkgrid"</span>)  <span class="co"># This gives the typical Seaborn look</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>sns.lineplot(x<span class="op">=</span>x, y<span class="op">=</span>y, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Sigmoid Function'</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'$\sigma(x)$'</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="adding-problem-part2_files/figure-html/cell-2-output-1.png" width="818" height="529" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>So now, let’s do something nice and get back our weights, that are gonna be useful for our next section.</p>
<div id="model-def-load-train" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, TensorDataset</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.notebook <span class="im">import</span> trange, tqdm</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AddingProblemGRU(nn.Module):</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size, hidden_size, output_size):</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(AddingProblemGRU, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gru <span class="op">=</span> nn.GRU(</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>            input_size, hidden_size, num_layers<span class="op">=</span><span class="dv">1</span>, batch_first<span class="op">=</span><span class="va">True</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear <span class="op">=</span> nn.Linear(hidden_size, output_size)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.init_weights()</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> init_weights(<span class="va">self</span>):</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> name, param <span class="kw">in</span> <span class="va">self</span>.gru.named_parameters():</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="st">"weight"</span> <span class="kw">in</span> name:</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>                nn.init.orthogonal_(param)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> <span class="st">"bias"</span> <span class="kw">in</span> name:</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>                nn.init.constant_(param, <span class="dv">0</span>)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>        nn.init.xavier_uniform_(<span class="va">self</span>.linear.weight)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>        nn.init.zeros_(<span class="va">self</span>.linear.bias)</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>        out, hn <span class="op">=</span> <span class="va">self</span>.gru(x)</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.linear(out[:, <span class="op">-</span><span class="dv">1</span>, :])</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output, out</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Reproducibility</span></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>RANDOM_SEED <span class="op">=</span> <span class="dv">37</span></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>np.random.seed(RANDOM_SEED)</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(RANDOM_SEED)</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>torch.cuda.manual_seed_all(RANDOM_SEED)</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>torch.backends.cudnn.deterministic <span class="op">=</span> <span class="va">True</span></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>torch.backends.cudnn.benchmark <span class="op">=</span> <span class="va">False</span></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparameters</span></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>DELTA <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>SEQ_LEN <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>HIGH <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>N_SAMPLES <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>TRAIN_SPLIT <span class="op">=</span> <span class="fl">0.8</span></span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>LEARNING_RATE <span class="op">=</span> <span class="fl">1e-4</span></span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>WEIGHT_DECAY <span class="op">=</span> <span class="fl">1e-5</span></span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>CLIP_VALUE <span class="op">=</span> <span class="fl">2.0</span></span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>NUM_EPOCHS <span class="op">=</span> <span class="dv">3000</span></span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>HIDDEN_SIZE <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>OUTPUT_SIZE <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>INPUT_SIZE <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adding_problem_generator(N, seq_len<span class="op">=</span><span class="dv">6</span>, high<span class="op">=</span><span class="dv">1</span>, delta<span class="op">=</span><span class="fl">0.6</span>):</span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a>    actual_seq_len <span class="op">=</span> np.random.randint(</span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>        <span class="bu">int</span>(seq_len <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> delta)), <span class="bu">int</span>(seq_len <span class="op">*</span> (<span class="dv">1</span> <span class="op">+</span> delta))</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a>    ) <span class="cf">if</span> delta <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> seq_len</span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a>    num_ones <span class="op">=</span> np.random.randint(<span class="dv">2</span>, <span class="bu">min</span>(actual_seq_len <span class="op">-</span> <span class="dv">1</span>, <span class="dv">4</span>))</span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a>    X_num <span class="op">=</span> np.random.randint(low<span class="op">=</span><span class="dv">0</span>, high<span class="op">=</span>high, size<span class="op">=</span>(N, actual_seq_len, <span class="dv">1</span>))</span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>    X_mask <span class="op">=</span> np.zeros((N, actual_seq_len, <span class="dv">1</span>))</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a>    Y <span class="op">=</span> np.ones((N, <span class="dv">1</span>))</span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a>        positions <span class="op">=</span> np.random.choice(actual_seq_len, size<span class="op">=</span>num_ones, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a>        X_mask[i, positions] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a>        Y[i, <span class="dv">0</span>] <span class="op">=</span> np.<span class="bu">sum</span>(X_num[i, positions])</span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> np.append(X_num, X_mask, axis<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X, Y</span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> adding_problem_generator(N_SAMPLES, seq_len<span class="op">=</span>SEQ_LEN, high<span class="op">=</span>HIGH, delta<span class="op">=</span>DELTA)</span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>training_len <span class="op">=</span> <span class="bu">int</span>(TRAIN_SPLIT <span class="op">*</span> N_SAMPLES)</span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a>train_X <span class="op">=</span> X[:training_len]</span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a>test_X <span class="op">=</span> X[training_len:]</span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a>train_Y <span class="op">=</span> Y[:training_len]</span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a>test_Y <span class="op">=</span> Y[training_len:]</span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> TensorDataset(</span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a>    torch.tensor(train_X).<span class="bu">float</span>(), torch.tensor(train_Y).<span class="bu">float</span>()</span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span>BATCH_SIZE, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a>test_dataset <span class="op">=</span> TensorDataset(</span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a>    torch.tensor(test_X).<span class="bu">float</span>(), torch.tensor(test_Y).<span class="bu">float</span>()</span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a>test_loader <span class="op">=</span> DataLoader(test_dataset, batch_size<span class="op">=</span>BATCH_SIZE, shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-92"><a href="#cb2-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-93"><a href="#cb2-93" aria-hidden="true" tabindex="-1"></a><span class="co"># File paths for saved data</span></span>
<span id="cb2-94"><a href="#cb2-94" aria-hidden="true" tabindex="-1"></a>train_losses_path <span class="op">=</span> <span class="st">"train_losses.json"</span></span>
<span id="cb2-95"><a href="#cb2-95" aria-hidden="true" tabindex="-1"></a>test_losses_path <span class="op">=</span> <span class="st">"test_losses.json"</span></span>
<span id="cb2-96"><a href="#cb2-96" aria-hidden="true" tabindex="-1"></a>all_weights_path <span class="op">=</span> <span class="st">"all_weights.json"</span></span>
<span id="cb2-97"><a href="#cb2-97" aria-hidden="true" tabindex="-1"></a>model_save_path <span class="op">=</span> (</span>
<span id="cb2-98"><a href="#cb2-98" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f"gru_adding_problem_model_epochs_</span><span class="sc">{</span>NUM_EPOCHS<span class="sc">}</span><span class="ss">_hidden_</span><span class="sc">{</span>HIDDEN_SIZE<span class="sc">}</span><span class="ss">.pth"</span></span>
<span id="cb2-99"><a href="#cb2-99" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-100"><a href="#cb2-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-101"><a href="#cb2-101" aria-hidden="true" tabindex="-1"></a>FORCE_TRAIN <span class="op">=</span> <span class="va">False</span></span>
<span id="cb2-102"><a href="#cb2-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-103"><a href="#cb2-103" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb2-104"><a href="#cb2-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-105"><a href="#cb2-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-106"><a href="#cb2-106" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.MSELoss()</span>
<span id="cb2-107"><a href="#cb2-107" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate(model, data_loader, criterion, high):</span>
<span id="cb2-108"><a href="#cb2-108" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb2-109"><a href="#cb2-109" aria-hidden="true" tabindex="-1"></a>    total_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb2-110"><a href="#cb2-110" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb2-111"><a href="#cb2-111" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> inputs, labels <span class="kw">in</span> data_loader:</span>
<span id="cb2-112"><a href="#cb2-112" aria-hidden="true" tabindex="-1"></a>            inputs, labels <span class="op">=</span> inputs.to(device), labels.to(device)</span>
<span id="cb2-113"><a href="#cb2-113" aria-hidden="true" tabindex="-1"></a>            inputs[:, :, <span class="dv">0</span>] <span class="op">/=</span> high</span>
<span id="cb2-114"><a href="#cb2-114" aria-hidden="true" tabindex="-1"></a>            outputs, _ <span class="op">=</span> model(inputs)</span>
<span id="cb2-115"><a href="#cb2-115" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> outputs <span class="op">*</span> high</span>
<span id="cb2-116"><a href="#cb2-116" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(outputs, labels)</span>
<span id="cb2-117"><a href="#cb2-117" aria-hidden="true" tabindex="-1"></a>            total_loss <span class="op">+=</span> loss.item() <span class="op">*</span> inputs.size(<span class="dv">0</span>)</span>
<span id="cb2-118"><a href="#cb2-118" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> total_loss <span class="op">/</span> <span class="bu">len</span>(data_loader.dataset)</span>
<span id="cb2-119"><a href="#cb2-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-120"><a href="#cb2-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-121"><a href="#cb2-121" aria-hidden="true" tabindex="-1"></a><span class="co"># Try to load data from files</span></span>
<span id="cb2-122"><a href="#cb2-122" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> FORCE_TRAIN <span class="op">==</span> <span class="va">False</span> <span class="kw">and</span> os.path.exists(train_losses_path) <span class="kw">and</span> os.path.exists(test_losses_path) <span class="kw">and</span> os.path.exists(all_weights_path) <span class="kw">and</span> os.path.exists(model_save_path):</span>
<span id="cb2-123"><a href="#cb2-123" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(train_losses_path, <span class="st">"r"</span>) <span class="im">as</span> f:</span>
<span id="cb2-124"><a href="#cb2-124" aria-hidden="true" tabindex="-1"></a>        train_losses <span class="op">=</span> json.load(f)</span>
<span id="cb2-125"><a href="#cb2-125" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(test_losses_path, <span class="st">"r"</span>) <span class="im">as</span> f:</span>
<span id="cb2-126"><a href="#cb2-126" aria-hidden="true" tabindex="-1"></a>        test_losses <span class="op">=</span> json.load(f)</span>
<span id="cb2-127"><a href="#cb2-127" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(all_weights_path, <span class="st">"r"</span>) <span class="im">as</span> f:</span>
<span id="cb2-128"><a href="#cb2-128" aria-hidden="true" tabindex="-1"></a>        all_weights_loaded <span class="op">=</span> json.load(f)</span>
<span id="cb2-129"><a href="#cb2-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-130"><a href="#cb2-130" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert loaded weights (which are lists) back to numpy arrays</span></span>
<span id="cb2-131"><a href="#cb2-131" aria-hidden="true" tabindex="-1"></a>    all_weights <span class="op">=</span> []</span>
<span id="cb2-132"><a href="#cb2-132" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch_weights_list <span class="kw">in</span> all_weights_loaded:</span>
<span id="cb2-133"><a href="#cb2-133" aria-hidden="true" tabindex="-1"></a>        epoch_weights_dict <span class="op">=</span> {}</span>
<span id="cb2-134"><a href="#cb2-134" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> name, weights_list <span class="kw">in</span> epoch_weights_list.items():</span>
<span id="cb2-135"><a href="#cb2-135" aria-hidden="true" tabindex="-1"></a>            epoch_weights_dict[name] <span class="op">=</span> np.array(weights_list)</span>
<span id="cb2-136"><a href="#cb2-136" aria-hidden="true" tabindex="-1"></a>        all_weights.append(epoch_weights_dict)</span>
<span id="cb2-137"><a href="#cb2-137" aria-hidden="true" tabindex="-1"></a>    <span class="co">#load model</span></span>
<span id="cb2-138"><a href="#cb2-138" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> AddingProblemGRU(</span>
<span id="cb2-139"><a href="#cb2-139" aria-hidden="true" tabindex="-1"></a>    input_size<span class="op">=</span>INPUT_SIZE, hidden_size<span class="op">=</span>HIDDEN_SIZE, output_size<span class="op">=</span>OUTPUT_SIZE)</span>
<span id="cb2-140"><a href="#cb2-140" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(torch.load(model_save_path))</span>
<span id="cb2-141"><a href="#cb2-141" aria-hidden="true" tabindex="-1"></a>    model.to(device)</span>
<span id="cb2-142"><a href="#cb2-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-143"><a href="#cb2-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-144"><a href="#cb2-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-145"><a href="#cb2-145" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb2-146"><a href="#cb2-146" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> AddingProblemGRU(</span>
<span id="cb2-147"><a href="#cb2-147" aria-hidden="true" tabindex="-1"></a>        input_size<span class="op">=</span>INPUT_SIZE, hidden_size<span class="op">=</span>HIDDEN_SIZE, output_size<span class="op">=</span>OUTPUT_SIZE</span>
<span id="cb2-148"><a href="#cb2-148" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb2-149"><a href="#cb2-149" aria-hidden="true" tabindex="-1"></a>    model.to(device)</span>
<span id="cb2-150"><a href="#cb2-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-151"><a href="#cb2-151" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> torch.optim.Adam(</span>
<span id="cb2-152"><a href="#cb2-152" aria-hidden="true" tabindex="-1"></a>        model.parameters(), lr<span class="op">=</span>LEARNING_RATE, weight_decay<span class="op">=</span>WEIGHT_DECAY</span>
<span id="cb2-153"><a href="#cb2-153" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb2-154"><a href="#cb2-154" aria-hidden="true" tabindex="-1"></a>    scheduler <span class="op">=</span> torch.optim.lr_scheduler.ReduceLROnPlateau(</span>
<span id="cb2-155"><a href="#cb2-155" aria-hidden="true" tabindex="-1"></a>        optimizer, mode<span class="op">=</span><span class="st">"min"</span>, factor<span class="op">=</span><span class="fl">0.5</span>, patience<span class="op">=</span><span class="dv">5</span>, min_lr<span class="op">=</span><span class="fl">1e-6</span>, verbose<span class="op">=</span><span class="va">False</span></span>
<span id="cb2-156"><a href="#cb2-156" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb2-157"><a href="#cb2-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-158"><a href="#cb2-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-159"><a href="#cb2-159" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-160"><a href="#cb2-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-161"><a href="#cb2-161" aria-hidden="true" tabindex="-1"></a>    train_losses <span class="op">=</span> []</span>
<span id="cb2-162"><a href="#cb2-162" aria-hidden="true" tabindex="-1"></a>    test_losses <span class="op">=</span> []</span>
<span id="cb2-163"><a href="#cb2-163" aria-hidden="true" tabindex="-1"></a>    all_weights <span class="op">=</span> []</span>
<span id="cb2-164"><a href="#cb2-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-165"><a href="#cb2-165" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> trange(NUM_EPOCHS, desc<span class="op">=</span><span class="st">"Epoch"</span>):</span>
<span id="cb2-166"><a href="#cb2-166" aria-hidden="true" tabindex="-1"></a>        running_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb2-167"><a href="#cb2-167" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> inputs, labels <span class="kw">in</span> train_loader:</span>
<span id="cb2-168"><a href="#cb2-168" aria-hidden="true" tabindex="-1"></a>            inputs, labels <span class="op">=</span> inputs.to(device), labels.to(device)</span>
<span id="cb2-169"><a href="#cb2-169" aria-hidden="true" tabindex="-1"></a>            inputs[:, :, <span class="dv">0</span>] <span class="op">/=</span> HIGH</span>
<span id="cb2-170"><a href="#cb2-170" aria-hidden="true" tabindex="-1"></a>            labels_scaled <span class="op">=</span> labels <span class="op">/</span> HIGH</span>
<span id="cb2-171"><a href="#cb2-171" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb2-172"><a href="#cb2-172" aria-hidden="true" tabindex="-1"></a>            outputs, _ <span class="op">=</span> model(inputs)</span>
<span id="cb2-173"><a href="#cb2-173" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(outputs, labels_scaled)</span>
<span id="cb2-174"><a href="#cb2-174" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb2-175"><a href="#cb2-175" aria-hidden="true" tabindex="-1"></a>            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_VALUE)</span>
<span id="cb2-176"><a href="#cb2-176" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb2-177"><a href="#cb2-177" aria-hidden="true" tabindex="-1"></a>            running_loss <span class="op">+=</span> loss.item() <span class="op">*</span> inputs.size(<span class="dv">0</span>)</span>
<span id="cb2-178"><a href="#cb2-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-179"><a href="#cb2-179" aria-hidden="true" tabindex="-1"></a>        epoch_loss <span class="op">=</span> running_loss <span class="op">/</span> <span class="bu">len</span>(train_loader.dataset)</span>
<span id="cb2-180"><a href="#cb2-180" aria-hidden="true" tabindex="-1"></a>        train_losses.append(epoch_loss)</span>
<span id="cb2-181"><a href="#cb2-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-182"><a href="#cb2-182" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">49</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb2-183"><a href="#cb2-183" aria-hidden="true" tabindex="-1"></a>            test_loss <span class="op">=</span> evaluate(model, test_loader, criterion, HIGH)</span>
<span id="cb2-184"><a href="#cb2-184" aria-hidden="true" tabindex="-1"></a>            test_losses.append(test_loss)</span>
<span id="cb2-185"><a href="#cb2-185" aria-hidden="true" tabindex="-1"></a>            scheduler.step(test_loss)</span>
<span id="cb2-186"><a href="#cb2-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-187"><a href="#cb2-187" aria-hidden="true" tabindex="-1"></a>            weights_dict <span class="op">=</span> {}</span>
<span id="cb2-188"><a href="#cb2-188" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb2-189"><a href="#cb2-189" aria-hidden="true" tabindex="-1"></a>                weights_dict[name] <span class="op">=</span> param.data.cpu().numpy().copy()</span>
<span id="cb2-190"><a href="#cb2-190" aria-hidden="true" tabindex="-1"></a>            all_weights.append(weights_dict)</span>
<span id="cb2-191"><a href="#cb2-191" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb2-192"><a href="#cb2-192" aria-hidden="true" tabindex="-1"></a>            test_losses.append(<span class="va">None</span>)</span>
<span id="cb2-193"><a href="#cb2-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-194"><a href="#cb2-194" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Save data to files</span></span>
<span id="cb2-195"><a href="#cb2-195" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(train_losses_path, <span class="st">"w"</span>) <span class="im">as</span> f:</span>
<span id="cb2-196"><a href="#cb2-196" aria-hidden="true" tabindex="-1"></a>        json.dump(train_losses, f)</span>
<span id="cb2-197"><a href="#cb2-197" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(test_losses_path, <span class="st">"w"</span>) <span class="im">as</span> f:</span>
<span id="cb2-198"><a href="#cb2-198" aria-hidden="true" tabindex="-1"></a>        json.dump(test_losses, f)</span>
<span id="cb2-199"><a href="#cb2-199" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert weights to lists for JSON serialization</span></span>
<span id="cb2-200"><a href="#cb2-200" aria-hidden="true" tabindex="-1"></a>    all_weights_serializable <span class="op">=</span> [</span>
<span id="cb2-201"><a href="#cb2-201" aria-hidden="true" tabindex="-1"></a>        {k: v.tolist() <span class="cf">for</span> k, v <span class="kw">in</span> epoch_weights.items()}</span>
<span id="cb2-202"><a href="#cb2-202" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> epoch_weights <span class="kw">in</span> all_weights</span>
<span id="cb2-203"><a href="#cb2-203" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb2-204"><a href="#cb2-204" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(all_weights_path, <span class="st">"w"</span>) <span class="im">as</span> f:</span>
<span id="cb2-205"><a href="#cb2-205" aria-hidden="true" tabindex="-1"></a>        json.dump(all_weights_serializable, f)</span>
<span id="cb2-206"><a href="#cb2-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-207"><a href="#cb2-207" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Save Model</span></span>
<span id="cb2-208"><a href="#cb2-208" aria-hidden="true" tabindex="-1"></a>    model_save_path <span class="op">=</span> (</span>
<span id="cb2-209"><a href="#cb2-209" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f"gru_adding_problem_model_epochs_</span><span class="sc">{</span>NUM_EPOCHS<span class="sc">}</span><span class="ss">_hidden_</span><span class="sc">{</span>HIDDEN_SIZE<span class="sc">}</span><span class="ss">.pth"</span></span>
<span id="cb2-210"><a href="#cb2-210" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb2-211"><a href="#cb2-211" aria-hidden="true" tabindex="-1"></a>    torch.save(model.state_dict(), model_save_path)</span>
<span id="cb2-212"><a href="#cb2-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-213"><a href="#cb2-213" aria-hidden="true" tabindex="-1"></a>input_sequence <span class="op">=</span> torch.tensor([</span>
<span id="cb2-214"><a href="#cb2-214" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">12</span>,<span class="dv">0</span>],</span>
<span id="cb2-215"><a href="#cb2-215" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">37</span>,<span class="dv">1</span>],</span>
<span id="cb2-216"><a href="#cb2-216" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">12</span>,<span class="dv">0</span>],</span>
<span id="cb2-217"><a href="#cb2-217" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">21</span>,<span class="dv">1</span>],</span>
<span id="cb2-218"><a href="#cb2-218" aria-hidden="true" tabindex="-1"></a>]).<span class="bu">float</span>().unsqueeze(<span class="dv">0</span>)</span>
<span id="cb2-219"><a href="#cb2-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-220"><a href="#cb2-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-221"><a href="#cb2-221" aria-hidden="true" tabindex="-1"></a>input_sequence[:, :, <span class="dv">0</span>] <span class="op">/=</span> HIGH</span>
<span id="cb2-222"><a href="#cb2-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-223"><a href="#cb2-223" aria-hidden="true" tabindex="-1"></a><span class="co"># Get weights from the last training epoch.  'all_weights' is populated by the loading/training section.</span></span>
<span id="cb2-224"><a href="#cb2-224" aria-hidden="true" tabindex="-1"></a>last_epoch_weights <span class="op">=</span> model.state_dict() <span class="co">#all_weights[-1]</span></span>
<span id="cb2-225"><a href="#cb2-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-226"><a href="#cb2-226" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract the relevant weight matrices</span></span>
<span id="cb2-227"><a href="#cb2-227" aria-hidden="true" tabindex="-1"></a>W_ih <span class="op">=</span> torch.tensor(last_epoch_weights[<span class="st">'gru.weight_ih_l0'</span>]).<span class="bu">float</span>()  <span class="co"># Input-to-hidden</span></span>
<span id="cb2-228"><a href="#cb2-228" aria-hidden="true" tabindex="-1"></a>W_hh <span class="op">=</span> torch.tensor(last_epoch_weights[<span class="st">'gru.weight_hh_l0'</span>]).<span class="bu">float</span>()  <span class="co"># Hidden-to-hidden</span></span>
<span id="cb2-229"><a href="#cb2-229" aria-hidden="true" tabindex="-1"></a>b_ih <span class="op">=</span> torch.tensor(last_epoch_weights[<span class="st">'gru.bias_ih_l0'</span>]).<span class="bu">float</span>()  <span class="co"># Input-to-hidden bias</span></span>
<span id="cb2-230"><a href="#cb2-230" aria-hidden="true" tabindex="-1"></a>b_hh <span class="op">=</span> torch.tensor(last_epoch_weights[<span class="st">'gru.bias_hh_l0'</span>]).<span class="bu">float</span>()  <span class="co"># Hidden-to-hidden bias</span></span>
<span id="cb2-231"><a href="#cb2-231" aria-hidden="true" tabindex="-1"></a>W_linear <span class="op">=</span> torch.tensor(last_epoch_weights[<span class="st">'linear.weight'</span>]).<span class="bu">float</span>() <span class="co"># Linear layer weights</span></span>
<span id="cb2-232"><a href="#cb2-232" aria-hidden="true" tabindex="-1"></a>b_linear <span class="op">=</span> torch.tensor(last_epoch_weights[<span class="st">'linear.bias'</span>]).<span class="bu">float</span>()   <span class="co"># Linear layer bias</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="the-update-gate-at-t0" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="the-update-gate-at-t0"><span class="header-section-number">3</span> The Update Gate at <span class="math inline">\(t=0\)</span></h2>
<p>Let’s start our analysis from the bottom up in the calculation of the updated value of the hidden state. For how PyTorch implements the GRU cell under the hood the <span class="math inline">\(z_t\)</span> value is used in this way:</p>
<p><span class="math display">\[
\texttt{hy} = (1 - \texttt{updategate}) * \texttt{newgate} + \texttt{updategate} * \texttt{h}
\]</span></p>
<p>This means basically that the <em>higher</em> the value of <span class="math inline">\(\texttt{updategate}\)</span>, the more we should keep in memory our <em>previous value</em> (sum?) and <em>ignore</em> the newgate (the update in memory, the term to be added to the sum?). This sums up with our previous observation: with flag <span class="math inline">\(1\)</span>, update gate was pretty low and when flag was <span class="math inline">\(0\)</span> then <span class="math inline">\(z_t\)</span> was pretty high. But how much each component is involved in this behaviour?</p>
<div id="0d7e761c" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>x_1 <span class="op">=</span> input_sequence[:, <span class="dv">0</span>, :]</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> torch.zeros(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Stable version (using torch.matmul)</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>gi_stable <span class="op">=</span> torch.matmul(x_1, W_ih.t())</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>gh_stable <span class="op">=</span> torch.matmul(h, W_hh.t())</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Get gates</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>i_r, i_z, i_n <span class="op">=</span> gi_stable.chunk(<span class="dv">3</span>, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>h_r, h_z, h_n <span class="op">=</span> gh_stable.chunk(<span class="dv">3</span>, dim<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply gate operations with controlled precision</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>resetgate <span class="op">=</span> torch.sigmoid(i_r <span class="op">+</span> h_r  <span class="op">+</span> b_ih[<span class="dv">0</span>] <span class="op">+</span> b_hh[<span class="dv">0</span>])</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>updategate <span class="op">=</span> torch.sigmoid(i_z <span class="op">+</span> h_z <span class="op">+</span> b_ih[<span class="dv">1</span>] <span class="op">+</span> b_hh[<span class="dv">1</span>])</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>newgate <span class="op">=</span> torch.tanh(i_n <span class="op">+</span> b_ih[<span class="dv">2</span>] <span class="op">+</span> (resetgate <span class="op">*</span> (h_n <span class="op">+</span> b_hh[<span class="dv">2</span>])))</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>hy <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> updategate) <span class="op">*</span> newgate <span class="op">+</span> updategate <span class="op">*</span> h</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>h_t <span class="op">=</span> hy</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>These are the three components that go through the sigmoid for the update gate:</p>
<p><span class="math display">\[\eqalign{
b_{ih_z} +  b_{hh_z} &amp;= 1.6185756921768188 \cr
i_z &amp;= -0.05037130042910576; \cr
h_z &amp;= -0.0; \cr
i_z + h_z + b_{ih_z} + b_{hh_z} &amp;= 1.568204402923584 \cr
z_t &amp;= 0.8275274634361267 \cr
}
\]</span></p>
<p>This is interesting. So let’s break this down a little bit.</p>
<p>Probably it’s a good idea to verify how each term here is calculated and what happens. Let’s begin with seeing how <span class="math inline">\(i_z\)</span> is calculated, using the original weight matrix.</p>
<p><span class="math display">\[\eqalign{
i_z &amp;= x_t \cdot W_{ih_z}^T\cr
W_{ih_z}^T &amp;=
\begin{bmatrix}
-0.4197608530521393 \\ -3.02486252784729
\end{bmatrix} \cr
}
\]</span></p>
<p>What we do to obtain <span class="math inline">\(i_z\)</span> is multiply our input row vector <span class="math inline">\(x_0^T = \left[\begin{smallmatrix}0.12 \\ 0.0\end{smallmatrix}\right]\)</span> by the column vector <span class="math inline">\(W_{ih_z}^T\)</span></p>
<p><span class="math display">\[
\begin{bmatrix}0.12 &amp; 0.0\end{bmatrix}\cdot \begin{bmatrix}
-0.4197608530521393 \\ -3.02486252784729
\end{bmatrix}
\]</span></p>
<p>which becomes:</p>
<p><span class="math display">\[\eqalign{
i_z &amp;= 0.12  \cdot -0.4197608530521393 + 0.0 \cdot  -3.02486252784729 \cr
i_z &amp;= 0.12  \cdot -0.4197608530521393 + \cancel{0.0 \cdot  -3.02486252784729} (\texttt{flag}=0) \cr
i_z &amp;= 0.12  \cdot -0.4197608530521393 \cr
i_z &amp;= -0.05037130042910576
}
\]</span></p>
<p>Now this conveys a super important information: when the <span class="math inline">\(\texttt{flag} = 0\)</span> then only the number has some importance in the final calculation because the flag cancels the second term of the sum as saw before. Now remember, this is only one of the three terms that go through the sigmoid at the end to obtain the final <span class="math inline">\(z_t\)</span> term.</p>
<p>So let’s carry on with the second part of it, <span class="math inline">\(h_z\)</span>: basically it’s <span class="math inline">\(0\)</span> since when our GRU cell sees the input for the first time its input state is <span class="math inline">\(h=0\)</span>, and whatever we multiply here stays zero.</p>
<p><span class="math display">\[
h_z = -0.0
\]</span></p>
<p>Now the last portion of our sum, the bias. Now, you should know that many ML scientists avoid using the bias term when the data is already centered or when the model inherently accounts for offsets, as it can be redundant and complicate interpretation. But in my case the bias was left there. And in the first calculation you can observe that without the bias term accounts super heavily on the final sum:</p>
<p><span class="math display">\[\eqalign{
b_{ih_z} &amp;= 0.8092878460884094 \cr
b_{hh_z} &amp;= 0.8092878460884094 \cr
}
\]</span></p>
<p>So the final sum is:</p>
<p><span class="math display">\[
\texttt{temp} = -0.05037130042910576 + 0 + 0.8092878460884094 + 0.8092878460884094;
\]</span></p>
<p>I’ve always believed that a plot tells more then hundred numbers, so let’s plot a cumulative sum of the four terms to check how much each of them accounts for the gran total:</p>
<div id="ef138241" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">"text.usetex"</span>] <span class="op">=</span> <span class="va">True</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>items <span class="op">=</span> [<span class="vs">r"$i_z$"</span>, <span class="vs">r"$h_z$"</span>, <span class="vs">r"$b_</span><span class="sc">{ih_z}</span><span class="vs">$"</span>, <span class="vs">r"$b_</span><span class="sc">{hh_z}</span><span class="vs">$"</span>]</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> [(input_sequence[<span class="dv">0</span>][<span class="dv">0</span>][<span class="dv">0</span>] <span class="op">*</span> W_ih[<span class="dv">1</span>][<span class="dv">0</span>]).item(), <span class="dv">0</span>, (b_ih[<span class="dv">1</span>]).item(), (b_hh[<span class="dv">1</span>]).item()]</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>cumsum_values <span class="op">=</span> np.cumsum(values)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({<span class="st">"Term"</span>: items, <span class="st">"Cumulative Sum"</span>: cumsum_values})</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>sns.barplot(x<span class="op">=</span><span class="st">"Term"</span>, y<span class="op">=</span><span class="st">"Cumulative Sum"</span>, data<span class="op">=</span>df, color<span class="op">=</span><span class="st">"skyblue"</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (item, value) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(items, values)):</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    plt.text(i, cumsum_values[i] <span class="op">-</span> value <span class="op">/</span> <span class="dv">2</span>, <span class="ss">f"$+</span><span class="sc">{</span>value<span class="sc">}</span><span class="ss">$"</span>, ha<span class="op">=</span><span class="st">"center"</span>, fontsize<span class="op">=</span><span class="dv">12</span>, color<span class="op">=</span><span class="st">"black"</span>)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"\textbf{Cumulative Sum}"</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"\textbf</span><span class="sc">{Terms}</span><span class="vs">"</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="vs">r"\textbf{Cumulative Sum Contribution}"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="bu">max</span>(cumsum_values) <span class="op">+</span> <span class="fl">.5</span>)  <span class="co"># Adjust y-limit</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="adding-problem-part2_files/figure-html/cell-5-output-1.png" width="673" height="456" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Wow, so basically during the first iteration, we’re only dealing with bias terms. An important distinction to remember is that bias terms don’t depend on or connect to any individual data point being analyzed. Instead, they capture and convey overall patterns or information from the entire dataset as a whole.</p>
<p>To complete this first step, let’s do a sanity check. I plot here what would have happened after the first iteration to a number in the range <span class="math inline">\([0, 100]\)</span> (remember normalized by a factor of <span class="math inline">\(100\)</span>, so in the final range of <span class="math inline">\([0,1]\)</span>) There are 4 plots:</p>
<ol type="1">
<li><p><strong><span class="math inline">\(y_0\)</span> with bias</strong>: In our trained GRU cell (green line), when <span class="math inline">\(\sigma\)</span> is applied to an input flagged with <span class="math inline">\(0\)</span>, the values oscillate between <span class="math inline">\([0.77, 0.83]\)</span>.</p></li>
<li><p><strong><span class="math inline">\(y_1\)</span> with bias</strong>: In our trained GRU cell (red line), when <span class="math inline">\(\sigma\)</span> is applied to an input flagged with <span class="math inline">\(1\)</span>, the values oscillate between <span class="math inline">\([0.14, 0.20]\)</span>.</p></li>
<li><p><strong><span class="math inline">\(y_0\)</span> no bias</strong>: If we removed the bias term <span class="math inline">\(b_z\)</span> from the GRU cell (blue line) and applied <span class="math inline">\(\sigma\)</span> to an input flagged with <span class="math inline">\(0\)</span>, the value stays around <span class="math inline">\(0.5\)</span>. The reason for the cell choosing such a strong bias remains unclear - it seems significant since it pushes <span class="math inline">\(z_t\)</span> considerably higher.</p></li>
<li><p><strong><span class="math inline">\(y_1\)</span> no bias</strong>: If we removed the bias term <span class="math inline">\(b_z\)</span> from the GRU cell (orange line), and applied <span class="math inline">\(\sigma\)</span> to an input flagged with <span class="math inline">\(1\)</span>, the values oscillate between <span class="math inline">\(0.05\)</span>. Again, the strong bias choice is super cool. One (crazy!) hypothesis: it pushes up <span class="math inline">\(z_t\)</span> to act as a <em>factor <span class="math inline">\(\frac{1}{4}\)</span></em>, possibly because the network expects 4 items where 2 flags are in unknown positions and maintains this factor to account for missing items by adding <span class="math inline">\(\frac{1}{4}\)</span> of the current candidate.</p></li>
</ol>
<p>The takeaway here is how the bias term strongly affects the outputs, particularly in pushing up <span class="math inline">\(z_t\)</span>. While we can hypothesize about the network’s strategy (especially regarding the <span class="math inline">\(\frac{1}{4}\)</span> factor), the exact reason for such a strong bias will be hopefully uncovered in the next sections.</p>
<div id="cell-fig-zt-activation-plot" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="lst-zt-activation-plot"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="lst-zt-activation-plot-1"><a href="#lst-zt-activation-plot-1" aria-hidden="true" tabindex="-1"></a>bz <span class="op">=</span>  b_ih[<span class="dv">1</span>].item() <span class="op">+</span> b_hh[<span class="dv">1</span>].item()</span>
<span id="lst-zt-activation-plot-2"><a href="#lst-zt-activation-plot-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">1</span>, <span class="dv">1000</span>)</span>
<span id="lst-zt-activation-plot-3"><a href="#lst-zt-activation-plot-3" aria-hidden="true" tabindex="-1"></a>y0 <span class="op">=</span> x <span class="op">*</span> W_ih[<span class="dv">1</span>][<span class="dv">0</span>].item()</span>
<span id="lst-zt-activation-plot-4"><a href="#lst-zt-activation-plot-4" aria-hidden="true" tabindex="-1"></a>y1 <span class="op">=</span> x <span class="op">*</span> W_ih[<span class="dv">1</span>][<span class="dv">0</span>].item() <span class="op">+</span> W_ih[<span class="dv">1</span>][<span class="dv">1</span>].item() </span>
<span id="lst-zt-activation-plot-5"><a href="#lst-zt-activation-plot-5" aria-hidden="true" tabindex="-1"></a>y0_nb <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>(y0)))</span>
<span id="lst-zt-activation-plot-6"><a href="#lst-zt-activation-plot-6" aria-hidden="true" tabindex="-1"></a>y1_nb <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>(y1)))</span>
<span id="lst-zt-activation-plot-7"><a href="#lst-zt-activation-plot-7" aria-hidden="true" tabindex="-1"></a>y0_b <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>(y0 <span class="op">+</span> bz)))</span>
<span id="lst-zt-activation-plot-8"><a href="#lst-zt-activation-plot-8" aria-hidden="true" tabindex="-1"></a>y1_b <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>(y1 <span class="op">+</span> bz)))</span>
<span id="lst-zt-activation-plot-9"><a href="#lst-zt-activation-plot-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-zt-activation-plot-10"><a href="#lst-zt-activation-plot-10" aria-hidden="true" tabindex="-1"></a>sns.set_theme(style<span class="op">=</span><span class="st">"darkgrid"</span>)  <span class="co"># This gives the typical Seaborn look</span></span>
<span id="lst-zt-activation-plot-11"><a href="#lst-zt-activation-plot-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-zt-activation-plot-12"><a href="#lst-zt-activation-plot-12" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="lst-zt-activation-plot-13"><a href="#lst-zt-activation-plot-13" aria-hidden="true" tabindex="-1"></a>sns.lineplot(x<span class="op">=</span>x, y<span class="op">=</span>y0_nb, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'$y_0$ no bias'</span>)</span>
<span id="lst-zt-activation-plot-14"><a href="#lst-zt-activation-plot-14" aria-hidden="true" tabindex="-1"></a>sns.lineplot(x<span class="op">=</span>x, y<span class="op">=</span>y1_nb, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'$y_1$ no bias'</span>)</span>
<span id="lst-zt-activation-plot-15"><a href="#lst-zt-activation-plot-15" aria-hidden="true" tabindex="-1"></a>sns.lineplot(x<span class="op">=</span>x, y<span class="op">=</span>y0_b, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'$y_0$ with bias'</span>)</span>
<span id="lst-zt-activation-plot-16"><a href="#lst-zt-activation-plot-16" aria-hidden="true" tabindex="-1"></a>sns.lineplot(x<span class="op">=</span>x, y<span class="op">=</span>y1_b, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'$y_1$ with bias'</span>)</span>
<span id="lst-zt-activation-plot-17"><a href="#lst-zt-activation-plot-17" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Sigmoid Function'</span>)</span>
<span id="lst-zt-activation-plot-18"><a href="#lst-zt-activation-plot-18" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="lst-zt-activation-plot-19"><a href="#lst-zt-activation-plot-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-zt-activation-plot-20"><a href="#lst-zt-activation-plot-20" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'$i_z$'</span>)</span>
<span id="lst-zt-activation-plot-21"><a href="#lst-zt-activation-plot-21" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="lst-zt-activation-plot-22"><a href="#lst-zt-activation-plot-22" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="lst-zt-activation-plot-23"><a href="#lst-zt-activation-plot-23" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-zt-activation-plot" class="quarto-float quarto-figure quarto-figure-center anchored" width="814" height="527">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-zt-activation-plot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="adding-problem-part2_files/figure-html/fig-zt-activation-plot-output-1.png" id="fig-zt-activation-plot" width="814" height="527" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-zt-activation-plot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="the-reset-gate-at-t0" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="the-reset-gate-at-t0"><span class="header-section-number">4</span> The Reset gate at <span class="math inline">\(t=0\)</span></h2>
<p>I lied earlier. I said we were going bottom up, but we’re actually approaching this top down because next in our discussion is the reset gate. The reasons is easy to tell: the update gate value <span class="math inline">\(z_t\)</span> acts on the previous <span class="math inline">\(h_{t-1}\)</span> as well as on <span class="math inline">\(\hat{h}_t\)</span> (the candidate memory update). But to make a discussion about <span class="math inline">\(\hat{h}_t\)</span> we need to discuss the reset gate <span class="math inline">\(r_t\)</span> first.</p>
<p>Let’s recall again how both the reset gate and newgate are computed</p>
<p><span class="math display">\[\eqalign{
r_t &amp;= \sigma(W_r x_t + U_r h_{t-1} + b_r) \\
\hat{h}_t &amp;= \phi(W_h x_t + U_h (r_t \odot h_{t-1}) + b_h) \\
}
\]</span></p>
<p>First of all, you might have noticed already in the previous post that there was a different activation function in one step, which is <span class="math inline">\(\phi\)</span>. But what is <span class="math inline">\(\phi\)</span>? It is the <span class="math inline">\(\tanh\)</span>, which yields a plot like this:</p>
<div id="f312ac08" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>,<span class="dv">10</span>,<span class="dv">100</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.tanh(x)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>sns.set_theme(style<span class="op">=</span><span class="st">"darkgrid"</span>)  <span class="co"># This gives the typical Seaborn look</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>sns.lineplot(x<span class="op">=</span>x, y<span class="op">=</span>y, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Sigmoid Function'</span>)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="va">True</span>)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'$\sigma(x)$'</span>)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="adding-problem-part2_files/figure-html/cell-7-output-1.png" width="834" height="527" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Previously <span class="math inline">\(\sigma\)</span> was <em>squeezing</em> its input in the range <span class="math inline">\([0, 1]\)</span>, whereas <span class="math inline">\(\phi\)</span> maps its input to a value in the range <span class="math inline">\([-1, 1]\)</span>. The reason why the update gate employs this activation function is that the range ([-1, 1]) allows the gate to dynamically amplify, suppress, or invert features from the previous hidden state, enabling more nuanced control over the update mechanism compared to the purely additive or multiplicative behavior of <span class="math inline">\([0, 1]\)</span>. Moreover: this symmetry and bidirectional scaling can improve gradient flow during training and help the model learn richer representations by incorporating both positive and negative adjustments to the hidden state. In other words <span class="math inline">\(\sigma\)</span> was telling: “How much of this should I add to this other thing?”, whereas <span class="math inline">\(\phi\)</span> is telling “How much of this should I add or remove from this other thing?”</p>
<p>Now, why is this important? Remember that the GRU cell during its computation tries to <em>build</em> an internal hidden state that conveys some information. Think of the hidden state as some sort of scratchpad, where you take notes as you read more data. Sometimes some of the new information should be added to the previous state, sometimes new data should be ignored. Sometimes new data instead needs to kinda <em>remove</em> information from the state, in order to have fresher information.</p>
<p>Imagine for the sake of example to be a detective, which is trying to solve a mystery. You might get new information as time flows. At some point you might even have a track on a suspect and build your knowledge on that. But then at some point, you find out that your suspect was just cheating on his wife and as such you need to forget about him, otherwise you’ll focus on something that’s not needed in your ivnestigation.</p>
<p>How does our network know what to forget, what to keep, what to update and so on is the task of machine learning. But we can surely observe what happened here!</p>
<p>First we need to study the reset gate <span class="math inline">\(r_t\)</span>, similarly to how we did before!</p>
<p>Again, there are three components that go through the sigmoid for the update gate:</p>
<p><span class="math display">\[\eqalign{
b_{ih_r} +  b_{hh_r} &amp;= 1.4853122234344482 \cr
i_r &amp;= 0.027704410254955292;  \cr
h_r &amp;= -0.0;  \cr
i_r + h_r + b_{ih_r} + b_{hh_r} &amp;= 1.513016700744629 \cr
r_t &amp;= 0.819507896900177 \cr
}
\]</span></p>
<p>As we observed before the bias is king again, because it accounts for the largest part of the activation. But we can observe something else here, let’s compare how the flag affects both the update (<span class="math inline">\(z_t\)</span>) and reset (<span class="math inline">\(r_t\)</span>) gate:</p>
<p>Update gate <span class="math inline">\(z_t\)</span> with <span class="math inline">\(\texttt{flag} = 0\)</span></p>
<p><span class="math display">\[\eqalign{
i_z + h_z + b_{ih_z} + b_{hh_z} &amp;= 1.568204402923584\cr
z_t &amp;= 0.8275274634361267 \cr
}\]</span> and with <span class="math inline">\(\texttt{flag} = 1\)</span> <span class="math display">\[\eqalign{
i_z + h_z + b_{ih_z} + b_{hh_z} &amp;= -1.456658124923706\cr
z   _t &amp;= 0.18897898495197296 \cr
}\]</span></p>
<p>In <a href="#fig-zt-activation-plot" class="quarto-xref">Figure&nbsp;1</a> we saw how <span class="math inline">\(z_t\)</span> was affected pretty heavily by the flag at position <span class="math inline">\(1\)</span> of the input item, whereas in this case we observe that the reset gate is not affected too much about the flag. Basically it looks like if we have <span class="math inline">\(\texttt{flag} = 0\)</span></p>
<p><span class="math display">\[\eqalign{
i_r + h_r + b_{ih_r} + b_{hh_r} &amp;= 1.513016700744629\cr
r_t &amp;= 0.819507896900177 \cr
}\]</span></p>
<p>or <span class="math inline">\(\texttt{flag} = 1\)</span></p>
<p><span class="math display">\[\eqalign{
i_r + h_r + b_{ih_r} + b_{hh_r} &amp;= 2.146040916442871\cr
r_t &amp;= 0.8952982425689697 \cr
}\]</span></p>
<p>the reset gate <span class="math inline">\(r_t\)</span> keeps taking a pretty large value (remember that sigmoid is in the range <span class="math inline">\([0,1]\)</span>, so you can consider it as a <em>percentage</em>, meaning that the reset gate is always above <span class="math inline">\(80\%\)</span>)</p>
<p>Why is this? Let’s take a look at the weights involved. Again being this the first step in the sequence the hidden state is not yer involved and we can safely (FOR NOW!) ignore it.</p>
<p><span class="math display">\[\eqalign{
i_r &amp;= x_t \cdot W_{ih_r}^T\cr
W_{ih_r}^T &amp;=
\begin{bmatrix}
0.2308700829744339 \\ 0.6330242156982422
\end{bmatrix} \cr
}\]</span></p>
<p>Let’s compare it with the update gate weights: <span class="math display">\[\eqalign{
i_z &amp;= x_t \cdot W_{ih_z}^T\cr
W_{ih_z}^T &amp;=
\begin{bmatrix}
-0.4197608530521393 \\ -3.02486252784729
\end{bmatrix} \cr
}\]</span></p>
<p>We can take two key observations:</p>
<ol type="1">
<li>The update weights are both negative and reset are both positive</li>
<li>The difference in magnitude is noteworthy: in the update gate is heavily affected by the flag input, moving the activation along the <span class="math inline">\(\hat{y}\)</span>, whereas the reset gate is not so strongly affected by it.</li>
</ol>
<p>Let’s plot them both here where <span class="math inline">\(y_{0z}\)</span> and <span class="math inline">\(y_{1z}\)</span> are the update gates when <span class="math inline">\(\texttt{flag} = 0\)</span> and <span class="math inline">\(\texttt{flag} = 1\)</span>, and <span class="math inline">\(y_{0r}\)</span> and <span class="math inline">\(y_{1r}\)</span> are the reset gates in the same cases, respectively.</p>
<div id="cell-fig-rt-zt-activation-plot" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="lst-rt-zt-activation-plot"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="lst-rt-zt-activation-plot-1"><a href="#lst-rt-zt-activation-plot-1" aria-hidden="true" tabindex="-1"></a>br <span class="op">=</span>  b_ih[<span class="dv">0</span>].item() <span class="op">+</span> b_hh[<span class="dv">0</span>].item()</span>
<span id="lst-rt-zt-activation-plot-2"><a href="#lst-rt-zt-activation-plot-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">1</span>, <span class="dv">1000</span>)</span>
<span id="lst-rt-zt-activation-plot-3"><a href="#lst-rt-zt-activation-plot-3" aria-hidden="true" tabindex="-1"></a>y0 <span class="op">=</span> x <span class="op">*</span> W_ih[<span class="dv">1</span>][<span class="dv">0</span>].item()</span>
<span id="lst-rt-zt-activation-plot-4"><a href="#lst-rt-zt-activation-plot-4" aria-hidden="true" tabindex="-1"></a>y1 <span class="op">=</span> x <span class="op">*</span> W_ih[<span class="dv">1</span>][<span class="dv">0</span>].item() <span class="op">+</span> W_ih[<span class="dv">1</span>][<span class="dv">1</span>].item() </span>
<span id="lst-rt-zt-activation-plot-5"><a href="#lst-rt-zt-activation-plot-5" aria-hidden="true" tabindex="-1"></a>y0_r <span class="op">=</span> x <span class="op">*</span> W_ih[<span class="dv">0</span>][<span class="dv">0</span>].item()</span>
<span id="lst-rt-zt-activation-plot-6"><a href="#lst-rt-zt-activation-plot-6" aria-hidden="true" tabindex="-1"></a>y1_r <span class="op">=</span> x <span class="op">*</span> W_ih[<span class="dv">0</span>][<span class="dv">0</span>].item() <span class="op">+</span> W_ih[<span class="dv">0</span>][<span class="dv">1</span>].item() </span>
<span id="lst-rt-zt-activation-plot-7"><a href="#lst-rt-zt-activation-plot-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-rt-zt-activation-plot-8"><a href="#lst-rt-zt-activation-plot-8" aria-hidden="true" tabindex="-1"></a>y0_b <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>(y0 <span class="op">+</span> br)))</span>
<span id="lst-rt-zt-activation-plot-9"><a href="#lst-rt-zt-activation-plot-9" aria-hidden="true" tabindex="-1"></a>y1_b <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>(y1 <span class="op">+</span> br)))</span>
<span id="lst-rt-zt-activation-plot-10"><a href="#lst-rt-zt-activation-plot-10" aria-hidden="true" tabindex="-1"></a>y0_r_b <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>(y0_r <span class="op">+</span> br)))</span>
<span id="lst-rt-zt-activation-plot-11"><a href="#lst-rt-zt-activation-plot-11" aria-hidden="true" tabindex="-1"></a>y1_r_b <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>(y1_r <span class="op">+</span> br)))</span>
<span id="lst-rt-zt-activation-plot-12"><a href="#lst-rt-zt-activation-plot-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-rt-zt-activation-plot-13"><a href="#lst-rt-zt-activation-plot-13" aria-hidden="true" tabindex="-1"></a>sns.set_theme(style<span class="op">=</span><span class="st">"darkgrid"</span>)</span>
<span id="lst-rt-zt-activation-plot-14"><a href="#lst-rt-zt-activation-plot-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-rt-zt-activation-plot-15"><a href="#lst-rt-zt-activation-plot-15" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="lst-rt-zt-activation-plot-16"><a href="#lst-rt-zt-activation-plot-16" aria-hidden="true" tabindex="-1"></a>sns.lineplot(x<span class="op">=</span>x, y<span class="op">=</span>y0_b, linewidth<span class="op">=</span><span class="dv">2</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, color<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'$y_</span><span class="sc">{0z}</span><span class="st">$'</span>)</span>
<span id="lst-rt-zt-activation-plot-17"><a href="#lst-rt-zt-activation-plot-17" aria-hidden="true" tabindex="-1"></a>sns.lineplot(x<span class="op">=</span>x, y<span class="op">=</span>y1_b, linewidth<span class="op">=</span><span class="dv">2</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, color<span class="op">=</span><span class="st">'orange'</span>, label<span class="op">=</span><span class="st">'$y_</span><span class="sc">{1z}</span><span class="st">$'</span>)</span>
<span id="lst-rt-zt-activation-plot-18"><a href="#lst-rt-zt-activation-plot-18" aria-hidden="true" tabindex="-1"></a>sns.lineplot(x<span class="op">=</span>x, y<span class="op">=</span>y0_r_b, linewidth<span class="op">=</span><span class="dv">2</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, color<span class="op">=</span><span class="st">'green'</span>, label<span class="op">=</span><span class="st">'$y_</span><span class="sc">{0r}</span><span class="st">$'</span>)</span>
<span id="lst-rt-zt-activation-plot-19"><a href="#lst-rt-zt-activation-plot-19" aria-hidden="true" tabindex="-1"></a>sns.lineplot(x<span class="op">=</span>x, y<span class="op">=</span>y1_r_b, linewidth<span class="op">=</span><span class="dv">2</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, color<span class="op">=</span><span class="st">'red'</span>, label<span class="op">=</span><span class="st">'$y_</span><span class="sc">{1r}</span><span class="st">$'</span>)</span>
<span id="lst-rt-zt-activation-plot-20"><a href="#lst-rt-zt-activation-plot-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst-rt-zt-activation-plot-21"><a href="#lst-rt-zt-activation-plot-21" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Comparison between $z_t$ and $r_t$'</span>)</span>
<span id="lst-rt-zt-activation-plot-22"><a href="#lst-rt-zt-activation-plot-22" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="lst-rt-zt-activation-plot-23"><a href="#lst-rt-zt-activation-plot-23" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'$i_r$'</span>)</span>
<span id="lst-rt-zt-activation-plot-24"><a href="#lst-rt-zt-activation-plot-24" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="lst-rt-zt-activation-plot-25"><a href="#lst-rt-zt-activation-plot-25" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="lst-rt-zt-activation-plot-26"><a href="#lst-rt-zt-activation-plot-26" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-rt-zt-activation-plot" class="quarto-float quarto-figure quarto-figure-center anchored" width="814" height="527">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rt-zt-activation-plot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="adding-problem-part2_files/figure-html/fig-rt-zt-activation-plot-output-1.png" id="fig-rt-zt-activation-plot" width="814" height="527" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-rt-zt-activation-plot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2
</figcaption>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p>This is all cool and stuff… But I forgot to mention that is also <strong>useless</strong>. Just kidding. But thre’s an actual catch here: at <span class="math inline">\(t=0\)</span> this is acting only on the bias term because <span class="math inline">\(r_t\)</span> is applied via a Hadamard product (element-wise multiplication) to the previous hidden state plus the bias. Its purpose is to control how much of it we want to remember. But at <span class="math inline">\(t=0\)</span>, <span class="math inline">\(h_t = h_0 = 0\)</span>, meaning <span class="math inline">\(r_0\)</span> only acts on the bias term!</p>
</div>
</div>
</section>
<section id="the-new-gate-at-t0" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="the-new-gate-at-t0"><span class="header-section-number">5</span> The New gate at <span class="math inline">\(t=0\)</span></h2>
<p>You will find in literature multiple ways to refer to it, but I think the most useful is: <em>candidate</em> <span class="math inline">\(\hat{h}_t\)</span>. Basically, it weights some of the input <span class="math inline">\(\mathbf{x}_t\)</span> and some of the <span class="math inline">\(h_{t-1}\)</span> value using the reset gate <span class="math inline">\(r_t\)</span> to decide how much it should be brought to the activation. In other words, the candidate <span class="math inline">\(\hat{h}_t\)</span> represents a proposed new hidden state that combines the current input <span class="math inline">\(\mathbf{x}_t\)</span> and the previous hidden state <span class="math inline">\(h_{t-1}\)</span>, modulated by the reset gate <span class="math inline">\(r_t\)</span>.</p>
<p>Let’s recall how the candidate hidden state <span class="math inline">\(\hat{h}_t\)</span> is computed:</p>
<p><span class="math display">\[
\hat{h}_t = \phi(W_h \mathbf{x}_t + b_{{ih}_n} + r_t \odot  [(U_h \cdot h_{t-1}) + b_{{hh}_n}])
\]</span> where <span class="math inline">\(\phi = \tanh(\cdot)\)</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Up until now we always treated the bias terms as a single value, summing them up. But now we need to make extra care! Previously we had 2 terms added up and each of them had its bias, so we could safely add them either during the $$ product or later when inside the <span class="math inline">\(\sigma\)</span> sigmoid. But now, the bias term is multipled to the reset gate as much as the projected hidden state.</p>
</div>
</div>
<p>One term cancels out, because at <span class="math inline">\(t=0\)</span> <span class="math inline">\(h_0 = 0\)</span>:</p>
<p><span class="math display">\[\eqalign{
\hat{h}_t &amp;= \phi(W_h \mathbf{x}_t + b_{{ih}_n} + r_t \odot  [\cancel{(U_h \cdot h_{t-1})} + b_{{hh}_n}])\cr
\hat{h}_t &amp;= \phi(W_h \mathbf{x}_t + b_{{ih}_n} + r_t \odot  b_{{hh}_n})
}\]</span></p>
<p>from the previous section we learnt that <span class="math inline">\(r_t\)</span> has always values above <span class="math inline">\(.8\)</span>, which at <span class="math inline">\(t=0\)</span> means that we carry on with us <span class="math inline">\(80\%\)</span> of the bias in the activatoin <span class="math inline">\(\phi\)</span>.</p>
<p>Now this leaves us with these terms: <span class="math display">\[\eqalign{
b_{ih_n}  &amp;= 0.6560405492782593 \cr
b_{hh_n} &amp;= 0.16526484489440918 \cr
i_n &amp;= -0.03407169133424759;  \cr
h_t &amp;= 0.0;  \cr
h_t + b_{hh_n} &amp;= 0.16526484489440918;  \cr
\hat{h}_t &amp;= \phi(i_n + b_{ih_n} + r_t \odot b_{hh_n}) \cr
\hat{h}_t &amp;= \phi(-0.03407169133424759 + 0.6560405492782593 + 0.819507896900177 \odot 0.16526484489440918) \cr
\hat{h}_t &amp;= \phi(0.7574047034149629) \cr
\hat{h}_t &amp;= 0.639545738697052 \cr
}
\]</span></p>
<p>Again, bias is king! It accounts for basically the whole activation. And again a reminder about it:</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>Bias is not about a single data point. Instead, it reflects a systematic shift in the entire dataset or even during the learning process. It tells us something about the data as a whole, rather than just the point we’re observing. This is so important: <em>no matter what data</em> we input, the bias would remain the same!</p>
</div>
</div>
</section>
<section id="final-step" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="final-step"><span class="header-section-number">6</span> Final step</h2>
<p>So we’re finally ready for our final hidden state:</p>
<p><span class="math display">\[\eqalign{
h_t &amp;= (1 - z_t) \cdot  \hat{h} + z_t \cdot h_0 \cr
h_t &amp;= 0.1724725365638733 \cdot 0.639545738697052 + \cancel{0.8275274634361267 \cdot 0.0}\cr
h_t &amp;= 0.11030407580169665 + \cancel{0.0}\cr
h_t &amp;= 0.11030407249927521\cr
}\]</span></p>
<p>Now: this is a Recurrent Neural Network, so theory tells us that it should be able to handle sequences of arbitrary length. But is it?</p>
<p>Let’s try to just project into the final output through the linear layer:</p>
<p><span class="math display">\[
\text{x} =
\begin{bmatrix}
12 &amp; 0 \\
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\hat{y} = 41.076526045799255 \neq 0
\]</span></p>
<p>Woooh! That’s waaay off. What if we let the flag be one instead?</p>
<p><span class="math display">\[
\text{x} =
\begin{bmatrix}
12 &amp; 1 \\
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\hat{y} = 101.57540893554688 \gg 12
\]</span></p>
<p>Way off again!</p>
<p>Wait… let’s see hat happend if we put 4 elements, 2 of which are set to true?</p>
<p><span class="math display">\[
\text{x} =
\left[
\begin{bmatrix}
75 \\ 1
\end{bmatrix}
\begin{bmatrix}
38 \\ 1
\end{bmatrix}
\begin{bmatrix}
12 \\ 0
\end{bmatrix}
\begin{bmatrix}
12 \\ 0
\end{bmatrix}
\right]
\]</span></p>
<p>This shows your 2-dimensional temporal sequence in the format you wanted. Let me know if you’d like to adjust the spacing or format further.</p>
<p><span class="math display">\[
\hat{y} = 112.553955078125 \approxeq 113 = y
\]</span></p>
<p>Mmmmh! That works much better. But why? It seems the network has learned three key things:</p>
<ol type="1">
<li><strong>It’s a sum</strong>: The network has learned to sum only the numbers flagged with <span class="math inline">\(1\)</span>. Well, that was actually the main task.<br>
</li>
<li><strong>Fixed length <span class="math inline">\(n = 4\)</span></strong>: It expects an input sequence of length <span class="math inline">\(n = 4\)</span>.<br>
</li>
<li><strong>Exactly two flagged items</strong>: It assumes that exactly two elements in the input are flagged with <span class="math inline">\(1\)</span>.</li>
</ol>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
First Step Insights: What Have We Learned?
</div>
</div>
<div class="callout-body-container callout-body">
<p>Alright, let’s pause and take stock. After dissecting the GRU’s very first move, some things are becoming clearer. We’re seeing how much those <strong>bias terms</strong> are driving the initial behavior – they’re the unsung heroes at <span class="math inline">\(t=0\)</span>! And it’s fascinating how the <strong>flag</strong> in the input is already so specifically wired to control the <strong>update gate</strong> (but less so the reset gate, interesting!). Plus, we’re starting to suspect the network is already ‘assuming’ a certain kind of input – fixed length, maybe even expecting those two flagged numbers.</p>
<p>But remember our starting questions? We’re just scratching the surface of “what these weights <em>mean</em>.” And the big “WHY?” – “how did we even <em>get</em> these weights?” – is still a complete mystery! This first step analysis is cool, but it’s just the beginning. To really understand this GRU, we gotta dig deeper into how these weights <em>learned</em> to be this way. And <em>that’s</em> where the real fun begins…</p>
</div>
</div>
</section>
<section id="coming-next" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="coming-next"><span class="header-section-number">7</span> Coming next…</h2>
<p>In this part, we took a thorough investagion into the inner workings of our GRU cell, dissecting its very first iteration piece by piece. We carefully traced how the input and weights influenced the ** <span class="math inline">\(z_t\)</span>, <span class="math inline">\(r_t\)</span>, and <span class="math inline">\(\hat{h}\)</span> gates**, step by step, uncovering how activations evolved and what they actually meant. Along the way, we stumbled upon three key insights about what the network had learned.</p>
<p>But that is still just the <strong>what</strong>—now it’s time to ask <strong>why</strong>.</p>
<p>Why did the model learn to do this seemingly strange thing? Was it always heading in this direction, or did it explore different strategies earlier in training? Were the weights trying to do something entirely different at first?</p>
<p>To answer these questions, we’ll rewind the clock and analyze how the model’s weights evolved over time. Did they start off chaotic before settling into a structured pattern? Were different strategies competing before the final approach emerged?</p>
<p>And then, we’ll take things a step further. Instead of just observing the learned weights, we’ll <strong>craft our own by hand</strong>—designing a set that does precisely what we expect. Then, we’ll compare our manually created weights with the network’s chosen ones. Did the network find a more efficient solution? Did it take shortcuts we wouldn’t have thought of? Or did it stumble upon an elegant trick that we can learn from?</p>
<p>Next up: reverse-engineering learning itself. Let’s crack this thing open!</p>
<p>I LOVE THIS SO MUCH!</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/theghoul21\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025, Luca Simonetti</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>